Structured Expression Project Toolkit Design Document
(Renamed from Abstract Data Representation, so the acronym ADR should be substituted with SEPT)

2019.08.11

-   Had long ago come up with an idea for the following, now to get some notes down about it
    after having a very inspiring conversation with Raffi Bedikian (one of many inspiring convos).
    -   Want to develop a general-purpose grammar/relation/data/structure (in the sense of
        mathematical structure) representation that can be used to represent whatever data,
        can be used to represent mathematical relationships, present ideas, express grammatical
        constructions, and satisfy constraints (using some sort of type system).  For now, call
        it ADR (Abstract Data Representation).  Should come up with a better and more specific
        name.
    -   ADR could then have a GUI viewer/editor app which uses the view/document model and can view
        the "ADR document" dynamically, manipulating how it's viewed, examining different aspects of
        it, or organizing/rendering it in myriad different ways.  This should also be used to edit
        an ADR document in a rich way.
    -   Biome would then be implemented as a particular subset of ADR and as a way to map those to
        machine code.  This viewer/editor GUI app would be a way to get an IDE for biome for "free".
    -   Other languages and data formats and generally anything could be implemented in ADR as a
        "sub-format", and editing those things would be "for free" in the viewer/editor GUI app,
        though there could be sub-format specific plugins to enhance the app.
    -   2019.08.18 - ADR could/should be structured so that hierarchical data don't have to be
        contiguous in the representation, but rather can be streamed in and well-formed in its
        partial presence -- and you don't have to parse the whole ADR file to have valid partial
        ADR. An example of this would be for hierarchical logging data, where there are several
        parallel child "threads" that are being added to in real time (where the additions are
        interleaved), and the complete eventual state of each "thread" is not known.  Thus being
        able to have a well-formed partial view is crucial.  As a contrasting example, JSON does
        not satisfy this.  YAML almost does, in that a segment of YAML can still be well-formed
        YAML, but there's no way to interleave child nodes in a YAML document in above-described
        way.  Furthermore, it should be possible to make sense of an ADR (not necessarily all,
        but the format shouldn't prohibit this) if the beginning and the end aren't present, i.e.
        if you `tune in` for just part of it (e.g. watching/listening to live-streaming video
        or music).
    -   2019.08.30 - ADR should be formatted in such a way that efficient structured queries can
        be performed on it.  Perhaps this means having some sort of indexing scheme that allows
        constant/log time lookups.  This design criteria can also be thought of in the sense that
        it shouldn't be necessary to parse the whole ADR file just to get a single piece of
        data out of it.  This design consideration may not be compatible with the streaming
        design consideration, or perhaps just can't coexist simultaneously.
        2020.03.07 - This constant/log time lookup is probably not possible in general, but perhaps
        could just be done in ADR itself; a subtype which specifies a data structure which has
        constant/log time lookup complexity.

2019.12.26

-   Email conversation with Raffi 2019.12.26 - 2020.01.09
    -   Digested highlights:
        -   Raffi: "A layperson could benefit from a lowered barrier to structured thinking"
        -   Raffi: "The closest thing I could find was the idea of “projectional editing” from
            2008: https://www.martinfowler.com/bliki/ProjectionalEditing.html"
        -   Victor: "A possible slippery slope is to give ADR its own type system, after which it
            would be tempting to make that type system more and more powerful, giving it computational
            ability, and then you're in the realm of a programming language, but because it wasn't
            designed intentionally, it would be a bad programming language.  One way out of this
            trap is to have certain levels of the language:
            -   Level 0: inductively typed data using built-in constructions, but there would be no
                         type constraints.
            -   Level 1: specify type constraints using the same inductive type system using built-in
                         constructions, allow definition of new types within this system, but disallow
                         computation within the type system.
            -   Level 2: use a stateless, side-effect free language (i.e. functional) to augment the
                         type system.
            -   Level 3: use a full Turing-complete language to augment the type system."

    Random comment
    Inbox
    x

    Victor Dods <victor.dods@gmail.com>
    Thu, Dec 26, 2019, 8:39 PM
    to Raffi

    Man, occasionally I notice and am frustrated by the fact that something like our Leap Shell doesn't already exist in modern filesystems/OSes.  Like recording many relevant metadata structures that can each be used to view and slice the data.

    End of random frustration due to my computer not automatically creating this sort of metadata for files I work on as part of a math research project.

    I hope your world is going well! :)

    Victor

    Raffi Bedikian <rabedik@gmail.com>
    Tue, Dec 31, 2019, 12:26 PM
    to me

    Hey,

    (Sorry for the delay, I was visiting family)

    Yeah I want this too, for handling metadata of many forms: files, web pages, music, photos, ... then eventually algorithms, experiences, and simulations.

    I've been thinking about how this fits into the broader ability to map between high-dimensional spaces. Like the pixels on your screen have a dimensionality of width * height * channels, but most configurations are unusable because of our visual system - therefore we use some low-dimensional embedding of this space, with windows/tabs/buttons/etc., but other embeddings could be more efficient. In particular the weighting of different dimensions at both ends should be dynamically controlled, to handle different hierarchies, search operations, and visualization methods.

    Similarly for input, 2D mouse position and binary keystrokes are a low-bandwidth way to express high-dimensional intent. Hopefully it's possible to solve these problems without waiting for brain interfaces.

    -Raffi



    Victor Dods <victor.dods@gmail.com>
    Sat, Jan 4, 11:08 PM
    to Raffi

    Haha, i like that way of thinking about the dimensionality of visual representations - I’ve never thought about it that way. Do you envision something that would still be comprehensible to a layperson, or would it be something that requires some training, analogous to learning a new language?

    A useful primitive analogy for the high dimensional input problem is this editor I use for math (essentially a GUI for Latex). I’ve set up keyboard shortcuts for inputting everything I use, and I can actually type math in this thing as fast as I can write it by hand, which is a pretty decent benchmark. Though obviously the goal would be to type it as fast as I can think it ;)

    I’ve been thinking along those lines regarding this visual editor/renderer we’ve talked about for this abstract data representation. I figure each application of that ADR (e.g. tabular data, a particular programming language, compressed image data structures, whatever other particular structured data) would correspond to a particular subset of the ADR, and a corresponding set of keyboard shortcuts to edit/traverse it and manipulate the rendering of it.

    I’ve been making very slow progress on my practice programming language cbz, and I’ve been thinking about making a stab at a first pass at this ADR concept.  It would solve so many problems in a programming language that are really just because we represent code as formatted text.  I just wish I could spend full time on it ;)


    Raffi Bedikian <rabedik@gmail.com>
    Mon, Jan 6, 1:00 PM
    to me

    I love that idea, yeah I hope it would be one of those rare things that’s simultaneously way more powerful and easier to use. So a layperson would benefit from lower barrier to entry for structured thinking, while a professional would benefit from reduced boilerplate and more intuitive visualization/control. And of course teams would benefit from reduced friction - instead of having to come up with a single set of rules, everyone would have their own customized rendering of the project.

    The closest thing I could find was the idea of “projectional editing” from 2008: https://www.martinfowler.com/bliki/ProjectionalEditing.html

    On that page he describes the core problem being the ADR. It seems like one goal would be a “self-hosting” implementation, such that the system can edit itself.

    -Raffi

    On Jan 4, 2020, at 11:09 PM, Victor Dods <victor.dods@gmail.com> wrote:


    Victor Dods <victor.dods@gmail.com>
    Mon, Jan 6, 1:39 PM
    to Raffi

    Totally agreed on the layperson/professional benefit.  I'm always bummed when something is designed to the exclusive benefit of one at the cost of the other.

    Thanks for the link, that looks super interesting/apropos.  I've also been thinking the ADR is the challenge.

    And maybe even more so, there's this temptation to make the ADR more and more powerful, even starting with the idea that the ADR should have its own type system (meaning like you could specify that a particular document has to obey some constraints on its values -- it can't just be any ADR content).  But then that so easily leads to the territory of types being specified by nontrivial computations (like say if value X is an array having N elements, then value Y is an array having N*N elements), and then you're basically looking squarely down the hallway of requiring that the ADR itself provide a Turing-complete language, which is a hilarious irony.

    Though I was thinking that one way to deal with this temptation is to specify different "levels" to the type system.  Like going from having no type requirements (call that level 0), to basic types that can be constructed from builtin types (e.g. float, int, bool, etc) and builtin type constructors (arrays, structs, maps, etc) (call that level 1).  And then maybe the next level up (level 2) is one where the computation allowed in the type system is restricted say to be free of side-effects and state, but allows people to define their own type constructors.  And then the next level would be a full Turing-complete language (call that level 3).

    It seems like level 0 would be a superior replacement for JSON, and level 1 could be sufficient for almost all practical situations.  In the schema for whatever data you're working with, you'd specify what level of ADR type system you're adhering to, generally using the lowest sufficient level.

    Gah, this is making me want to drop everything else and just work on this :-D


    Raffi Bedikian <rabedik@gmail.com>
    Tue, Jan 7, 7:51 PM
    to me

    Interesting, have you messed around with functional programming at all? I fooled around with Lisp in college and liked the “code is data” principle. It also cleanly handles the side effects you mentioned by requiring a function to only depend on its parameters.

    The levels idea is cool, reminds me of the hierarchies of expressive power (regular, context free, context sensitive). I’d wager there’s some elegant way to present this to the user so they can use it without understanding the theory behind it. How would someone debug this? Would we need a renderer for the renderer? ;)

    -Raffi

    On Jan 6, 2020, at 1:40 PM, Victor Dods <victor.dods@gmail.com> wrote:


    Victor Dods <victor.dods@gmail.com>
    Tue, Jan 7, 9:53 PM
    to Raffi

    Haha, uh oh, that sounds like the start to something that ends in the halting problem ;)

    But yeah totally! That hierarchy of Computational languages is a good analogy. Use the weakest tool that gets the job done, because it will be simpler that way.

    Yeah the debugging tools are gonna be a challenge. Though I think my first effort would involve:
    1. Render the ADR-based code to text form
    2. Uhhh.. add debugging info to the compiled code that correspond to the text source file locations
    3. Use standard text based debugging tools, like gdb and the GUI frontends for it

    That would at least be a start using real debugging tools, even if the text rendering of the code is not ideal.

    As for debugging using the ADR-based code.. maybe debugging symbols can be arbitrary strings, which could be “addresses” for ADR nodes (still not sure how that would work.. hashes perhaps), and maybe gdb could pass them through unchanged and let the front end do all the work.


    I have messed around with functional a little bit. Some Haskell mostly. It was fun but also really uncompromising in its type system (though I suppose that’s good). I did a tiny bit of lisp in college in this hilariously simplistic AI class that had basically nothing to do with machine learning or neural nets. I thought the square bracket that ended all open parens was hilarious ;)


    Raffi Bedikian <rabedik@gmail.com>
    Thu, Jan 9, 9:53 AM
    to me

    Sounds interesting to have pointers in the rendered code referring to ADR nodes, I wonder what would be needed to be able to losslessly compile back to ADR. I mean, a cycle of rendering/compiling should become a steady state at some point (ideally after only one step)

    And yeah I had the same experience with Lisp. The official textbook for our AI class was “artificial intelligence: a modern approach” but it was anything but that.

    -Raffi

    On Jan 7, 2020, at 9:53 PM, Victor Dods <victor.dods@gmail.com> wrote:


    Victor Dods <victor.dods@gmail.com>
    Thu, Jan 9, 6:20 PM
    to Raffi

    Hahaha, I may have used that book too.  We had to implement agents for the lamest games.  Games that no human would choose to play.

    I'm in Mexico at the moment on a 3 week climbing trip, and have to dedicate my rest time to my paid work, but otherwise I'd be tempted to start work on this ADR concept right away, I'm sorta stoked on it :)


2020.03.07

-   Better possible names?
    -   Abstract Data Structure Representation (ADSR) -- does "representation" defeat the purpose?
    -   Omni Structure
    -   Omega [Structure]
    -   https://en.wikipedia.org/wiki/Tir_(god)
-   Design criteria
    -   Level 0 -- data conforms to an inductive type system based on well-known built-in types and type
        constructors.  Data is strongly typed, in that each piece of data has a well-defined type.
        -   Data elements should adhere to an inductive type system using standard, well-known types (e.g.
            bool, sint8, sint16, sint32, sint64, uint8, uint16, uint32, uint64 (and probably arbitrary
            bit-width integer types), float32 (IEEE), float64 (IEEE) (and probably other float types),
            char types (e.g. a unicode char), arrays (which would therefore include strings), maps, etc.
            This in particular implies that a data node itself my comprise a hierarchy.
        -   Data types and constructors should be representable.  This implies that there has to be a
            "meta-type" specifier which defines the type for those types.  "meta-type" should not be
            a type in Level 0, as it has to stop somewhere.
        -   Types
            -   Arrays (an array is a sequence of elements indexed by the nonnegative integers)
                -   Element type
                    -   Uniform element type
                        -   Uniform element type is static (i.e. known in advance/externally)
                        -   Uniform element type is dynamic (i.e. specified alongside the array type "header")
                    -   Nonuniform element type
                        -   Nonuniform element type is static (i.e. specified by an equivalently indexed array
                            of types)
                        -   Nonuniform element type is dynamic (i.e. specified on a per-element basis, either
                            via reference, or locally with the element)
                -   Length
                    -   Length is fixed
                        -   Length is static (i.e. known in advance/externally)
                        -   Length is dynamic (i.e. specified along the array type "header")
                    -   Length is not fixed (loosely meaning that the number of elements isn't tracked; this
                        probably implies that ADR serialization of the elements has to be contiguous)
            -   Sets (a collection of distinct elements; requires some mechanism for distinguishing elements;
                a set has an element membership test; intersection/union/difference operations)
                -   UnorderedSets (an unordered collection of distinct elements)
                    -   Element type
                        -   Uniform element type
                            -   Uniform element type is static (i.e. known in advance/externally)
                            -   Uniform element type is dynamic (i.e. specified alongside the set type "header")
                        -   Nonuniform element type
                            -   Nonuniform element type is static (IS THIS POSSIBLE?  E.g. specified somehow by
                                some external data type)
                            -   Nonuniform element type is dynamic (i.e. specified on a per-element basis, either
                                via reference, or locally with the element)
                    -   Distinguishing elements (this really only applies regarding in-memory computation, as
                        the serialized form of a set should be guaranteed to satisfy the type constraint of
                        element uniqueness)
                        -   There has to be some function which tests element equality.  This sort of thing can
                            probably be defined in a natural way which respects the inductive type system.
                        -   In particular, differently-typed elements are different.
                    -   Set membership detection (this really only applies regarding in-memory computation,
                        since by construction, determining set membership is computation)
                        -   Without additional structure, this is an O(n) operation.  If the set were totally
                            ordered (an ordered set would be a subtype of set), this could be O(log(n)).  If
                            the set's element uniqueness were defined via hash map, then this could be O(1) (or
                            rather, the complexity of the hash function).
                    -   Set operations (union/intersection/difference/etc)
                        -   Similar considerations as set membership.
                -   OrderedSets -- this is a set with a total order function (i.e. a comparison operator "el1 < el2")
                    which is used to define the element equality function needed for a set.  It also allows element
                    membership to be determined in O(log(n)) time.
                    -   Comparison operators on data types can be defined in a natural way which respect the
                        inductive type system.
                -   HashSets -- this a set with a hash function (i.e. assignment of a "random-looking" integer for
                    each element) which is used to define the element equality function.  It also allows element
                    membership to be determined in O(1) time (or rather, the complexity of the hash function).
                    Note that because integers are totally ordered, a HashSet is also an ordered set (based on
                    ordering the element hash values), except that the order "looks random" with respect to the
                    actual element values.
                    -   Hash functions for (most?) data types can be defined in a natural way which respect the
                        inductive type system.
                -   Note about the various set subtypes: regarding serialization of these data types, there is a
                    representational constraint for each subtype which must be satisfied in serialized form, so
                    that there is no costly re-imposition of that constraint upon deserialization.
            -   Maps (aka dictionaries) (a map has a domain set and codomain set; for each domain element, there is
                a well-defined assignment of a codomain element)
                -   Domain element type
                    -   Uniform TODO
            -   Tables (having arbitrary numbers of parameters; not just 2) -- this is a combinatorial structure
                in which the various cases of combinations of the table parameters each have their own table
                entry.  This allows for case-wise expressions.  For boolean parameters, could visualize using
                Karnaugh diagrams (by which I mean the visual representation of a Karnaugh map).  Note that
                tables can be "sliced" by fixing one (or more) keys in a particular slot, thereby producing
                "subtables".  Note that this concept of table is different than the relational DB concept of
                table.  Maybe a better name for this would be CombinatorialTable.  Matrix is a related concept,
                but that name doesn't nicely generalize to arbitrary numbers of parameters.
            -   Graphs, DirectedGraphs, and DirectedAcyclicGraphs.
        -   The endianness of the concrete representation of a type should be specifiable.  It's not
            clear if this should mean that there are 2 different kinds of e.g. each integer type, or
            if endianness is a separate consideration.  The distinction is really about the conversion
            of a type (e.g. float32, uint64, etc) to a byte array.  This probably only really concerns
            the serialization of ADR, since the typed, in-memory representation should be in the native
            formats.
        -   Concrete representation (i.e. as a bit/byte stream) should be as space-efficient as possible.
            Human readability is NOT a concern because the idea is that a separate viewer will be used for
            a human to view such a file.
        -   There should be some mechanism for naming data nodes and referring to named data nodes
            (similar to anchors/refs in YAML).  For example, this would allow multiple metadata structures
            to index the same underlying data.
        -   It should be possible to specify external (i.e. unresolved) references.
        -   It should be possible to exactly quantify what context a particular data node or set of data
            nodes depend on (i.e. references that are external to the extracted data).  The extracted
            data would then be its own well-formed ADR document (which would have "context-dependent"
            external references)
        -   It should be possible to extract a data node or set of data nodes (which would become its own
            well-formed ADR document) in the following ways:
            -   Making its external references all "unresolved" references, which would therefore depend
                on a larger context unknown to the newly formed ADR document.
            -   Also extracting all its dependent references (and optionally everything they depend on, up
                to some point), so that there are no "newly unresolved" references, and the new ADR
                document can be considered a "sub-document" of the original.
            This suggests that there is some category theory regarding natural operations on ADR documents
            that can be explored and developed in order to have a rigorous and complete set of actions.
        -   The type of data has to be represented somewhere.  Options are:
            -   Locally, within the data node itself.  This would require the least context, but would be
                the biggest.
            -   Using some reference to a previously declared type schema (which could be internal as a
                preamble for the file, or could be external, based on the "external reference" criteria).
        -   There should probably be some convention regarding external references which are universally
            resolvable, something like a convention on namespaces, so that projects/people can specify
            reference names without worrying about collision.
        -   There are a few distinctions to be made regarding the complexity of an ADR Level 0 document:
            -   Is the type schema for all data known in advance? (i.e. is it known a-priori, or specified
                as an external, predetermined type schema?)  Or are the data types specified locally within
                the document?
            -   Are there references in the document?
                -   If there are no references, then the document can be read directly into memory with no
                    "linking".
                -   If there are references, then are any references local?  If so, then there is nontrivial
                    "linking" to be done to assemble the document locally.
            -   Does the endianness of the serialized ADR document match that of the machine?
                -   If so, then there is no endian translation needed, and data can be read in directly.
                -   If not, then there is a translation needed.
        -   Pipedream idea: Using a type schema external to the ADR document itself, could potentially read
            in other file formats altogether!  Though potentially there could be challenges if the file
            formats are dependently typed (e.g. the number of elements of an array is specified in one field).
        -   The former item (pipedream idea) brings up the idea of using a dependent type system.  This would
            be one intermediate step before "Level 2 -- functional computation allow in type system", and
            would allow things like:
            -   Dynamic, but fixed, number of elements in an array/map/etc, specified by an integer value in
                the ADR document.
            -   Maybe specify types dynamically?  Although is this already built into Level 0 implicitly?
            -   TODO: Think up more
    -   Level 1 -- a strict subset of Level 0; adds type constraints within the same type system as Level 0.
        TODO: Figure out if this is actually subsumed by Level 0
        -   Should have type constraints implemented as follows
            -   An ADR document (Level 0 or possibly Level 1 with some monotonicity constraint) specifies
                the type constraint schema by which another ADR Level 1 document can be verified.  This
                gives the ability to specify rudimentary subformats of ADR Level 1, which are effectively
                most "ordinary" file formats.
            -   Notably, this does NOT include type constraints that require computation beyond the inductive
                type system to decide (that would be Level 2 and beyond).
        -   The schema specification should be able to operate in the following modes:
            -   Previously known, external schema: Some name/reference would specify the schema, which is
                understood to be accessible to the entity doing the type checking.
            -   Embedded schema: The type constraint schema document is the "first part" in the file,
                and the "second part" of the document is subject to that schema.

-   Implementation notes for Level 0
    -   Active form (in-memory representation)
        -   In a dynamically-typed language such as Python, there is only one challenge to solve here.
            In a statically-typed language such as C++, there are two challenges to solve here, because
            the type schema of an ADR document may not be fully known in advance (the type schema could
            be arbitrary, specified locally with each piece of data).  Thus here, the two scenarios will
            be addressed:

            If the type schema for an ADR document is known in advance, then that ADR document will be
            called "statically typed" (in analogy with statically typed languages), and the data from
            that ADR document can be read directly into memory which have predefined types.  This would
            be the preferred mode from an time and space efficiency standpoint.

            If the type schema for an ADR document is not known in advance, then that ADR document will
            be called "dynamically typed".  The in-memory representation has to also be dynamically
            typed.  In general, this requires having some sort of generic "any" type, such as C++17's
            std::any type.  In other languages, some reflection capability could be used.  Otherwise,
            some form of "any" type has to be implemented manually.

    -   Serialization/deserialization
        -   Serialization is the process of taking the in-memory representation of ADR and turning it into a
            byte (or bit) stream.  Deserialization is the reverse of that process.

            These operations can be defined in a natural way that respects the inductive type system.
            However, there are potentially some parameters associated with the [de]serialization process.
            -   Endianness.  There are a few approaches here
                -   Specify a single, global endianness in the serialization of the ADR.  This is used as
                    the parameter value for all operations that require an endianness to be specified.
                    In this scenario, for example if a uint32-typed data node were encountered in serialized
                    form and that serialization was bigendian, then that uint32-typed data would be understood
                    to be uint32_bigendian-typed.
                -   The built-in data types could be speciated into big endian and little endian forms
                    of each for the purposes of serialization.  This has the advantage that each data node
                    can have a distinct endianness in its representation, though it has the disadvantage
                    that the type schema of the in-memory form of the ADR is different (though recoverable
                    from) the serialized form of the ADR.  For example, a uint32-typed value in memory would
                    be serialized to either a uint32_bigendian-typed or uint32_littleendian-typed value
                    in serialization.
            -   Type schema.  The default approach should be to make the serialized type schema reflect the
                one present in memory.  So if the in-memory type schema is e.g. static, then the serialized
                form should be just the serialized content of the data nodes and not their type schema (though
                that type schema might be present elsewhere in the serialized ADR).  If the in-memory schema
                is e.g. dynamic, then the type for each data node would be specified alongside it.

        -   Noting that the in-memory representation of the abstract form of ADR is programming language
            dependent, maybe a better way to think of the in-memory vs serialized representations are in
            relation to the abstract ADR data themselves.  Thus there are three "spaces" and two isomorphisms
            at play here.  The spaces are:
            -   Active form -- this is how ADR manifests in the memory, type system, and runtime of the relevant
                programming language, as well as aspects of the computer architecture (e.g. word size, possibly
                the endianness).
            -   Conceptual form -- this could be thought of as the mathematics/category theory of ADR.  Has no
                parameters or intrinsic representation.
            -   Serialized form -- this is how ADR manifests as a serialized byte (or bit) stream.  The parameters
                include the endianness of the byte (or bit) representation of the various data types.

            The two isomorphisms are:
            -   Isomorphism between active form (with specified parameters) and conceptual form.
            -   Isomorphism between serialized form (with specified parameters) and conceptual form.

            In terms of practical manifestation, unless the conceptual form has a way to be represented, there is
            no tangible representation for each of those isomorphisms, but rather the composition of those isomorphisms
            is what manifests tangibly, e.g:
            -   Isomorphism from active form of C++ on x86_64 to serialized form using little endian.
            -   Isomorphism from active form of C++ on x86_64 to serialized form using big endian.
            -   Isomorphism from active form of Python on ARM to serialized form using big endian

            As a practical matter, the isomorphism would be implemented a single time per language and would have
            as its parameters the remaining parameters of the active form space (in particular, the computer
            architecture) and the parameters serialized form space (in particular, the endianness).  So for example,
            if the computer architecture's endianness matches that of the requested serialized form, then there
            is no endian translation step necessary.

            It might be useful to have a representation of the conceptual form that is human readable text.  Though
            this could potentially be used in a misguided way as a drop-in replacement for JSON.  Instead, the
            human readable text form should be considered a projection, and treated that way.  And actually, a human
            readable text form of the conceptual form should not be considered the conceptual form, but rather its
            own form, which is naturally called human-readable text form.  Again, it would be antithetical to the
            purpose of ADR to treat the human readable text form as a first class form.

            By construction, active form is where all the transformations necessarily happen (though perhaps there
            could be a set of transformations done on the serialized form directly).

    -   Visualization
        -   One of the main goals of ADR is to separate the representation of the data from its visualization,
            so that the data can be sliced and diced and presented in flexible ways.

            Generally, a data node will have a collection of "visualization functions", each of which produce
            a 2D image region (though in the future, the visualization could include other things like 3D
            renderings, auditory renderings, text, other data).  The placement of the image region should
            (probably) not affect the rendering.  The visualization function should accept some sort of
            parameters indicating the min/max/preferred 2D image dimensions.  The visualization function should
            come with a "weaker" function which accepts the same parameters and produces the 2D image size
            that would result from rendering with those parameters.  This way, composite layouts of data
            nodes can be determined before actually rendering anything.

            Potentially, a "stronger" size function could be defined which also allows for some amount of
            dynamism and interaction/feedback between other data nodes' layouts, e.g. where there is a
            finite space on screen and the various data nodes are vying for space, applying forces on one
            another.

            Want to come up with concrete examples of visualization, which will produce design criteria.
            Start with some standard data types.
            -   Array (there is a known number of elements; elements are indexed by nonnegative integers)
                -   Display the elements in order
                -   Display a range of elements in order
                -   Display the number of elements
                -   Display the elements in a different, specified, order
                -   Display unique elements of the array
                -   Display duplicate elements of the array
                -   Display the array resulting from applying a "visualization function" to it (e.g.
                    apply a hash function to each element; e.g. assign a color to each element; e.g. assign
                    the element's type to each element)
                -   Collapse/expand the visualization of each element, showing some reduced summary of each
                    element for collapsed elements.

    -   Editing
        -   Called "projectional editing" by Martin Fowler: https://www.martinfowler.com/bliki/ProjectionalEditing.html
        -   Challenge will be to integrate changes made in "projection" into the original data.
            This likely depends strongly on the projection itself.  For example, if the projection
            involves a hash function (or really any lossy function), so that the hash value of a
            data node is the projection, and that hash value is edited, then it's impossible to
            update the original data node in a well-defined to affect the change to the hash value.
        -   One way to avoid the problem of integrating changes to a projection is to display both the
            original data and the projection, then as the user updates the original data, the projection
            is updated in realtime.  This would give some semblance of control over the projection.
        -   In order to affect changes in the original based on changes to a projection, a "change
            integration" function should be specified along with each projection function that intends
            to produce editable content.
            -   There can be constraints on the editable content which, if violated, would preclude the
                ability to integrated the edited projection.  If this is the case, then a warning should
                be issued to the user before it's done.
        -   If a projection function doesn't have a change integration function, then it should be indicated
            as a "read-only" projection.  Examples:
            -   In general, lossy projection functions
            -   Hash functions (these are generally lossy, but even to find a preimage of a hash value is
                computationally infeasible)
            -   A text representation of something that no one has bothered to produce a parser for.
            -   In general, conversion of a data type to a weaker type (e.g. OrderedSet to Set)
            -   Producing the "type of" a data node (which in the case of a hierarchical data type, would
                produce an entire hierarchical type schema).
        -   Examples of projection functions that do have change integration functions
            -   A faithful text rendering of something (e.g. rendering of source code from its abstract
                representation); the change integration involves parsing the changed text and integrating
                the resulting abstract representation into the original data.
            -   Take a sub-range of elements from an array; change integration just involves changing the
                elements in the original that corresponded to the changes in the projection.
            -   In general, anything where some component of the projected data is preserved unchanged;
                the change integration involves applying the changes to the corresponding original data nodes.
            -   More generally, taking one component of a data type that is a natural product (e.g. video +
                audio); the changes to the projection can be applied to the corresponding component of the
                original.
            -   Project video+audio to just audio, then cut out segments of the audio; the change integration
                is to cut out the corresponding segments of the video.
            -   In general, if there are two components in the original data that share the same indexing
                scheme (e.g. mapping domains) and the projection is to take just one component, then modification
                to the mapping domain of one component can be applied to the others in order to integrate
                the changes.
            -   FFT of a signal; change integration is just inverse FFT.  Care must be taken here to pay
                attention to accumulation of numerical error.
            -   In general, any isomorphism applied to data can be reversed via its inverse isomorphism.
            -   Certain data-type-weakening conversions:
                -   Convert a list to a set; changes to the resulting set could be integrated by integrating
                    changes to respective elements (noting that one element of the set could correspond to
                    multiple elements of the list).
        -   Projection functions that admit a corresponding change integration function if supplied with
            additional information (basically corresponding to the information lost in projection)
            -   Taking an integer modulo some number; if the modulo'ed number is changed, then the original
                integer can be updated if the branch of the inverse-modulo function is specified (i.e. which
                integer range the updated value should appear in).
            -   In general, a projection function P can be thought of as one of a pair (P,F), where F is
                a function that captures the forgotten information.  So really to invert the action of P,
                it's necessary to specify the forgotten information and then apply the inverse of the
                function (P,F).
        -   Nested projections (this will probably be a very tempting and useful use case) -- if multiple
            projections are composed, then changes can be integrated if each of the change integration functions
            can be composed.

    -   Diff'ing -- ideally, diffs can be defined naturally in a way that respects the inductive type system.
        -   What is appropriate terminology here?  Diff and patch?  Or is this misleading?  Maybe use ADR-diff and ADR-patch?
            References:
            -   https://en.wikipedia.org/wiki/Diff#Context_format -- the "unified diff" seems to be the standard for
                source code now (git uses it).
            -   https://en.wikipedia.org/wiki/Patch_(Unix)
        -   Because ADR is intended to represent abstract data structures, the diff/patch operations should be defined
            correspondingly abstractly.  In particular, these operations should NOT depend on any serialized representation.
            Thus there needs to be some way to navigate/address nodes in the ADR in order to specify which node(s) are to
            be operated upon.
        -   Identity patch
            -   There should be a unique identity patch for each data type, and that patch is a no-op.
        -   Inverting a patch
            -   Question: Should/Could a patch contain enough information as to be invertible?  It appears that
                standard patches are meant to be reversible.
        -   Composing patches
            -   Finite sequences of patches should be composable, producing a well-defined patch.
            -   If the composed patches are each invertible, then the composed patch should be invertible.
        -   Determining commutativity of patches.
            -   In general, patches are not commutative (here, "commutative" means that the order of application
                doesn't matter; both orders produce the same result).  For example, if there is an ADR document
                with a single uint16 node, and there are two patches which change the value of that node, then
                those patches are not commutative, and it is necessary to specify an order of patch application
                in order to get a well-defined result.  Although in this case, the second patch will fail to find
                the expected value to be changed (since it's different due to the first patch), and so some sort
                of "merged" patch must be produced from the two original patches.
            -   Is there a general way to determine patch commutativity?
        -   Applying a patch to a data node
            -   The specifics of how to apply the patch are specific to the data type(s) involved.
        -   Diff'ing data nodes, to produce a patch
            -   For the atomic data types (e.g. bool, int types, float types), a diff is just a value replacement.
                If invertibility is required, then the patch would need to specify the value that is being replaced,
                so that it can be inverted.
            -   For arrays, there are different forms of diffs
                -   In-place element change -- the diff falls to be a diff on the corresponding "before" and "after" elements.
                -   Inserting an element -- requires some sort of context indicating where the element is to be inserted
                -   Deleting an element -- requires some sort of context indicating which element is to be deleted
        -   Patch/merge conflicts
            -   As mentioned in the item regarding commutativity, some mechanism for resolving non-commutative patches
                is needed.  In the context of programming, non-commutative patches are called a "merge conflict", and
                it is resolved by the programmer manually specifying what the result of the merge should be.  Probably
                in general that's the best that can be hoped for (a human-driven resolution process).
    -   Diffs as a form of editing
        -   A simple example to motivate: In the example where a uint16 value is being edited, instead of replacing a
            known, fixed quantity with another known, fixed quantity, there could be an integer-specific kind of a
            diff which just adds some known, fixed value to whatever it finds there.  This particular kind of edit
            would need to be specified directly by the [human] editor (as opposed to an edit which is just a replacement
            of the value).  Then, if two of these patches were applied, even though they're changing the same value,
            they would be commutative, because integer addition is commutative (noting that integer overflow is still
            a thing).
        -   Data-specific diffs will give more and more richness to the editing/diffing/merging process.  Other examples:
            -   Taking the logical not of a boolean value
            -   Negating an integral value
            -   Changing an integral value by 1
            -   Multiplying a numerical value by a known, fixed value
            -   A gotcha to watch out for is that certain floating point operations are not commutative, due to the specifics
                of how numerical precision works, and also how floating point math is often CPU implementation dependent.
            -   Inverting an invertible map
            -   Applying a permutation to an array

    -   Adaptation of existing common file formats as design criteria (this is sort of like challenge mode for this
        design document).  Just to name a few:
        -   tar
        -   zip
        -   bmp
        -   jpeg
        -   mp3

        Here's a huge list: https://en.wikipedia.org/wiki/List_of_file_formats

-   Notes on type schema
    -   In terms of efficiently indexing and traversing data within the serialized form, it's probably a
        good idea to, for each data node, store the offset from the beginning to the end of the node, so
        that it can be skipped in constant time.  Maybe this should involve some sort of serialization
        structure itself, like "data with offset", and this wouldn't appear in the active form, but would
        be a parameter of the serialized form.  The "type of" a "data with offset" would be the type of
        the underlying data (i.e. it would fall through to the data).

    -   Idea for way to think about serialization and use of external context for type information:
        Consider serialization to be a projection, and the forgotten information is the context that's
        needed to reconstruct the original ADR.  The forgotten information could be considered to be
        the type schema.

        A way to visualize this would be to have two parallel columns.  On the left is some representation
        of the serialized ADR (in some human readable form, not as raw bytes).  On the right is the type
        schema information that annotates the serialized ADR on the left, where the correspondence is made
        by aligning corresponding serialized data on the same row as the type schema info.  You could then
        manipulate the exact nature of the serialization by moving the type schema info into or out of
        the serialized column.  Moving it out of the serialized column would correspond to externalizing
        more of the type schema, leaving the serialization more bare and efficient (though more context-
        dependent).

    -   Atomic types
        -   type (stores a type; possibly including `type` itself)
        -   byte (same as uint8 but has no arithmetic) -- is this needed?
        -   bool
        -   uint8
        -   uint16
        -   uint32
        -   uint64
        -   sint8
        -   sint16
        -   sint32
        -   sint64
        -   float32 (IEEE single precision)
        -   float64 (IEEE double precision)
        -   float80 (IEEE double extended precision)
        -   float128 (IEEE quadruple precision)
        -   asciichar (equivalent to byte or maybe uint8)
        -   utf8codeunit (equivalent to byte or maybe uint8; is this well-defined?)
        -   utf16codeunit (equivalent to uint16; is this well-defined?)
        -   utf32codeunit (equivalent to uint32; is this well-defined?)
        -   unicode (unicode char; equivalent to uint32; as of May 2019 the most recent version, Unicode 12.1,
            contains a repertoire of 137,994 characters -- see https://en.wikipedia.org/wiki/Unicode )
    -   Inductive types
        -   any : The type of the data is specified alongside the data; dynamically typed.
            -   Type.
            -   Serialized data of the given type.
        -   tuple : A sequence with a fixed, known length; see section on tuples.
        -   array : A sequence with a uniform element type.
        -   list/sequence : TODO: Figure out if there's something here distinct from tuple and array.
        -   reference : The mechanism for referring to the data underlying an anchor using some unique identifier.
        -   anchor : The mechanism for specifying the unique identifier for the underlying data.
        -   set : A collection of distinct pieces of data.
        -   map : Given a domain and a codomain, a map assigns each element of the domain an element of the codomain
            in a well-defined, reproducible way.
        -   tree ? (and n-tree for fixed n?)
        -   EXPERIMENTAL:
            -   dataoffset : A way of specifying the length of the underlying data so that it can be efficiently skipped.
            -   TODO start here




    -   Notes on tuple
        -   A tuple is a finite sequence of elements, and thus has a well-defined length.
        -   There are a few ways to encode the length of a tuple:
            -   With a "start" and "end" delimiter, as in JSON
            -   With a pre-defined element count
        -   There are a few ways to encode the tuple's element types:
            -   With a type tuple, before the elements are presented
            -   Alongside each element
            Noting that there has to be the same number of elements in the tuple and its type tuple,
            it's wasteful to encode the length of each redundantly.  Thus it may be better to encode
            the tuple element types alongside each element, like

                tuple( any(uint8,123), any(float32,1.455), any(bool,true) )

            an "any" is a pair where the first one is a type and the second one is the data itself.

        -   IDEA: Allow all encodings of tuples (e.g. types specified separately, types specified
            alongside data, etc)

        -   If the type tuple should be specified separately (e.g. via some reference), then it would be
            something like

                typedtuple( typetuple(uint8, float32, bool), datatuple(123, 1.455, true) )

            maybe this would also have an element offset for efficient access?

        -   If the type of the tuple were known using totally external context, but still it was desirable to be
            able to access each element of the tuple:

                untypedtuple( rawbytesoflength(<length>, <byte-array>), rawbytesoflength(<length>, <byte-array>), ... )

        -   Another way to serialize efficiently would be to use anchors/references, like such:

                tuple( any(uint8,*el0), any(float32,*el1), any(bool,*el2) )
                anchor( el0, 123 )   // the length of the data would need to be encoded here
                anchor( el1, 1.455 ) // the length of the data would need to be encoded here
                anchor( el2, true )  // the length of the data would need to be encoded here

            or if the types should be within the anchor, it would be

                tuple( *el4, *el5, *el6 )
                anchor( el4, any(uint8,123) )
                anchor( el5, any(float32,1.455) )
                anchor( el6, any(bool,true) )

        -   Second take on tuple; free-form jamming
            -   Note that a serialization construction has the form

                    constructor( arg0, arg1, ... )

                and is rendered into the byte stream as

                    <begin-constructor spec> // Conforms to a particular, known spec, and has known, fixed length; e.g. is the
                                             // numerical id of the type to be constructed.
                    <serialization of arg0>
                    <serialization of arg1>
                    ...
                    <end-of-constructor spec> // If called for by the constructor; also contains the id of the type (needed for matching)

                If a keyword is given for an arg (e.g. array(length = xyz, ...)), that's just for sake of readability, it
                doesn't actually change the above form.

            -   0. StatTuple type totally specified in external context (i.e. its length and its element types, and therefore its size)

                    <raw-bytes> // Nothing can be known by looking at the raw bytes, not even where it ends (apart from EOF)

                The information present in the serialization is nothing.

            -   1. StatTuple type totally specified in external context (i.e. its length and its element types, but sizeof tuple is
                encoded in serialization for purposes of being able to skip over that field).

                    bytearray(length = uint32(9999999), <raw-bytes-of-length-9999999>) // Full serialization of tuple

                The information present in the serialization is the length of raw bytes

            -   2. StatTuple type totally specified in external context, but it's still desired to be able to index each
                tuple element's raw bytes within the serialization.  However it's necessary to iterate over all
                elements to skip this whole thing in the serialization.

                    bytearray(length = uint16(30000), <raw-bytes-of-length-30000>)         // Element 0
                    bytearray(length = uint8(4), <raw-bytes-of-length-4>)                  // Element 1
                    bytearray(length = uint32(16000000), <raw-bytes-of-length-16000000>)   // Element 2

                The information present in the serialization is the element lengths, but not the overall length,
                or the types of the elements, or even that a tuple is being represented here.  Sort of; a serialized
                ADR is a tuple in a weak way, except that you have to know that the first thing you're looking at
                is a serialized constructor, and not just raw bytes.

            -   2a. Same as previous, but the overall length is included as in 1 so that the whole thing can be skipped easily.

                    bytearray(                                                          // This is the size "header"
                        length = uint8(
                            sizeof(any(uint8,100))+100+sizeof(any(uint8,20))+20+sizeof(any(uint8,3))+3
                        ),
                        raw bytes = {
                            bytearray(length = any(uint8, 100), <raw-bytes-of-length-100>)  // Element 0
                            bytearray(length = any(uint8, 20), <raw-bytes-of-length-20>)    // Element 1
                            bytearray(length = any(uint8, 3), <raw-bytes-of-length-3>)      // Element 2
                        }
                    )

                The information present in the serialization is the element lengths and overall length, but what is not
                known are types of the elements and the fact that it is a tuple being represented here.  Sort of; a
                serialized ADR is a tuple in a weak way.

            -   3. Element types are specified in serialization, although the fact that it's a tuple is not known.

                    float32(7.024)
                    sint32(-1000000)
                    asciichar('a')
                    bool(true)

                The information present in the serialization is the element types (and therefore lengths).  It isn't
                explicitly known if this corresponds to a tuple, although implicitly it is, since all ADR serializations
                are tuples in a weak way.

            -   4. A tuple is explicitly presented, though its length is not known ahead of time.

                    tuple(
                        float32(7.024)
                        sint32(-1000000)
                        asciichar('a')
                        bool(true)
                    )

                The serialization is aware of everything except how to quickly skip the whole tuple.

            -   4a. Same as 4, except that the length of the tuple is given so that it can be easily skipped.

                    tuplewithknownsize(
                        sizeof = uint32(...),
                        elements = {
                            float32(7.024),
                            sint32(-1000000),
                            asciichar('a'),
                            bool(true)
                        }
                    )

                The serialization is aware of everything, though efficient indexing of the tuple is not possible.

            -   5. Same as 4, except that

                    tuplewithoffsets(
                        offsets = array(
                            length = uint8(4),
                            elementtype = uint16,
                            elementends = {
                                sizeof(float32(7.024)), // Note that `float32(7.024)` here is really sizeof(type)+sizeof(7.024)
                                sizeof(float32(7.024))+sizeof(sint32(-1000000)),
                                sizeof(float32(7.024))+sizeof(sint32(-1000000))+sizeof(asciichar('a')),
                                sizeof(float32(7.024))+sizeof(sint32(-1000000))+sizeof(asciichar('a'))+bool(true)
                            }
                        ),
                        elements = {
                            float32(7.024),
                            sint32(-1000000),
                            asciichar('a'),
                            bool(true),
                        }

                Now the serialization is aware that the thing is a tuple, it knows the number of elements in the tuple,
                it knows the types of each element, and it can look up an element type or an element value in constant
                time.  And because the element offsets are given by their ends, the end of the tuple is known.

            -   Observation: In some cases it could be better to use anchors/references to handle the differing element
                lengths, and make the tuple itself just be a tuple of references.  On the other hand, not doing that
                could allow a direct read-into-memory serialization process.

            -   Other thoughts on how to formalize serialization form types
                -   Identify what aspects/guarantees there are and then make each of those an option for the
                    tuple type (analogously, other inductive types).  For example:
                    -   ConstantTimeSizeof -- so that the serialized tuple can be skipped in constant time
                    -   ConstantTimeElementLookup -- so that a tuple element can be looked up in constant time
                    -   ConstantTimeElementTypeLookup -- so that a tuple element's type can be looked up in constant time
                                                         does this come automatically with ConstantTimeElementLookup?
                                                         no, not if the types of the elements aren't specified.
                    -   ConstantTimeElementCount -- so that the number of elements can be determined in constant time
                    -   ConstantTimeTypeTuple -- so that the type tuple of the tuple can be determined in constant time,
                                                 although this brings up the question of the spec of that type tuple
                                                 (i.e. what of these flags does it have?)

                -   Some examples of the above:

                    -   Can look up elements, can count them,

                        begintuple
                            tuplespec = uint8(0) // None of the above flags
                            elements = {
                                <element-with-known-size>
                                <element-with-known-size>
                                ...
                                <element-with-known-size>
                            }
                        endtuple

                        begintuple
                            tuplespec = uint8(ConstantTimeSizeof)
                            sizeof = uint32(...) // I suppose this should include everything from begintuple to endtuple
                            elements = {
                                <element-with-known-size>
                                <element-with-known-size>
                                ...
                                <element-with-known-size>
                            }
                        endtuple

                        begintuple
                            tuplespec = uint8(ConstantTimeElementLookup) // This implies ConstantTimeSizeof and ConstantTimeElementCount
                            elementendoffsets = beginarray
                                length = uint8(...)
                                elementtype = uint16
                                elements = {
                                    uint16(offset to end of element 0)
                                    uint16(offset to end of element 1)
                                    ...
                                    uint16(offset to end of last element) // This can be used to determine sizeof
                                }
                            endarray
                            elements = {
                                ...
                            }
                        endtuple


        -   Arrays, because they have a uniform element type, don't suffer from some of the problems of tuple.

            -   0.





    -   Notes on arrays
        -   An array is a tuple except that all elements have the same type, and thus the type specification
            is somewhat simpler.

                array( uint8, 123, 101, 88, 10, 254, 201 )
                array( uint8, *el0, *el1, *el2, *el2, *el1 ) // Note repeats
                typedarray( arraytype(float32,3), rawbytesof(80.08:float32), rawbytesof(1.234:float32), rawbytesof(3.14159:float32) )
                untypedarray( rawbytesof(80.08:float32), rawbytesof(1.234:float32), rawbytesof(3.14159:float32) ) // All elements must have same size

2020.03.10

-   I met up with Max Sills for coffee and sammiches and told him about this ADR stuff, and he was
    very excited about it, and came up with a really novel and potentially huge application of
    this projection editing concept.  This applies to server/client interactions.  Servers typically
    house "all" the data, and clients see and edit some small fragment of it.  Think of the server-to-
    client communication as a projection of the data.  The client views it, edits it, and informs the
    server of its edits.  This can be thought of as the change integration.  If thought of in this way,
    then server/client modeling and communication could potentially be reduced to a natural implementation
    and complex APIs done away with entirely.  A proof of concept is certainly called for, and there may
    be complications based on different assumptions or constraints on server/client communication, but
    I think there's a very valuable kernel there.

2020.03.12

-   Development notes
    -   Conceptual form
        -   Defining data types and inductive type system
            -   The challenge here is to make a rich enough type system (e.g. having gradations of strength
                in various data types such as UnorderedSet, HashSet, OrderedSet, etc)
            -   Note that thinking of serialization as projection (see below) requires the serialized form
                to be a data type (in particular, a bit/byte array, which it would have been anyway).
        -   Defining projections (call this extraction?  would be a better opposite to "integration")
            -   Crucially, a projection is lossy -- quantify exactly what information is lost, as this
                is the external context that's needed to perform a corresponding change integration.
        -   Defining change integrations
        -   Defining diffs and patches
        -   Work out type schema projection/integration logic (as mentioned in "think of serialization as projection")
    -   Serialized form
        -   Think of serialization as a projection and deserialization as change integration
            -   In particular, consider the type schema for the active form of some data node vs
                the type schema for the serialized form of the same node.  The serialized form
                type schema should be a projection of the active form type schema, and the lost
                information is the external context that's necessary to perform the integration.
        -   Definitions of various gradations of local typedness
    -   Active form -- this might also be thought of as a projection, since each different programming language
        is going to have different support for the various types.
        -   Language-specific choices in representation of the ADR
        -   Implementing projections (and specifically, serialization)
        -   Implementing change integrations (and specifically, deserialization)
        -   Implementing diffs

    Priorities, high to low
    -   Conceptual form : Define data types and some basic collection of inductive types
    -   Conceptual form : Define a decent set of projections and integrations on the available types
    -   Active form : Implement language-specific representation of the defined conceptual form types
    -   Active form : Implement projections and integrations

-   Notes on defining data types and inductive type system (some of this will be repeating previous notes,
    but in an effort to refine the notions)
    -   Atomic, built-in data types
        -   type (stores a type; possibly including `type` itself); numerical encoding of a type
        -   byte (same as uint8 but has no arithmetic) -- is this needed?
        -   bool
        -   uint8
        -   uint16
        -   uint32
        -   uint64
        -   sint8
        -   sint16
        -   sint32
        -   sint64
        -   float32 (IEEE single precision)
        -   float64 (IEEE double precision)
        -   float80 (IEEE double extended precision)
        -   float128 (IEEE quadruple precision)
        -   asciichar (equivalent to byte or maybe uint8)
        -   utf8codeunit (equivalent to byte or maybe uint8; is this well-defined?)
        -   utf16codeunit (equivalent to uint16; is this well-defined?)
        -   utf32codeunit (equivalent to uint32; is this well-defined?)
        -   unicode (unicode char; equivalent to uint32; as of May 2019 the most recent version, Unicode 12.1,
            contains a repertoire of 137,994 characters -- see https://en.wikipedia.org/wiki/Unicode )
    -   Inductive types; these have a type constructor and are built out of other types inductively.
        There's a subtle distinction to be made here.  Each of the inductive type categories (e.g. StatTuple, Array,
        Map, Set, etc) actually require parameters to fully specify a particular realized type.  Use C++ template
        brackets <> to denote these params, e.g. StatTuple<bool,byte>.
        -   StatTuple: A sequence of N data nodes for some known, nonnegative N.  The parameters for StatTuple are the
            types of its elements.  The value of N is determined from counting the elements.
            -   Constructor notation

                    StatTuple<> // Empty tuple type
                    StatTuple<T1>
                    StatTuple<T1,T2,...,TN>

            -   Possible intermediate subtype

                    TupleOfLength<N> // Would be a supertype of all length N StatTuple types.

            -   kindof(StatTuple<...>(...)) := StatTuple // Is this actually useful?  The kind of an atomic datatype could be `Atomic`
            -   typeof(StatTuple<T1,T2,...,TN>(v1, v2, ..., vN)) := StatTuple<T1,T2,...,TN>
            -   Constructed instance notation

                    StatTuple<>() // Empty StatTuple
                    StatTuple<T1>(v1)
                    StatTuple<T1,T2,...,TN>(v1, v2, ..., vN)

            -   StatTuple has a length function which counts the number of elements.
            -   StatTuple has an element access function which uses 0-based indices.  If typeof(t) is StatTuple<...), then

                    t[i]

                gives the ith element.
        -   Array: Subtype of StatTuple, where all data nodes have the same type.   Array has all the same properties as
            StatTuple, but it has some additional structure, and some subtype-specific overrides.
            -   Constructor notation

                    Array<T,N>

            -   Possible intermediate subtypes

                    ArrayOfLength<N>
                    ArrayOfElType<T>

            -   kindof(Array<...>(...)) := Array
            -   typeof(Array<T,N>(...)) := Array<T,N>
            -   Constructed instance notation

                    Array<T,0>() // Empty array -- element types are distinguished, but there's a canonical isomorphism at play here
                    Array<T,1>(v1)
                    Array<T,N>(v1, v2, ..., vN)

            -


-   Random scratch notes
    -   There's a tricky distinction to be made:
        -   The "external context" for a data node is its type.  Whether or not some of that information
            is encoded also as data is up to the type.  For example, StatArray<T,N> vs DynamicArray<T>.
        -   In active form, many data types must know how many elements they have (e.g. arrays, maps),
            and some can't know without caching that (e.g. linked lists, trees, etc).
        -   In serialized form, unless the number of elements is encoded as data, it may require computation
            to determine the number of elements.
    -   IDEA: Attempt to make a C++ implementation of conceptual form.  In this form, there would be a distinct
        separation between data and its type.  The type would be encoded explicitly as C++ data.  Then a projection
        of this could be made for "static types" to produce C++ code that defines those types statically.
    -   QUESTION: What's a good opposite word to "project"?
        -   embed
        -   include
        -   lift
        -   integrate
        -   reintegrate
        -   restore
        Maybe "project" should be changed.
        -   slice (this is good, since that's already a programming word)
        -   extract (a nice opposite to restore)
        And then the opposite could be
        -   combine
        -   include
        -   integrate
        -   restore

    -   StatTuple                                           An instance of StatTuple has known length, known element types, can access elements by index number.
        StatTupleOfLen<N> <= StatTuple
        StatTuple<T1,...,TN> <= StatTupleOfLen<N>
        StatTupleHmgn <= StatTuple                          An instance of StatTupleHmgn (homogeneous) has elements having identical type
        StatTupleHmgnOfLen<N> <= StatTupleOfLen<N>
        StatTupleHmgnOfElType<T> <= StatTupleHmgn
        StatTupleHmgn<T,N> <= StatTupleHmgnOfLen<N>
        StatTupleHmgn<T,N> <= StatTupleHmgnOfElType<T>

        StatArray <= StatTuple                              An instance of StatArray is a StatTuple and all element have same type
        StatArrayOfElType<T> <= StatArray
        StatArrayOfLen<N> <= StatArray
        StatArrayOfLen<N> <= StatTupleOfLen<N>
        StatArray<T,N> <= StatArrayOfElType<T>
        StatArray<T,N> <= StatArrayOfLen<N>
        StatArray<T,N> <= StatTupleHmgn<T,N>

        DynArray                                            An instance of DynArray has a fixed type but its length is stored as data,
                                                            which in particular means that there's a well-formedness constraint on the
                                                            constructor.  E.g. DynArray(len=N, v1, v2, ..., vN)
        DynArray<T> <= DynArray
        StatArray<T,N> ~ DynArray<T>, Context(len=N)        StatArray<T,N> can be projected to DynArray<T>; lost information is N, so in order
                                                            to lift, N must be supplied.

        Array                                               An instance of Array has a fixed type but its length isn't specified at all
                                                            and may require computation to determine.  Some "end of Array" marker is used
                                                            instead.
        Array<T> <= Array
        DynArray<T> ~ Array<T>, Context(len)                This projection loses the length data.

    -   Define type kinds and their fundamental contracts (which regard runtime complexity for various operations)
        -   Sequential types
            -   LinkedList
            -   Tuple -- has length (O(1) to compute), element access is O(1)
                -   This necessitates storing the length, and representing the elements as `any` data nodes with reference indirection.
            -   Tuple_strong -- is a Tuple but also provides O(1) computation of its type (i.e. the Tuple of its element types)
                -   This necessitates storing the element types (which otherwise would be stored in the `any` data nodes) and
                    representing the elements as references.
            -   Tuple_weakA -- has length (O(1) to compute), element access is O(length)
                -   This necessitates storing the length, and representing the elements as `any` data nodes inline.
            -   Tuple_weakB -- has length (O(length) to compute), element access is O(length)
                -   This can be done using an "end tuple" marker and traversing the elements in order.
            -   Array -- has length (O(1) to compute), element access is O(1), has uniform element type

        -   List (call this FiniteList, and leave List as possibly unbounded?)
            -   Has a length, call it N, requires O(N) to compute.
            -   Elements are indexed by integers i with 0 <= i < N, requiring O(N) to compute.
            -   `typeof` function commutes with constructor; i.e. the type of an instance of Sequence is
                the Sequence of the element types.  Requires O(N) to compute.
        -   Sequence : Is a List, but
            -   Element access is O(1)
            -   Note that having element access be O(1) but allowing length computation to be O(N) implies that
                either the length has to be known before element access, or that element access may result in access
                beyond the end of the Sequence and therefore be undefined.  This is probably not what is wanted.
        BLAH, getting bogged down.

    -   Decide on a few ordinary type kinds to start with
        -   Type
            -   Has type membership test; no complexity specified
            -
        -   Array<T,size>
            -   Has size; O(1)
            -   Has index element access; O(1)
            -   Has ability to iterate over elements; O(1) per element
            -   Has set membership test; O(size)
            -   Element types are uniform
            -   typeof an Array<T,size> instance is O(1); Array<T,size>
        -   NDArray<T,shape> -- TODO
        -   Tuple (potentially could be thought of as an array of `any` type elements)
            -   Has size; O(1)
            -   Has indexed element access; O(1)
            -   Has ability to iterate over elements; O(1) per element
            -   Has set membership test; O(size)
            -   typeof a Tuple instance is O(size); Tuple(T1,...,Tsize) // This suggests that an inductive type whose data is
                                                                        // types (e.g. a Tuple of types) is itself a type
                                                                        // and should admit a constructor.
        -   Maybe List can be a Tuple but with weaker guarantees (but don't worry about this for now).
        -   Set<T,EqFunc> -- order is not specified, no iteration possible
            -   Has size; O(1)
            -   Has ability to iterate over elements in arbitrary order (based on storage order); O(1) per element
            -   Has set membership test, defined by EqFunc; O(size)
            -   Element types are uniform
            -   typeof a Set<T> instance is O(1); Set<T>
        -   UnorderedSet<T,HashFunc>
            -   Has size; O(1)
            -   Has ability to iterate over elements in pseudo-random (but deterministic, defined by HashFunc) order; O(1) per element
            -   Has set membership test; O(1) (not counting HashFunc complexity)
            -   Element types are uniform
            -   typeof an UnorderedSet<T,HashFunc> instance is O(1); UnorderedSet<T,HashFunc>
            -   Can be used as a Type, where type membership is defined as set membership.
        -   OrderedSet<T,CompareFunc>
            -   Has size; O(1)
            -   Has ability to iterate over elements in order; O(log(size)) per element
            -   Has set membership test; O(log(size)) (not counting CompareFunc complexity)
            -   Element types are uniform
            -   typeof an OrderedSet<T,CompareFunc> instance is O(1); OrderedSet<T,CompareFunc>
            -   Can be used as a Type, where type membership is defined as set membership.
        -   Map<Domain,DomainEqFunc,Codomain> (potentially could be thought of as Set<Tuple<Domain,Codomain>,FirstElEqFunc> with some additional functions)
            -   Has size (number key/value pairs); O(1)
            -   Has ability to iterate over key/value pairs in arbitrary order (based on storage order); O(1) per element
            -   Has domain set membership test; O(size)
            -   Has key-based element access; O(size)
            -   typeof a Map<Domain,DomainEqFunc,Codomain> instance is O(1); Map<Domain,DomainEqFunc,Codomain>
            -   Probably doesn't represent a Type, unless it's just the particular instance of Set<Tuple<Domain,Codomain>,FullEqFunc>;
                not sure what this would be useful for, but shouldn't rule it out for that reason.
        -   UnorderedMap<Domain,DomainHashFunc,Codomain> (potentially is just UnorderedSet<Tuple<Domain,Codomain>,FirstElHashFunc> with additional funcs)
            -   Has size (number key/value pairs); O(1)
            -   Has ability to iterate over key/value pairs in pseudo-random (but deterministic, defined by DomainHashFunc); O(1) per element
            -   Has domain set membership test; O(1)
            -   Has key-based element access; O(1)
            -   typeof an UnorderedMap<Domain,DomainHashFunc,Codomain> instance is O(1); UnorderedMap<Domain,DomainHashFunc,Codomain>
            -   Probably doesn't represent a Type, unless it's just the particular instance of UnorderedSet<Tuple<Domain,Codomain>,FullEqFunc>
                not sure what this would be useful for, but shouldn't rule it out for that reason.
        -   OrderedMap<Domain,DomainHashFunc,Codomain> (potentially is just OrderedSet<Tuple<Domain,Codomain>,FirstElHashFunc> with additional funcs)
            -   Has size (number key/value pairs); O(1)
            -   Has ability to iterate over key/value pairs in order; O(log(size)) per element
            -   Has domain set membership test; O(log(size))
            -   Has key-based element access; O(log(size))
            -   typeof an OrderedMap<Domain,DomainHashFunc,Codomain> instance is O(1); OrderedMap<Domain,DomainHashFunc,Codomain>
            -   Probably doesn't represent a Type, unless it's just the particular instance of OrderedSet<Tuple<Domain,Codomain>,FullEqFunc>
                not sure what this would be useful for, but shouldn't rule it out for that reason.

2020.03.13

-   AtomicType has enumerable inhabitants, which is a special kind of type.  This should be encoded somehow,
    because then it allows stuff like

        EnumerableTypeAsOrderedSet(EnumType)

-   Observation: There's a difference between the representation of something and its type in some abstract
    sense.  For example, if a Set could be used as a type (sort of like an enum is a set of integers), then
    while the elements of that Set have that Set as their abstract type, their representation would not be
    that Set, but could be int or double or whatever.

    Probably should specify this, like

        SetMember(member_value, SetAsType(WhateverSet))

    where

        typeof(SetMember(member_value, SetAsType(WhateverSet))) := SetAsType(WhateverSet)

    Since conflating set membership with type inhabitation could be dangerous from a distinction standpoint.
    Then note that typeof(member_value) will typically (always?) be different than SetAsType(WhateverSet).

-   Notes on projections/integrations of Array
    -   Projection: Extract one particular element based on index

        Lost context: Which array was projected from, and the extracted element index, call it i.  Although in terms
        of merging patches, this is not enough context, since the index of the relevant element could change.  To
        handle this, could use the unified context diff idea, where the index is stored (analogous to the line number)
        as well as the values of the element before and after, as well as the original extracted element value.

        Integration: Set the ith element of the original Array to the value of the changed projection.  For context diff,
        search the context (previous and next elements) to find the correct element to change.

        This type of projection has special structure, where there's sort of an embedding structure from the single
        element directly into the original array, where there is no transformation need on changes to the single element
        in order to change the original array element.

    -   Extract first element -- same as extract element using index 0

    -   Extract last element -- same as extract element using index -1

    -   Projection: Extract a subset of elements using a set of indices; the extracted value would be a Map<IndexSubset,ArrayElementType>
        Lost context: Would need to store context diff info for each of the extracted elements.
        Integration: Integrate each element.

    -   Projection: Convert into a Map where Domain is the index set and Codomain is the Array element type

        Lost context: Which array was projected from.  As long as the Map has a contiguous, 0-based index domain,
        it can be perfectly converted back to Array.

    -   Projection: Produce the type of the Array (e.g. Array<T,size>)

        Lost context: Which array was projected from and the element values of the array

        Integration: Let the changed-to type be Array<U,othersize>.  If there is a natural conversion T -> U and
        if othersize <= size, then the original Array can be modified such that its type would then be Array<U,othersize>.
        This is a type conversion on the Array.  If that condition doesn't hold, then the change can NOT be integrated.

    -   NOTE: This is really about Tuple, but I had wrote it up for Array, forgetting that Array has
        uniform element type.  But the concept here is useful to note, so I'm leaving it here for now.

        Projection: Produce a Tuple containing the types of each element of the original Tuple.
        Let the newly produced type Tuple be type_tuple, and let the original be original_tuple.

        Lost context: Which Tuple was projected from and te element values of the Tuple.

        Integration: If there is a natural conversion type_tuple[i] -> typeof(tuple[i]) for each i, then that
        type conversion can be applied to each element of the tuple, producing a new Tuple with the same number
        of elements, but potentially different element types.

    -   Projection: Apply an invertible function T -> U to each element, producing Array<U,size>

        Lost context: Which array was projected from.

        Integration: Apply the inverse of the function to the changed values.

    -   Projection: Produce a graph of numeric array data

        Lost context: Precise numerical values lost in the rendering of the graph
        OR: Use a vector graphics format which preserves the precision
        OR: Use an interactive graph representation where the data points can be modified using some
        mechanism (dragging the plot points around, using some smoothing brush tool, applying some
        overall transformations, etc)

        Integration: In the case of rasterized image format, this is tricky, probably not worth trying,
        but in the case of vector graphics format or interactive form, the numbers could be produced
        back in their original format directly.

        Really this probably represents just a particular kind of view, not an actual projection.

-   Direct in-place transformations of Array -- can just apply, no need to project+change+integrate.
    -   Reverse elements
    -   Shuffle elements using some particular permutation
    -   Shift elements right/left by some number (this is a specific permutation)
    -   Shift a subsequence of elements right/left by some number (e.g. used for inserting an element in middle)
    -   Overwrite all elements with some value
    -   Apply a function T -> T to all elements
    -   Append element(s) (this changes the size of the Array and therefore its type)
    -   Delete element(s) (this changes the size of the Array and therefore its type)
    -   Clear all elements (this changes the size of the Array and therefore its type)

2020.03.15

-   Notes on viewer/editor for ADR
    -   ADR uses an inductive type system.  This implies that the rendering of ADR
        should be done inductively.  In particular, because each data node is further
        composed of "child" data nodes (down to the atomic data types), each data node
        could be considered a "container" for its children, and a natural rendering
        of a given data node can proceed inductively.  Let the visual manifestation
        of a data node be called its "widget".
    -   The widget of a node could/should have some sort of visual frame that displays
        context, such as:
        -   Its type.
        -   Whether or not it's the result of a lossy projection.
        -   UI element that can collapse/expand the widget.

-   View/edit notes for specific data types
    -   Array
        -   Widget frame could/should include type information:
            -   Array element type
            -   Array element count
        -   Possible layouts (each of these is a configurable view option)
            -   Option to cycle back to start for endless, cyclic scroll
            -   Linear
                -   Vertical
                    -   Top to bottom
                    -   Bottom to top
                -   Horizontal
                    -   Left to right
                    -   Right to left
            -   Grid
                -   Layout
                    -   Row major
                    -   Column major
                -   Wrapping vs scrolling
                    -   Wrap based on widget size
                    -   Don't wrap, and instead scroll
            -   Visual stack
                -   Plain 2D rendering
                    -   Show one element at a time
                    -   Show one element with its two adjacent elements
                -   3D effect as if you're flipping through a rolodex
            -   Circular layout of elements (this is automatically cyclic)
        -   Element index could be displayed (or hidden).  This could be done using a widget
            frame which could also have other context information.
        -   Negative element index could be displayed (i.e. last element is -1, etc).
        -   Scrolling should be possible, so that the widget can be confined in size.

-   Rendering implementation notes
    -   Ideally, this would all be 3D and accelerated.  That would make it easiest to make the
        visualizations rich, beautiful, and flexible.  The input and editing may be harder that
        way, because this probably wouldn't be able to use existing stuff like QT's widgets and
        input methods.
    -   QT might be able to do both -- 3D and handle input methods.
    -   Question about rendering: Should it be 3D, where each widget is rendered to its own buffer
        and then rendered into a scene?  Or should the rendering attempt to be directly rasterized,
        which would be slightly less gorgeous, but probably very effective.
    -   What would be SUPER cool and would feel very agile and powerful is to render the ADR
        inductively, but in a manner where you zoom into the elements to explore them, and the
        details fade into existence when you zoom close enough to them.  So this would be a
        level-of-detail sort of thing.  Say you have a complicated ADR that's deeply nested.
        You start fully zoomed out, and you see the overall "shape" of the ADR, but below some
        child level, the details are hidden (to reduce visual clutter, but also processing
        power and memory requirements).  As you zoom in, and the child node grow within the
        view, their details fade into view and resolve.  And as you zoom out and the child
        node grows smaller, its details fade out of view and dissolve.  This could potentially
        be used to view and navigate really big ADRs easily.

        I'm imagining a codebase being layed out in a 2D area as all these boxes denoting the
        various modules.  As you zoom in, the functions and other symbols resolve as boxes within
        those module boxes.  As you zoom in further, the details within those functions start
        to resolve.  The number of levels of detail that you can see at once could be configurable
        too, so like maybe you want to be able to see some amount of detail all the way down to
        the function content level from the seeing-multiple-modules level (this would be nested
        boxes with details within).

        Then instead of looking at the module -> function -> line-of-code hierarchy, you could
        switch to the function call tree, perhaps layed out with profile information such that
        the size of each box would indicate how much time each function call is taking (see
        the "call map" feature of KCacheGrind: http://kcachegrind.sourceforge.net/html/Shot3.html).
        Or you could switch to the class hierarchy, or the type hierarchy, etc.

        One additional consideration is that there should be some way to specify that child widget
        elements should all appear at the same scale (as opposed to child widgets appearing much
        smaller within their parent widget).  This would be necessary for viewing things like
        source code, where you probably want to see all lines of code within a function all at
        the same scale, so it's all readable simultaneously.

2020.03.16

-   QT-based viewer notes
    -   I read that in order to use a custom widget per item in a list (see QListView or
        QListWidget), it's better to use a QScrollArea and then put the custom widgets in there.
        https://forum.qt.io/topic/90194/how-to-use-custom-widgets-in-a-qlistview/2

        Though with thousands/millions of items, who knows if this would be be too much.
        Perhaps it could warn upon too many items, and then suggest that the user switch
        to a different view mode which only shows a smaller range at a time.

    -   Some implementation notes regarding the OpenGL recursive zooming rendering: If it's
        possible to render QT widgets to a buffer, then render that buffer into an OpenGL
        scene, it could be possible to do this rendering naturally and recursively and
        efficiently (depending on how efficient this composition of buffers is).  A
        level-of-detail (LOD) approach would limit how many buffers need to be rendered.

        The very end of this (old, from 2009) article suggests using QGLFramebufferObject:
        https://www.informit.com/articles/article.aspx?p=1405557&seqNum=3

        Looking at modern QT docs, it looks like the relevant class is QOpenGLFramebufferObject.
        It can be rendered to using OpenGL commands, or using QPainter with the help of
        QOpenGLPaintDevice.

        The render method of QWidget has two versions:
        https://doc.qt.io/qt-5/qwidget.html
        -   render(QPaintDevice *target, const QPoint &targetOffset = QPoint(), const QRegion &sourceRegion = QRegion(), QWidget::RenderFlags renderFlags = RenderFlags(DrawWindowBackground | DrawChildren))
        -   render(QPainter *painter, const QPoint &targetOffset = QPoint(), const QRegion &sourceRegion = QRegion(), QWidget::RenderFlags renderFlags = RenderFlags(DrawWindowBackground | DrawChildren))

        One takes QPaintDevice as the target, and one takes QPainter as the target.  One of these
        has got to work.  Presumably invoking render by hand is what's needed.

        Because there's a relationship between the screen pixel dimensions and the texture pixel
        dimensions for any given rendered polygon, there will need to be some logic regarding
        how large of a framebuffer to render each widget into.  The point is that the texture
        pixel density as it appears on screen should be bounded by some function of the screen
        pixel density.

        Once rendering is accomplished, events have to be hooked up.  In particular, mouse events
        have to be correctly transformed to the surface that the click hits.  Focus has to be
        handled as well.  This might be tricky.  Maybe the child widgets can be kept as literal
        child widgets with the same recursive structure, so that focus and keyboard events go to
        the right places, and mouse events will be intercepted so that the coordinates can be
        transformed appropriately.

        The next challenge will be dynamic adding/removing of QWidgets in the hierarchy of widgets
        which represent the visible ADR nodes.  If the LOD mechanisms function as intended, then
        this should be very doable, it just requires rigorous bookkeeping.

        See section on Threading in QOpenGLWidget class docs!  In particular, rendering to frame
        buffer objects in worker threads.

    -   Found a demo of this concept:

            https://www.youtube.com/watch?v=MXS3xKV-UM0

        This links to a non-existent trolltech blog entry, but the internet archive has it:

            https://web.archive.org/web/20081217172153/http://labs.trolltech.com/blogs/2008/12/02/widgets-enter-the-third-dimension-wolfenqt/

        The relevant QT classes are QGraphicsScene, QGraphicsView, QGraphicsItem, QGraphicsProxyWidget

2020.03.22

-   TODO
    -   Figure out a way to use drag hand scrolling that isn't interfered with by selectable
        QLabel or QTextEdit.
    -   Use the mouse cursor as the zoom center
    -   Allow scrolling past the content of the QGraphicsView, so that you can center on whatever
        you want.

-   Ideas for GUI visualization and interaction
    -   There should be a concept of "visually focusing on" a particular ADR node, where the view
        brings that node front and center, fully zoomed up if appropriate, and doesn't show any
        of its ancestor or sibling nodes.  This can have 2 visual effect modes:
        -   Smooth zoom in, where the ancestors and siblings fade to black (or not).  This would
            be very pretty but would take nonzero time.  It should be possible to "queue" these
            focus actions while zooming, so that you can continue with your interaction even during
            the pretty effects.  The zooming would just smoothly proceed to the new target.
        -   Instant zoom in, which would be less pretty, but very fast and simple.

        A few ideas on how to facilitate this from a UI standpoint:
        -   It should be very easy to click on the desired data node.  To this end, there should be
            some well-defined area for that node that is clickable that would visually focus that
            data node.  Perhaps there can be a keyboard shortcut for temporarily showing a bigger
            version of that clickable area (or toggling big/normal size), to make it even easier.
        -   In Chrome's developer tools, there's this great UI element selector that highlights the
            UI element under the mouse cursor.  This could be used, though it would improve the UX
            if it were guaranteed that the frame for each data node were nonzero (it's hard to select
            some nodes in Chrome's UI, since they're not intended to be used that way).  Again,
            this could be facilitated by toggling some "grow frame" flag.
        -   Random half-baked idea: Because these ADR structures are typically hierarchical, it could
            be useful to think of them as being like stacked boxes, each one having a "height".  A
            way to filter the selection possibilities would be to have some scrollable "height"
            selector, where the data nodes of that "height" are highlighted somehow, so then you
            can just click anywhere on that data node to select it, instead of being careful.

        There should also be keyboard shortcuts (and something like "tab order") so that this
        sort of navigation can be done entirely with the keyboard also.

2020.03.24

-   TODO: Simple proof of concept -- just create standard widget layouts for the ADR for now, not
    worrying about the fancier dynamic allocation or recursive QGraphicsViews.  Still use a
    QGraphicsView at the top level, so that it's easy to zoom and scroll though.

2020.03.26

-   Notes on design for Array.  There are 2 considerations here:
    -   Array constraints:
        -   Element type and array length
        -   Element type
        -   Array length
        -   No constraint
    -   Upon construction of an Array, there should be a specification of a constraint.  E.g.

            // Two examples of a-priori constraint, checked at the end of constructor
            Array(Constraint(ArrayTypeES(AtomicType::DOUBLE,3)), 4.0, 8.0, -1.0);
            Array(Constraint(ArrayTypeE(ArrayType)),
                std::make_any<Array>(6.0, 123, true),
                std::make_any<Array>(0.0, 0.0, 111, 123, true, false),
                std::make_any<Array>(Constraint(ArrayTypeS(4)), true, 5, false, 6.01)
            );

            Array();                                            // Empty array, no constraint here.
            Array(100, 200, 250, 260);                          // No constraint here
            Array(true, 555.001, 25, std::make_any<Array>(85)); // No constraint here
            Array(6.06, 7.89, true).constrain(ArrayTypeS(3));   // Constraint applied afterward
            Array(6.06, 7.89, true).constrain<ArrayTypeS>();    // Constraint computed and applied afterward
                                                                // (computed to be ArrayTypeS(3))
            Array(6.06, 7.89, true).constrain<ArrayTypeES>();   // Constraint computed and applied afterward
                                                                // (computed to be ArrayTypeES(AtomicType::ANY,3))
            Array(6.06, 7.89, true).constrain<ArrayTypeE>();    // Constraint computed and applied afterward
                                                                // (computed to be ArrayTypeE(AtomicType::ANY))

        In order for this to work with the std::any-typed element parameters, a struct Constraint represents the
        constraint to be used.  It must not allow use within std::any, and therefore making it not be
        copy-constructible is sufficient.

2020.03.27

-   QT's Simple Widget Mapper demo example looks promising in terms of making custom GUI based on data,
    though it looks like it can only show one item at a time, which is not awesome.

    QT's Editable Tree Model looks promising in that it can display and edit hierarchical data in
    a very reasonable form.  This could be adapted to view and edit many many types of ADR, since
    it's basically all hierarchical.  In other words, this could be a very basic editing mode for
    many types of ADR, among other more specialized modes.

    The QT Model/View architecture document is where it's at.  The QAbstractItemDelegate is the
    correct class to subclass in order to provide custom rendering for the element.

    QItemDelegate is also interesting, because it has the method createEditor, in which a QWidget
    is returned for a particular model item in order to provide editing capabilities for that item.
    Note that QAbstractItemDelegate also has this createEditor functionality.

    The notion of "proxy models" in the QT documentation seems to be peripherally related to a projection,
    in that it can give some processed form of the data which then multiple views can attach to and
    display.

-   QT's Syntax Highlighter example could be useful.

2020.03.28

-   Subtyping notes

        enum class AtomicType {
            ANY = 0, // Supertype of everything.  I.e. is_instance(d, AtomicType::ANY) is always true.
            TYPE, // Inhabitants are all types.  Should be the union of ATOMIC_TYPE and INDUCTIVE_TYPE
            ATOMIC_TYPE, // E.g. AtomicType::ANY, ::TYPE, ::ATOMIC_TYPE, ::INT etc are inhabitants.
            INDUCTIVE_TYPE, // Non-atomic types; Inhabitants are hierarchical data nodes which may themselves be types.
            ARRAY_TYPE_ES, // Type of ArrayTypeES(T,n) for all types T and integers n.
            ARRAY_TYPE_E, // Type of ArrayTypeE(T) for types T.
            ARRAY_TYPE_S, // Type of ArrayTypeS(n) for all integers n.
            ARRAY_TYPE, // Type of ArrayType(); Supertype of all array types.
            INT,
            DOUBLE,
            CHAR,

            __LOWEST__ = ANY,
            __HIGHEST__ = CHAR
        };

    `x : T` will denote "x is an instance of T"; TODO: Think about if this is concrete or abstract
    `T1 <= T2` will denote "T1 is a subtype of T2", meaning there is a canonical embedding of T1 into T2
    `T1 < T2` will denote "T1 is a strict subtype of T2".
    `typeof(x)` will denote the "minimal type" of x -- call this abstract type?
    `reprof(x)` will denote the representation type of x -- call this concrete type?

        ANY = Union(typeof(x) for all x) // x is anything
        T <= ANY for any type T
        T : ATOMIC_TYPE is true for T in {ANY, TYPE, ATOMIC_TYPE, INDUCTIVE_TYPE, etc}
        ATOMIC_TYPE < TYPE
        INDUCTIVE_TYPE < TYPE


        // Numeric

        SINT8 < SINT16 < SINT32 < SINT64 < SINT
        UINT8 < UINT16 < UINT32 < UINT64 < UINT
        SINT < INT
        UINT < INT

        FLOAT32

-   Regarding AtomicType

    Each of these types are non-parametric, meaning that each of these types is a singleton.
    To contrast, an example of a parametric type is ArrayTypeES(AtomicType::DOUBLE, 4).
    TODO: Should this be renamed to NonParametricType?  Then should INDUCTIVE_TYPE be called
    PARAMETRIC_TYPE?  Though perhaps Atomic and Inductive are better.

    NOTE: The only reason this enum exists is because types aren't first class values in C++,
    meaning that you can't do something like

        auto type1 = int;
        auto type2 = std::any;

    TODO: Maybe name these Any, NonType, AtomicNonType, ArrayTypeES, etc so they match up with
    various class names exactly.

    QUESTION: This enum shouldn't really be called AtomicType, because maybe it contains inductive types?
    QUESTION: What exactly is an inductive type?  An inductive type "is" the constructor for non-atomic
    non-types (e.g. ArrayType() is an inductive type with e.g. inhabitant Array(1,2,3); ArrayTypeESType() is an
    inductive type with e.g. inhabitant ArrayTypeES(int,3)).

    TODO: Rewrite verbiage to use "term" instead of "data" or "data node" or "value".
    See https://en.wikipedia.org/wiki/Type_theory

    Observation: Array(1,2,3) has "concrete/natural/actual type" ArrayTypeES(int,3), but could be
    interpreted as a different type using some constructor like Term(Array(1,2,3), Sequence), where
    the type of Term(Array(1,2,3), Sequence) would be Sequence, and the "concrete type" of
    Term(Array(1,2,3), Sequence) would be the concrete type of Array(1,2,3).

-   Regarding terms, each finite term effectively has a representation as a typed tuple, where the
    0th element always carries the representation type.  This way, there's always a forgetful
    transformation that can produce that tuple from any given term, and this tuple could be called
    its representation.

    For example:
    -   Array(1,2,3) could be thought of as Term(ArrayTypeES(int,3), (1,2,3)).  Presumably this
        would extend to the atomic types as well, so 1 is really Term(int,1), and Array(1,2,3) is
        really

            Term(Term(ArrayTypeES, ((AtomicType,int),())), ((int,1),(int,2),(int,3)))

        where in the expression Term(T,V), the concrete types of T and V are Type and Value.  Maybe
        use nicer syntax, like

            V:T

-   TODO: Rename Data to Term probably.

-   Implementation notes for creating QT-based model/view for ADR.
    -   Which data types will be supported at first?
        -   int
        -   double
        -   bool
        -   Array
    -   Adding/removing elements to/from Array is nontrivial (due to how ADRItem has to effectively
        track a pointer to each element).  Could potentially simplify this.
    -   Adding/removing elements from other stuff like set or map should be easier, since that
        doesn't necessarily invalidate any iterators.

2020.03.30

-   Regarding patches/diffs/merges, https://en.wikipedia.org/wiki/Operational_transformation
-   Very interesting approach to conflict-free data merging: https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type

2020.03.31

-   Regarding customized viewing/editing of items in QTreeView, in particular for double-valued items:
    https://forum.qt.io/topic/71692/using-setdata-qt-editrole-xxx-with-qtablewidgetitem/4

    A similar discussion was found regarding customizing the behavior for boolean-valued items.

    It makes sense to do some custom implementations of these.  In particular, each data type has
    several different natural view/edit modes.  For example:
    -   floating point values
        -   decimal representation (flawed, because base 10 decimals don't map onto floating point perfectly)
        -   scientific notation in decimal
        -   hex representation (gives an exact representation)
        -   scientific notation in hex
        -   binary representation?  Could be funny and potentially useful.
        -   scientific notation in binary?
        -   only the mantissa
        -   only the exponent
        -   only the sign
    -   boolean values
        -   checkbox
        -   True/False dropdown
        -   Different labels for true/false in a dropdown, e.g. + and -
    -   signed integer values
        -   decimal (use commas?)
        -   hex (use spaces after 4 digits?)
        -   binary (use spaces after 8 digits?)
        -   octal
        -   other bases?
        -   raw representation in whatever base (as a two's complement int)
        -   only the sign (can be edited unless there is no corresponding negated value
            e.g. -2^31 vs 2^31 as a 32 bit signed int; -2^31 is representable but 2^31 isn't.
        -   is nonzero (doesn't have a natural editing mode)
    -   unsigned integer values
        -   decimal (use commas?)
        -   hex (use spaces after 4 digits?)
        -   binary (use spaces after 8 digits?)
        -   octal
        -   other bases?
        -   is nonzero (doesn't have a natural editing mode)
    -   array values (really the "parent node" of the array)
        -   default display should show the number of elements (like `Array of size #`)
        -   in-line display (like `Array(1, 2, 3)`), which should probably be of limited length
    -   arrays of char values (asciichar, unicode are distinct from integer types)
        -   default display should just show it as a string, in-line.
        -   "expanded" where it displays as a generic array (`Array of size #`) and expands
            to show each element individually.
    -   timestamp
        -   human readable form (many different conventions)
        -   unix epoch time (in whatever precision; seconds, nanoseconds, etc).

-   These different view configurations should be easily switchable via:
    -   Keyboard shortcuts
    -   Menu actions
    -   A tool/sidebar "palette"

-   Implementation plan
    -   Implement a couple of the above, like
        -   floating point values
            -   decimal representation (flawed, because base 10 decimals don't map onto floating point perfectly)
            -   scientific notation in decimal
            -   hex representation (gives an exact representation)
            -   scientific notation in hex
        -   signed integer values
            -   decimal (use commas?)
            -   hex (use spaces after 4 digits?)
            -   binary (use spaces after 8 digits?)

-   TODO: Looks like qvariant_cast<> is probably the correct way to handle the type cases
    for QVariant, not toBool, toInt, etc.

2020.04.04

-   TODO
    x   Clean up array classes (get rid of unused ones)
    -   Implement a few projections and integrations -- do this in C++ only, not in GUI yet.
        -   "type of array" projection (produce e.g. ArrayTypeES(int,6) or ArrayTypeS(10)); integration
            is either a type cast on the elements or a type assertion, and/or truncation/extension of
            the array -- this depends on if the array element type is an abstract or concrete type.
            Not sure how well defined this one is.  Probably for now it should just be a type assertion.
        -   "array of types" projection (produce an array whose elements are the types of each
            element of the source array); integration is a type cast
        -   "extract element from array" projection; integration is updating the extracted element
        -   "extract subset of elements from array" projection; context is extracted element indices;
            integration is updating the extracted elements.
        -   "histogram of array" -- counts up the number of occurrences of each unique element and
            produces (for now) an array with pairs (source-value, occurrence-count).  It is not in
            general possible to integrate changes to this projection without additional structure or
            assumptions.  Actually I take that back -- the source-value can be changed.  The context
            for this has to be, for each source-value, the array of index values that that source-value
            occurred at.  Could furthermore apply some equivalence process to the input elements, so
            that certain elements would be considered equal (e.g. when doing analysis on log entries,
            you probably want to ignore certain things like the timestamp and other stuff).
    -   Add/remove Array elements -- this probably involves figuring out a better indexing scheme for
        ADRItem; perhaps keep a pointer to the "parent" and then the index into that parent (which could
        be a size_t or a key in the case of a Map).
-   Ideas for references
    -   There should be many kinds of references, not just ones that represent pointers to other parts of
        this ADR document or a related one, but even stuff like a URL/URI or IPFS address.
    -   There should be some path-like indexing scheme for addressing subelements of a data node, analogous
        to the syntax used in jq (JSON query CLI tool) or yq (YAML query CLI tool).
-   Ideas for custom Data class
    -   Data can inherit std::any and its functionality
    -   There can be a templated subclass of Data which adds no new members (so is bijectively
        castable back and forth between Data) except that it provides a type assertion for the
        named type, and a convenient cast operator, so it's less of a pain in the ass to use
        (e.g. std::any_cast<T const &>(x) etc)

2020.04.05

-   TODO
    -   Implement custom Data class (there seems to be more problems with `using Data = std::any` within
        namespace sept and the use of operator== overload).
    -   Change type::Conversion to use raw data types, instead of Data (i.e. std::any), and make the
        Data-using versions a layer on top.  Note that this would be much more template heavy, and
        might be more work than is worth at the moment.

2020.04.06

-   IDEA: Compile errors can project the offending code into the error log (in fact, logs should just
    be a series of ADR nodes anyway).  You can edit the code in the error log in-place, and then integrate
    the changes, all using the same projection/integration abstraction!  Awesome!

2020.04.11

-   TODO
    x   Replace std::any_cast<T_> with calls to Data::cast<T_>.
    -   Think about making a wrapper `Type` for Data which is just used as a strong type to indicate
        the semantic use of a particular Data argument, e.g. in Conversions::conversion_for.  Then
        the function sept::type_of(Data const &) can return Type.
    -   Create hierarchical conversions, like one that produces an identical hierarchical Data structure
        except where the leaf nodes are the types of the corresponding source leaf nodes.
    x   Add support for adding/removing Array elements in septview.
-   Conversions for type classes such as ArrayTypeES(T,N) are tricky, because of the parameters T and N.
    Thus there needs to be a way to create these conversions on demand.  Some other scheme for dynamically
    determining if a conversion exists is necessary.
-   Observation regarding the confusion over AtomicType -- really what I'm trying to get at with AtomicType
    is "non parametric" type.  Examples are:
    -   All the POD types (e.g. int64_t, double, char, bool),
    -   ArrayType() (i.e. the type of Array(1,2.0,true))
    -   ArrayTypeES (i.e. the type whose inhabitants have the form ArrayTypeES(T,N) for some T and N),
    -   ArrayTypeE (i.e. the type whose inhabitants have the form ArrayTypeE(T) for some T),
    -   ArrayTypeS (i.e. the type whose inhabitants have the form ArrayTypeS(N) for some N),
    -   ArrayType (i.e. the type whose inhabitant is ArrayType())
    -   Term (everything is a term; aka "Any")
    -   Type (a Term that has inhabitants -- QUESTION: Can/should there be an empty type?  If so, this would
        contradict the definition of NonType)
    -   NonType (a Term that isn't a Type, i.e. has no inhabitants; e.g. 1 or 2.0 or true)
    -   PODType (all the POD types e.g. int64_t, double, char, bool, etc).
    -   NonPODType (all non-POD types)
    -   NonParametricType itself (whose inhabitants require no parameters to construct; they can be thought
        of as singly-named types; they are each of these listed)
    -   ParametricType (whose inhabitants are e.g. ArrayTypeES(T,N), ArrayTypeE(T), ArrayTypeS(N), i.e.
        require parameters to construct)


    Inhabitation relationships (child items are inhabitants)
    -   PODType -- inhabitants are types whose inhabitants are POD
        Could also call this LeafType (meaning that inhabitants are Leaf terms, meaning that they
        aren't composed of other things)
        -   int64_t
            -   -1
            -   0
            -   1
            -   2
        -   double
            -   0.25
            -   -1.0e-7
        -   char
            -   'a'
            -   '\t'
        -   bool
            -   true
            -   false
        -   VoidType
            -   void -- has 0 size, contains no information.
    -   POD could be UnionType(int64_t, double, char, bool, ...)
        Could also call this Leaf
        -   -1
        -   0
        -   1
        -   2
        -   0.25
        -   -1.0e-7
        -   'a'
        -   '\t'
        -   true
        -   false
        -   void
    -   EmptyType -- its inhabitation predicate always returns false; this type is empty.
    -   ArrayType
        -   ArrayType() -- this is the sole inhabitant of ArrayType
            -   Array(1,2,3,'a')
            -   Array(5,6,7,8)
            -   Array(true, false, 3.0)
    -   ArrayTypeES
        -   ArrayTypeES(int32_t,3)
            -   Array(1,2,3).with_constraint(ArrayTypeES(int32_t,3))
    -   ArrayTypeE
        -   ArrayTypeE(int32_t)
            -   Array().with_constraint(ArrayTypeE(int32_t))
            -   Array(1,2,3).with_constraint(ArrayTypeE(int32_t))
            -   Array(5,6,7,8).with_constraint(ArrayTypeE(int32_t))
    -   ArrayTypeS
        -   ArrayTypeS(3)
            -   Array(1,2,3).with_constraint(ArrayTypeS(3))
            -   Array('a',2.01,true).with_constraint(ArrayTypeS(3))
    -   UnionType
        -   UnionType(int64_t,double)
            -   3
            -   10003
            -   78.901
        -   UnionType() ~ EmptyType
        -   UnionType(ArrayTypeES(int64_t,4), ArrayType(double,4))
            -   etc
    -   IntersectionType
        -   IntersectionType(T1,...,TN) -- inhabitants are inhabitants of all types T1,...,TN
        -   IntersectionType() -- all inhabitants (i.e. same as Term)
    -   Term -- literally everything is a Term
    -   Type -- its inhabitants are Terms that have inhabitants (this may not be the right definition of Type;
        maybe use a structure to define Type -- a Type has a well-defined inhabitation predicate (i.e. for
        a given Term x, one can decide if x is an inhabitant of T, which is denoted as x : T).  This allows
        for an Empty type.
        -   int64_t
        -   double
        -   char
        -   VoidType
        -   ArrayTypeES
        -   ArrayTypeES(T,N)
        -   UnionType
        -   UnionType(int64_t,char)
        -   Type
        -   NonParametricType
        -   ParametricType
        -   NonType
        -   EmptyType -- this is a type having no inhabitants
    -   NonType -- its inhabitants are Terms which are not Types (i.e. have no inhabitation predicate and
        therefore no inhabitants)
        -   -99
        -   100003
        -   50.65
        -   true
        -   void
        -   Array(0,1,4,9,16)
    -   NonParametricType -- types whose instantiation require no parameters
        -   PODType
        -   POD
        -   int64_t
        -   double
        -   char
        -   bool
        -   VoidType
        -   ArrayType
        -   ArrayType() -- could be called Array
        -   ArrayTypeES
        -   ArrayTypeE
        -   ArrayTypeS
        -   UnionType
        -   UnionType()
        -   ParametricType
        -   NonParametricType
        -   Term
        -   Type
        -   NonType
        -   EmptyType
    -   ParametricType -- could be thought of as a union of all types which require parameters to instantiate.
        -   ArrayTypeES(int64_t, 4)
        -   ArrayTypeES(double, 2)
        -   ArrayTypeE(char)
        -   ArrayTypeE(bool)
        -   ArrayTypeS(10)
        -   ArrayTypeS(200)
        -   UnionType(int8_t,int16_t)
        -   UnionType(ArrayTypeE(double), ArrayTypeE(int64_t))

    Subtype relationships -- X <= Y means that X is a subtype of Y.  X < Y means X is a proper subtype of Y.
    By definition X <= Y if all inhabitants of X are also inhabitants of Y.
    -   int64_t < POD
    -   double < POD
    -   char < POD
    -   bool < POD
    -   ArrayTypeES(T,N) < ArrayTypeE(T)
    -   ArrayTypeES(T,N) < ArrayTypeS(N)
    -   ArrayTypeE(T) < ArrayType()
    -   ArrayTypeS(N) < ArrayType()
    -   T0 <= UnionType(T0,T1,...)
    -   T1 <= UnionType(T0,T1,...)
    -   UnionType() <= T for all types T (i.e. UnionType() is empty)
    -   IntersectionType(T0,T1,...) <= T0
    -   IntersectionType(T0,T1,...) <= T1
    -   T <= Term for all types T
    -   EmptyType <= T for all types T
    -   ParametricType < Type
    -   NonParametricType < Type

2020.04.13

-   TODO
    -   Change keys in QTreeView to provide better UX
        -   Make enter key edit
        -   Make T and F keys instantly change bool (instead of entering edit mode)
        -   Make a key toggle bool
        -   Make + and - keys do stuff (add/subtract one to/from numeric types, probably also set true/false)
    -   Copy, cut, and paste of values -- need to somehow figure out how to handle different columns
    -   Add ability to edit multiple values at once (maybe do this using an interactive dialog)
    -   Add ability to copy a single value over many selected values
    -   Show root ADR node in QTreeView
    -   Make a way to define the type of value being inserted
    -   Make an interactive value insert action (brings up a dialog?)
    -   Make the type column editable, where there's a dropdown box to select types.
        Probably try to indicate which different types would produce a non-lossy conversion.
-   Idea: In analogy of abstract vs concrete type (where concrete type is the type of the representation),
    it would be useful to have a notion of abstract vs concrete value.  For example, if

        TypeFromArray(A)

    is a way to define a Type whose inhabitants are exactly the members of A, then the actual representation
    of an inhabitant of that type has the form

        Term(x, TypeFromArray(A))

    where x : A.  `Term(x, TypeFromArray(A))` is the "concrete value", whereas the abstract value is just x.
    However, this should probably extend to TypeFromArray(A) as well, which would be its concrete value,
    whereas A would be its abstract value.

    There is a commutative diagram that should be satisfied here.

        Term(x, TypeFromArray(A))  ---abstract_value_of--->   x

            |                                                 |
     concrete_type_of                                  concrete_type_of
            |                                                 |
            V                                                 V

        TypeFromArray(A)           ---abstract_value_of--->   A

    QUESTION: abstract_type_of(y) would be abstract_value_of(concrete_type_of(y)).

    Let

        P1 := TypeFromArray(S1)
        P2 := TypeFromArray(S2)
        U := UnionType(P1,P2)
        h1 : P1 // Meaning that h1 == Term(s1, P1) for some s1 : S1

    then

        Term(h1, UnionType(P1,P2))  ---abstract_value_of--->   h1

            |                                                  |
     concrete_type_of                                   concrete_type_of
            |                                                  |
            |                                                  V
            |
            V                                                  P1

        UnionType(P1,P2)  ---abstract_value_of---> UnionType(P1,P2)

    Let w1 := Term(h1, UnionType(P1,P2)) for brevity.

    This is a bit of a problem, as it shows that the diagram can't exactly commute, but it's certainly
    true that

        abstract_value_of(concrete_type_of(w1)) <= concrete_type_of(abstract_value_of(w1))

    Perhaps this isn't the right definition of UnionType, and it should be considered more of a disjoint
    union, where which of the summand types P1,P2 must be specified, so it would be something like

        UnionTerm(h1, SubType(P1, UnionType(P1,P2)))

    in which case, the diagram can commute, where abstract_value_of(SubType(T1, T2)) := T1.  Though
    this is also surprising, because then concrete_type_of(h1) is not UnionType(P1,P2) as one might
    expect.  Arguably UnionType(...) isn't a concrete type, so this shouldn't be expected to work,
    but on the other hand, making exceptional types sucks.

    QUESTION: Should the function abstract_value_of unwrap all the way?  If there were nested constructions
    like this, it wouldn't be obvious exactly where to stop, or that unwrapping all the way would be
    desirable.  Could potentially solve this by specifying some "decay policy" where some specification
    of the "floor" of the unwrapping could be specified.  Maybe as a subset of the partial order of
    the type hierarchy?  For example,

        v1 : S1
        z1 := Term(v1, P1)
        u1 := Term(z1, U)

    Then what should the following expressions evaluate to?
    -   abstract_value_of(u1)
        -   If the answer is z1 (which is really Term(v1, P1)), then this is "do one unwrap"
        -   If the answer is v1, then this is "do all unwraps"
    -   abstract_type_of(u1)
        -   If the answer is

    [handwritten notes ...]

    Summary:
    -   There are two syntaxes for Terms.  One, which is easy to use and abuse, could be considered
        human-level, informal syntax, which is not accurate regarding the actual representation of
        that term.  The other is the formal syntax, which accurately portrays the true representation
        of a term, and there can be used to make formal definitions of functions and not lie to
        oneself about the well-definedness of things.

            Repr(v, R)

        is the basic construction for a value v whose true representation type is R.  The representational
        type defines how the value is represented in memory or whatever medium is being used.

        A Term though can have a declared "abstract type", which can be different than the repr type,
        and has the form

            Term(Repr(v, R), T),

        where T is any type.  However, T itself must be a repr, so

            T = Repr(x, X) for some value x and representational type X.

        Is there a formal definition of a representational type vs a non-representational type?
        Obvious representational types are
        -   UintN and SintN for N in {8, 16, 32, 64}
        -   FloatN for N in {32, 64}
        -   Bool
        -   Char
        -   ArrayType() (its inhabitants are arrays of any element type and size)
        -   ArrayTypeES(T,N) (its inhabitants are arrays of element type T and size N)
        -   ArrayTypeE(T) (its inhabitants are arrays of element type T)
        -   ArrayTypeS(N) (its inhabitants are arrays of size N)
        -   UnionType (its inhabitants are UnionType(T1,...,TN))
        -   IntersectionType (its inhabitants are IntersectionType(T1,...,TN))
        The last few, the ones with parameters are tricky.  What are their representations exactly?
        Take an array a with type ArrayTypeES(T,N).  As a Repr it would be

            Repr(a, Repr(ArrayTypeES(T,N), ArrayTypeES))

        this seems valid from a C++ perspective because this dictates a memory layout exactly.
        However, this shows that a representational type can have parameters.  But clearly the
        nesting of Repr has to end somewhere with a non-parametric type (which in this case is
        ArrayTypeES).

        Are the following non-representational types?
        -   UnionType(T1,...,TN) -- what is is the memory layout of a type?  It's the memory layout of
            T1 or ... or TN.  Can this be somehow represented?  Say t1 : T1.  Then
            t1 : UnionType(T1,...,TN) abstractly (this is informal notation), which actually is

                Repr(UnionType(T1,...,TN), UnionType)

            But t1 is

                Repr(t1, T1)

            To deal with t1 as an inhabitant of UnionType(T1,...,TN), write

                Term(t1, UnionType(T1,...,TN))

            which is actually shorthand for

                Term(Repr(t1, T1), Repr(UnionType(T1,...,TN), UnionType))

            But there's a couple of functions (also discussed in next bullet item) which are useful:
            -   The value of a Term (first element)
            -   The type of a Term (second element)

            The goal with any type construction (e.g. UnionType(T1,...,TN)) is to make a diagram
            commute.  Let U := UnionType(T1,...,TN) for brevity.  Then

                              val
                Term(t1, U)  ---->  t1

                    |                  \
               type |                   \
                    V                    \

                    U        ---->  U  >=  T1
                              val

            But really we want this diagram to commute, not just satisfy a subtype relationship.
            Let X and Y be types, and assume that X <= Y.  Let Narrowed(X,Y) mean the type X explicitly
            as a subtype of Y.  Try the above again.  Let N := Narrowed(T1,U).  Define the value of
            Narrowed(X,Y) to be X (i.e. the "narrowed", more-specific type).

                              val
                Term(t1, N)  ---->  Term(t1, T1) // T1 came from Narrowed(T1,U); this is just the same as t1 itself

                    |                |
               type |                | type
                    V                V

                    N        ---->  T1
                              val

            Now work with I := IntersectionType(T1,...,TN).  Let N := Narrowed(I,T1) (here, I comes first
            because it's the narrowed, more-specific type).

                              val
                Term(t1, N)  ---->  Term(t1, I) // The type was taken from Narrowed(I,T1)

                    |                |
               type |                | type
                    V                V

                    N        ---->   I
                              val

    -   It will help to make the notation formal and precise.  Let "abstract notation" be as follows.

        There are a set of identifiers that name particular terms.  Each of these has a well-defined
        "representational type".  For example, some terms and their representational types:
        -   Sint32            with RT NonParametricType
        -   Bool              with RT NonParametricType
        -   ArrayTypeES       with RT NonParametricType
        -   UnionType         with RT NonParametricType
        -   NonParametricType with RT NonParametricType
        -   Void              with RT NonParametricType
        Arguably the following literals could be included, though on the other hand, the literal text
        could be considered the parameter.
        -   3                 with RT Sint64 (or whatever the literal `3` is defined to be)
        -   10.25             with RT Float64 (or whatever the literal `10.25` is defined to be)
        -   true              with RT Bool
        -   "hippo"           with RT ArrayTypeES(AsciiChar,5)

        Some of these terms are types, which means there is an inhabitation predicate function defined
        for that term (i.e. if T is a type, then there is a well-defined function which determines if
        any given term is an inhabitant of T or not).  For example:
        -   Sint32
        -   Uint64
        -   Bool
        -   ArrayTypeES
        -   ArrayTypeES(T,N)
        -   ArrayType
        -   ArrayType()
        -   UnionType
        -   UnionType(T1,...,TN)
        -   VoidType
        -   EmptyType

        Some of these terms are parametric, meaning that to instantiate them, some parameters must be
        specified.  For example
        -   ArrayTypeES -- inhabitants have the form ArrayTypeES(T,N) for some type T and Uint N.
        -   ArrayTypeES(T,N) -- inhabitants have the form Array(...) satisfying element type T and length N.
        -   UnionType -- inhabitants have the form UnionType(T1,...,TN).
        -   ArrayType() -- inhabitants are Array(...).

        So there are a few syntactical distinctions:
        -   NonParametric Term -- a Term that is instantiated with a fixed identifier.
        -   Parametric Term -- a Term that is instantiated with a sequence of parameters (potentially
            this includes the empty sequence, e.g. for Array())

        There is a notion of the "concrete type" of a Term, which is more or less what the type of
        its most basic concreteesentation is.  This maps directly on to how that term is concreteesented in
        memory or on disk.  So these fixed identifiers in the abstract notation map onto an object
        of the form

            ConcreteTerm(ConcreteType, TermParams)

        where this currently corresponds to the sept::Data class, where ConcreteType is stored in a
        std::type_info field (see std::make_any and std::any_cast) and the TermParams are whatever
        was passed to the constructor of that object (be it an integer value or array elements or
        whatever).

        Some examples of the mapping between abstract notation (left) and ConcreteTerm form (middle) and C++ form (right)
        -   Sint32              <-> ConcreteTerm(NonParametricType, Sint32)         <-> make_data<NPType>(NPType::Sint32)
        -   Bool                <-> ConcreteTerm(NonParametricType, Bool)           <-> make_data<NPType>(NPType::Bool)
        -   ArrayTypeES         <-> ConcreteTerm(NonParametricType, ArrayTypeES)    <-> make_data<NPType>(NPType::ArrayTypeES)
        -   ArrayTypeES(T,N)    <-> ConcreteTerm(ArrayTypeES, (T, N))               <-> make_data<ArrayTypeES>(T,N)
        -   ArrayType           <-> ConcreteTerm(NonParametricType, ArrayType)      <-> make_data<NPType>(NPType::ArrayType)
        -   ArrayType()         <-> ConcreteTerm(ArrayType, ())                     <-> make_data<ArrayType>()
        -   Array(1,2,3)        <-> ConcreteTerm(ArrayType(), Sint32)               <-> make_data<Array>(1,2,3)
                                                 ^^^^^^^^^^^ problem, this is not a NonParametricType

        To resolve this, the following will probably work:
        -   Drop the "Type" suffix from certain things, in particular:
            -   Array(1,2,3) stays the same
            -   ArrayType() -> Array
            -   ArrayTypeES(T,N) -> ArrayES(T,N)
            -   Then an inhabitant of ArrayES(T,N) can be instantiated as ArrayES(T,N)(1,2,3,...)
            -   UnionType -> Union
            -   Then an inhabitant of Union can be instantiated as Union(T1,...,TN)

        If this is done, then the lines above become
        -   ArrayES             <-> ConcreteTerm(NonParametricType, ArrayES)            <-> make_data<NPType>(NPType::ArrayES)
        -   ArrayES(T,N)        <-> ConcreteTerm(ArrayES, (T,N))                        <-> make_data<ArrayES>(T,N)
        -   ArrayES(T,N)(4,5)   <-> ConcreteTerm(ConcreteTerm(ArrayES, (T,N)), (4,5))   <-> Data(ArrayES(T,N)(4,5))
                                    ^ This could potentially be thought of as
                                    ConcreteTerm(ArrayES, (T,N), (4,5))
                                    where this represents repeated ()-based instantiation.
        -   Array               <-> ConcreteTerm(NonParametricType, Array)              <-> make_data<NPType>(NPType::Array)
        -   Array()             <-> ConcreteTerm(Array, ())                             <-> make_data<Array>() // empty array
        -   Array(1,2,3)        <-> ConcreteTerm(Array, (1,2,3))                        <-> make_data<Array>(1,2,3)

        This then gives a direct way to get the concrete type of a concrete term.

        As for non-concrete terms, i.e. a term with a type declaration tacked on (to either narrow or widen
        the type), abstractly, that has the form

            AbstractTerm(T, v)

        where T is the declared type and v is the value, where v : T must hold.  Also introduce a kind of Term that
        is an instantiation of a particular type.

            Inst(Type, Params)

        where Type could be a ConcreteType or an AbstractType, and Params are whatever are needed.  E.g.
        -   Inst(ConcreteType(ArrayES, (T,N)), (4,5,6,7,8)) // Produces ArrayES(T,N)(4,5,6,7,8)
        -   Inst(ConcreteType(Union, (Sint32,Bool)), Sint32(8)) // Produces Union(Sint32,Bool)(Sint32(8))
        Is this really just equivalent to AbstractTerm?  Probably not, because (4,5,6,7,8) by itself is
        not a ConcreteTerm.  So perhaps ConcreteTerm is actually a recursive type, meaning that in

            ConcreteTerm(T, v)

        Either the concrete type of T is NonParametricType, or T itself is a ConcreteTerm.  Thus let

            NonParametricType(T) -- or NonParametricTerm(T), so that it can include e.g. Void?  NO, since void
            is not a type; use ConcreteTerm(VoidType, Void) for that.

        denote this "base case".

        Define certain functions on ConcreteTerm and AbstractTerm:
        -   concrete_value_of
            -   concrete_value_of(NonParametricType(T)) := NonParametricType(T)
            -   concrete_value_of(ConcreteTerm(T, v)) := ConcreteTerm(T, v)
            -   concrete_value_of(AbstractTerm(T, v)) := concrete_value_of(v)
        -   concrete_type_of
            -   concrete_type_of(NonParametricType(T)) := NonParametricTerm(NonParametricTerm)
            -   concrete_type_of(ConcreteTerm(T, v)) := ConcreteTerm(NonParametricType, T)
            -   concrete_type_of(AbstractTerm(T, v)) := concrete_type_of(v)
        -   abstract_type_of
            -   abstract_type_of(NonParametricType(T)) := concrete_type_of(NonParametricType(T))
            -   abstract_type_of(ConcreteTerm(T, v)) := concrete_type_of(ConcreteTerm(T, v))
            -   abstract_type_of(AbstractTerm(T, v)) := T
        -   Is there any reasonable notion of "abstract_value_of"?  Maybe -- not sure if it's useful:
            -   abstract_value_of(NonParametricType(T)) := NonParametricType(T)
            -   abstract_value_of(ConcreteTerm(T, v)) := ConcreteTerm(T, v)
            -   abstract_value_of(AbstractTerm(T, v)) := v // This is unwrapping once AbstractTerm
        -   If abstract_value_of is not useful, then perhaps concrete_value_of should just be called value_of.
        -   Maybe type_of and value_of should be used (type_of would be abstract_type_of, value_of would be
            concrete_value_of), and then concrete_type_of would be kept as is, since it would be less-used.

        Another conceptually useful notation would just be a sequence of type->inhabitant instantiations, terminating
        with the actual value of the term.  Each item in the sequence carries the information for how to interpret
        the remaining items.  For example:
        -   NonParametricType ArrayES (T,N) (4,5,6,7,8)
        -   NonParametricType Union (Sint32,Bool) Sint32(8)
        -   NonParametricType Union (Sint32,Bool) Bool(true)
        Incidentally, this is how things will be laid out in serialization (or memory, assuming things are contiguous).
        Can this work for AbstractTerm as well?
        -   AbstractTerm T v
        where T is a Type (Term) and v is a Term.
        The notation for the above sequences starting with NonParametricType could be changed to
        -   ConcreteTerm ArrayES (T,N) (4,5,6,7,8)
        -   ConcreteTerm Union (Sint32,Bool) Sint32(8)
        -   ConcreteTerm Union (Sint32,Bool) Bool(true)
        because for a ConcreteTerm, it's understood that the first item in the sequence is NonParametricType.

        Representationally, AbstractTerm could be implementated in C++ as `class AbstractTerm`, which holds the
        pair (T, v) of Terms (a Term is either a ConcreteTerm or AbstractTerm).

        Examples of AbstractTerms
        -   AbstractTerm(ConcreteTerm(Union,(Sint32,Sint64)), ConcreteTerm(Sint32, 3))
        -   AbstractTerm(ConcreteTerm(Intersection,()), ConcreteTerm(Sint32, 3))

        -   Idea: Allow abstract terms to be constructed using [] instantiation ([] to distinguish against ()).  E.g.
            -   Union(Sint32,Bool)[Bool(true)]  := AbstractTerm(ConcreteTerm(Union,(Sint32,Bool)), Bool(true))
            -   Union(Sint32,Bool)[Sint32(8)]   := AbstractTerm(ConcreteTerm(Union,(Sint32,Bool)), Sint32(8))
            Or more generally,
            -   T[v]

            Also, use shorthand for concrete terms
            -   ArrayES(T,N)(1,2,3) := ConcreteType(ConcreteType(ArrayES, (T,N)), (1,2,3))

        Now the challenge is how to map this into a C++ implementation.  The challenges are:
        1.  Want to use an std::any-based storage mechanism to represent a general Term (see sept::Data).
        2.  Want to have subtypes of Data which represent ConcreteTerm and AbstractTerm.

        The solution to 2 could be that AbstractTerm is a meta-type that corresponds to a C++ class, e.g.

            auto U = Union(NPType::Sint32, NPType::Bool);
            auto i = int32_t(8);
            auto b = true;
            auto xi = AbstractTerm(U, i);
            auto xb = AbstractTerm(U, b);
            auto xi_ = U[i]; // Notation for AbstractTerm instantiation
            auto xb_ = U[b]; // Notation for AbstractTerm instantiation
            auto array_mid = ArrayE(NPType::Sint32)(5,6,7,8);
            auto array_loose_type = Data(NPType::Array);
            auto array_loose = array_loose_type[array_mid]; // widens the type from ArrayE(Sint32) to Array
            auto array_strict_type = ArrayES(NPType::Sint32,4);
            auto array_strict = array_strict_type[array_mid]; // narrows the type from ArrayE(Sint32) to ArrayES(Sint32,4)
            auto array_side_type = ArrayS(4);
            auto array_side = array_side_type[array_mid]; // changes the type "sideways" from ArrayE(Sint32)
                                                          // to ArrayS(4); depends on runtime value; verified at runtime;

        Idea: Create instances of the NonParametricTypes like Array, ArrayE, ArrayS, ArrayES, Union, etc
        which can be used with [] syntax to instantiate inhabitants.  The actual implementations of these
        would have to be either in a different namespace, or use different names (e.g. `class Array_`).

-   Idea: A cool use of ADR would be a subtype of ADR that render into something like Wikipedia pages,
    and the links are IPFS addresses.
-   Idea: For giant arrays and other structures, keep them collapsed and then offer a way to view
    pages of the content.  This will avoid blowing away the entire memory, and will avoid attempting
    to download too much when this is being done over a server/client connection.
-   Idea: Use ADR to do data analysis such as form histograms of data, plots of data, apply transformations
    to data to then plot, etc.  The GUI app will make it very easy to see all the possibilities for how
    to view that data.

2020.04.16

-   Idea: Define a Python-like data model on sept::Data and sept::Data_t, so that those objects can be used
    directly, instead of having to type check and type cast them.  E.g. define operator[] so that element
    access can be done, define size() method, etc.
-   TODO
    -   Refactor certain names : drop the "Type" suffix from certain things, in particular:
        -   Array(1,2,3) stays the same
        -   ArrayType() -> Array
        -   ArrayTypeES(T,N) -> ArrayES(T,N)
        -   Then an inhabitant of ArrayES(T,N) can be instantiated as ArrayES(T,N)(1,2,3,...)
        -   UnionType -> Union
        -   Then an inhabitant of Union can be instantiated as Union(T1,...,TN)
    -   Make an AbstractTerm class
    -   Make Data class have a data model, e.g. operator[], size(), etc.
        -   operator[] can only be used to instantiate AbstractTerm, not ConcreteTerm, since
            ConcreteTerm often needs multiple params, but in C++, operator[] can only take one
            param.
        -   operator() should be available for instantiation of ConcreteTerm.

2020.04.17

-   Legend
    -   Xyz : The singleton of the NonParametricType Xyz; provides a constructor for its inhabitants.
    -   Xyz_c : The C++ class which implements the NonParametricType Xyz; implements operator() to
                produce inhabitants (i.e. instances of XyzTerm)
    -   XyzTerm_c : The C++ class which implements a term of Xyz (i.e. an inhabitant of Xyz).

2020.04.18

-   TODO: Make hash tables to look up handler function for various things (see Data.cpp) so that all the
    Data-originated polymorphism is a constant time lookup.
-   Idea: There should be a Term which is a pair TypePair(AT, CT), which can be used to type-annotate a value.
    CT specifies the concrete type of the value, while AT specifies the abstract type of the value.  This
    could be used e.g. if Ascii (formerly AsciiChar) were considered an abstract type, so that Ascii
    was used as an abstract type that conveys context information, instead of representation information.
    The actual Term would look something like

        ConcreteTerm(TypePair(Ascii,Sint8), 'x')

    but the concrete type could be anything that can represent all values {0,1,2,...,127}, so the following
    would all be valid:

        ConcreteTerm(TypePair(Ascii,Uint8), 'x')
        ConcreteTerm(TypePair(Ascii,Uint16), 'x')
        ConcreteTerm(TypePair(Ascii,Sint32), 'x')
        ConcreteTerm(TypePair(Ascii,Float32), 'x')
        ConcreteTerm(TypePair(Ascii,Float64), 'x')

-   Idea: Make a "formal type of" term, which, for a given T : NonParametricType (or maybe any Type?  or maybe
    any Term?), creates FormalTypeOf(T), of which T is the sole inhabitant.  There could be a version of this
    which is FormalTypeOf^{n}(T), which is equivalent to FormalTypeOf(FormalTypeOf(...(FormalTypeOf(T))...))
    where there are n repetitions of FormalTypeOf.

-   Observation: The C++ classes I'm implementing in service of NonParametricType (and related, e.g. ArrayES
    and friends) are in a way an embedded DSL, which is a great way to prototype semantics for a formally
    separate DSL.

2020.04.20

-   LEB128 is a way to encode arbitrary integers.  https://en.wikipedia.org/wiki/LEB128

2020.04.21

-   TODO
    x   Move operator== from Array_c into SingletonBase_t or something.
    -   Figure out where operator== that deals with Data should go.
    -   There needs to be an abstract and a concrete version of is_instance.  Ideally there would be
        a shorter name for this, although the precedent in python is probably nice to adhere to.
    -   Certain types have zero or one abstract/concrete inhabitant -- should be able to determine that
        and retrieve that single inhabitant (if it's one).
-   Observation: Each C++ function really defines a huge set of functions, what with its overloading
    and implicit type conversions.  It would be better to have each of those mechanisms be explicitly
    specified and independently controlled.  There's too much conflation of concepts in C++ syntax.
    E.g. function parameters can be implicitly converted, which causes headaches when there are a ton
    of overload candidates, and you want to be more precise with them.  It should be possible to specify
    `explicit` or `implicit` for each function parameter in the function declaration (and the default
    really should be `explicit` so it is as minimal as possible, and you only get `implicit` by asking
    for it).

2020.04.22

-   Design notes for data model
    -   Goal is to provide a nice abstraction to ease use of Data and other specific types.
        Start with something like the Python data model, where types can provide implementations
        for different semantic functions, providing e.g. item access, length, function call,
        etc.
    -   Design criteria:
        -   Whatever the implementation is, it should play nicely with C++ class polymorphism
            (i.e. virtual methods)
        -   Function calls on Data should have constant-time overhead (i.e. not linear in the number of
            types, as there would be going through an if/else sequence for checking specific types).
    -   Particular methods
        -   size_t size () const
            For terms that has a notion of size, e.g. array length, set size.
        -   template <typename... Args_> auto operator () (Args_&&... args)
            Function call semantic.  It's not obvious what the ref qualifier for this should be.
        -   template <typename T_> auto operator [] (T_ &&arg)
            Item access semantic.  There should be const ref and nonconst ref versions of this.  And
            maybe rvalue ref version.
        -   template <typename T_> bool contains (T_ &&arg) const
            For container-like things, returns true iff this contains the given data.
        -   auto abstract_type () const
            Returns the abstract type of this thing
            TODO: It might be the case that a term doesn't have a single, well-defined abstract type.
        -   auto concrete_type () const
            Returns the concrete type of this thing
        -   template <typename T_> bool has_abstract_term (T_ &&arg) const
            Returns true iff arg is an abstract inhabitant of this thing.
        -   template <typename T_> bool has_concrete_term (T_ &&arg) const
            Returns true iff arg is an concrete inhabitant of this thing.
    -   In order to have constant-time overhead for calls on Data, it will be necessary to register each
        of these functions in an unordered_map lookup table for each type.  However, it's not obvious how to
        handle specific types in such a function registration.  Perhaps could use a void* or Data itself.

            struct IsInstanceTypeKey {
                std::type_index m_value;
                std::type_index m_type;
            };
            // TODO: Define hash for IsInstanceTypeKey
            std::unordered_map<IsInstanceTypeKey,std::function<bool(Data const &, Data const &)>> IS_INSTANCE;

            bool is_instance (Data const &value, Data const &type) {
                // Effectively multi-parameter polymorphism.
                auto func = IS_INSTANCE[{value.type(), type.type()}];
                return func(value, type);
            }

-   Reprioritize in order to get to demonstrating projectional editing and embedding.
    -   UI Design
        -   Show two panels:
            -   Original data
                Actions with buttons and keyboard shortcuts:
                -   Select projection -- dropdown menu?
                -   Project data
            -   Projected data and its context (this could potentially be in another document, as in
                a multiple-document interface (MDI), but for now just two a split-pane GUI)
                -   Integrate changes
    -   Notes regarding parallel editing
        -   In order to model parallel changes and be able to merge them, there has to be a way to refer
            to versions of data which reflects the DAG structure.
        -   Editing a piece of data directly should increment a sequential version number for it,
            where that version as a whole may refer to a branch of something.
        -   Each edit should have a corresponding "diff" which:
            -   Defines the change fully, and provides necessary context
            -   Is reversable
        -   A projection should be considered a "branch", and its version should capture that information.
            The same logic regarding edits applies to branches.
        -   Changes to a projection should be naturally transformable to changes to the original data,
            so that a diff in the projected space naturally produces a diff in the source space.
        -   Change integration comes in two forms
            -   Change integration where there have been no changes to the source branch, in which
                case, the diff in the projected space is translated into a diff in the source space
                and then applied.
            -   Change integration where there has been changes to the source branch, in which case
                the projected space diff gets translated to the source space diff, and then further
                transformations have to be applied to the diff to account for the changes to the
                source since the project branch was made.  For example:

                Source exists, say it's Array('a','b','c','d'), call this S.
                Projection branch is made: Extract element 1, producing 'b' with context "element 1".
                Source is edited: Delete element 0, which was 'a', result is Array('b','c','d'), call this change C.
                Projection branch is edited: change 'b' to 'z' (diff encodes original value and new value, so is reversable).
                Projection branch is integrated:
                -   Diff "change 'b' to 'z'" is transformed to "On element 1, change 'b' to 'z'", call this C1
                -   Commute C1 past C to produce change C2, defined by C(C1(S)) == C2(C(S)),
                    where C2 ends up being "On element 0, change 'b' to 'z'"
                -   Because we're at the HEAD of the source branch, C2 can be applied to the HEAD of source branch,
                    whose current state is C(S).  The result is that the source branch HEAD becomes
                    Array('z','c','d')

-   QT is a great toolkit but its licensing is not awesome -- either use use GPL or buy a commercial license.
    This is an obstacle to creating an septview which people will want to adopt and integrate into their stuff.
    ACTUALLY, I just noticed that apparently there's an LGPL licensing option for it.
    https://doc.qt.io/qt-5/licensing.html
    https://www.embeddeduse.com/2016/04/10/using-qt-5-6-and-later-under-lgpl/

    HOWEVER, it looks like QT For WebAssembly is GPL or commercial only
    https://wiki.qt.io/Qt_for_WebAssembly

    GTK+ uses LGPL, so it would be friendly to what I want to achieve with septview.  However it doesn't
    look like there's any webassembly support for GTK+ at the moment, which is surprising.

    wxWidgets is licenced using something almost identical to LGPL.

2020.04.24

-   Regarding is_subtype, there is some nontrivial computation involved, basically to determine <= within
    a poset, where not all relationships are explicitly specified.  Because subtyping is transitive,
    a search is needed to use those transitive relations to find any possible sequence of explicit
    subtype relationships.
-   There should be meet (greatest lower bound; gcd) and join (least upper bound; lcm) operations defined
    on types, so that e.g. the common type of a collection of types can be accurately expressed, and
    e.g. the intersection type of a collection of types can be accurately expressed (though this might
    up being EmptyType, which is a subtype of everything).
    See: https://en.wikipedia.org/wiki/Join_and_meet

2020.04.25

-   Priorities
    x   Refactor septview's MainWindow into a reusable ADRView widget, so that multiple such views can be
        used at the same time.
    x   Refactor MainWindow to implement an MDI app.
    -   UI
        -   Have a "stack" of view columns, going right
            OR
            Have a tree of view columns (not sure if there's an obviously natural way to lay this out).
        -   Each element on the stack / in the tree is an ADRView widget showing a particular ADR;
            generally each child view shows the projection of the previous one.  In the tree version,
            there can be different projections all being shown at the same time, simulating multiple
            people working on projections of the data simultaneously -- call this "concurrent edit" mode.
        -   In concurrent edit mode, there are different ways it could work:
            -   Changes in a projection are immediately integrated upward to the sources.
            -   Changes in a projection have to be explicitly committed to the source.
            -   Changes to the source automatically propagate down into existing projections
                (this would be like automatically receiving others' edits); this could potentially
                be a problem if there would be a merge conflict.
            -   Changes to the source have to be manually pulled into existing projections.

            There are two different paradigms that are worth providing a proof of concept for:
            -   Simulating realistic concurrent edit mode, where multiple humans are working on
                different projections on different computers at the same time, and have to deal
                with branches, merges, and potentially merge conflicts.
            -   One human working on multiple projections on the same computer, where it would
                be onerous/undesirable to have to manually merge edits, since you could in principle
                ensure that only one edit happens at a time, and that any changes could be fully
                propagated to all other projections, and therefore it would all happen automatically.

        -   To do the UI for this, an MDI (multiple document interface) application is necessary.
            Idea: Use tabbed MDI area for the views, and then use a sidebar QTreeView to show the
            tree structure itself, giving a very direct visualization and very direct navigational
            method.
        -   What are the actions for a particular ADRView?
            -   Project current selection, thereby creating a new view.
            -   Integrate current view's edits into parent (explicit or automatic?).
            -   Integrate parent's edits into current view.
            -   Close current view (bringing up a dialog if there are un-integrated edits).

    -   Implementation plan
        x   Make the root of the viewed data node show up in the tree.
        x   TODO: Fix bug where if items (2, 3, 4, 5) are deleted, it crashes, since it doesn't correctly
            determine that 3 is a parent of 4 and 5.

                root
                +   0
                +   1
                +   2
                +   3
                    +   4
                    +   5
                    +   6
                    +   7

            One way to do this, which would not be optimal, but at least it would be correct,
            would be to sort the items of row_index_map (in delete_selected_values) first by
            their ADRItem nesting depth (highest to lowest), then by their row indices (also
            highest to lowest).

        x   Add a "key" column to septview so that the array index (and later, map key) can be shown.
        -   Show both abstract type (first) and concrete type (second) in ADRTreeView.
        -   Implement the projection stuff first with the various actions, then implement
            the "Projection Tree" UI to provide a visual representation of it.

-   TODO: Think about how to do pointer/reference types (start with the plain kind like in C++,
    then later think about broader references which could potentially include arbitrary URIs)

-   After better understanding QModelIndex, I think ADRItem could be refactored to not need to
    keep an explicit tree, but rather access child nodes by their row index.  Not 100% sure.
    Actually, ADRItem may be totally unnecessary -- the only challenge would be to implement
    ADRModel::parent, since ADR Data doesn't store references to its parent, when applicable.

-   TODO: Could use QAbstractItemModel::fetchMore and ::canFetchMore to do efficient on-demand
    loading (e.g. in a log viewer that shouldn't load everything).

2020.04.27

-   ASN.1 seems like a primitive attempt at ADR, though probably its goals aren't so abstract.
    https://en.wikipedia.org/wiki/Abstract_Syntax_Notation_One

2020.04.29

-   Projections to prototype in UI
    -   Extract element(s) from array
        -   Minimal version should only allow elements from a single array to be extracted
            -   Create QAction which does the "extract element(s)" action
            -   It runs the projection, producing a totally new ADR
            -   It creates another MDI child, initializes its model with the ADR
            -   Create QAction which does the "integrate changes" action
        -   Full version should allow arbitrary elements from hierarchy to be extracted.
            There is potentially a case that has to be avoided (or designed around) which is
            that it's possible to select a parent and child simultaneously in the hierarchy view,
            which means in the projection, at least one value will appear twice, and if those
            values don't match when the changes are to be integrated, then the integration is
            not possible.  Perhaps allow this at first, presenting an error dialog when the
            integration couldn't work, and telling the user how to resolve it.

            Implementation detail for full version: Need some sort of addressing scheme, not unlike
            jq, where each hierarchical member has its own unique address.  This way, the precise
            element in the hierarchy can be tracked for integration of changes.

    -   Unique elements of array -- integration involves changing each occurrence of the original
    -   Type functor (non-structure preserving; e.g. turning an array into its array type)
    -   Type functor (structure preserving; e.g. turning an array into an array of its elements' types)

-   Found this link on projectional editing:
    https://dzone.com/articles/projectional-editing-the-future-of-programming-1

    Actual existing implementation of projectional editing for code:
    https://www.jetbrains.com/mps/concepts/

2020.05.21

-   QT septview has been been good, as the editing capabilities of QT are a great boon. However,
    it's rather inflexible from a view standpoint, and the programming overhead needed (ADRItem,
    ADRModel, etc) is less than ideal.

    Time to implement an OpenGL-based viewer which emphasizes flexibility in the presentation of
    the data.  Editing can perhaps be done "externally", meaning that viewing/selection is the main
    mode for the OpenGL viewer, and editing is done in a sidebar or in a modal dialog, so that
    rich QT widgets can still be used to perform that editing.

    Design criteria for OpenGL viewer
    -   Speed is crucial
    -   Should be fast regardless of amount of data being viewed
        -   For hierarchical data, use LOD rendering (i.e. only directly render a limited range
            of the levels of the hierarchy; your "view locality" involves a zoom level as well
            as a position; render some sort of "ellipsis" affordance for hidden, lower levels
            of the hierarchy).
        -   For list-like data, use pagination or summarization to allow display of lists of
            arbitrary size.
    -   Should be easily traversable with several input devices
        -   Mouse
            -   Point and click to focus particular elements
            -   Mouse wheel to zoom
            -   Left button drag to select nodes
            -   Right button drag to move the view
            -   Double click to edit
        -   Keyboard
            -   Use up and down to traverse siblings
            -   Use left and right to traverse parent/child
            -   Press enter to edit
            -   To commit the edit:
                -   Press enter to commit and stay that node
                -   Press ctrl+enter to commit and go to the next sibling
                -   Press ctrl+shift+enter to commit and go to the previous sibling
            -   Ideally just starting to type while a node is focused should also start editing
    -   Side bar could show a "sober view" which would be a QTreeView, perhaps in which the
        editing is done, and the OpenGL stuff is just for quick traversal and flexible viewing?

    Implementation notes
    -   The rendering of an ADR node should be defined recursively
    -   Two different render modes regarding a node itself
        -   Fully visible -- contents (potentially including children) are visible
        -   Suppressed -- contents are suppressed, instead showing a summary (if applicable)
    -   Two different render modes regarding children
        -   Scaled-down child nodes -- good for smooth zooming, easy positioning and sizing of visual nodes
        -   Equal-sized child nodes -- uses collapse/expand mechanism for "zooming", hard to determine
            position and sizing of visual nodes, but is more conducive to certain things like code ASTs.
    -   In order to display large datasets efficiently, some sort of view culling is necessary.
        The view culling happens mainly in (x,y) space, but a further refinement would have it culling
        in hierarchy-depth space as well.
    -   Potentially useful visual references
        -   KCacheGrind: http://kcachegrind.sourceforge.net/html/Screenshots.html
        -   Flame graphs: http://www.brendangregg.com/flamegraphs.html
    -   In order to accomplish the above, need to learn how to render QT widgets more by-hand,
        instead of using existing layouts, since which ADR nodes will be displayed has to be
        tightly controlled.

2020.05.28

-   IDEA: Use ADR viewer with webasm to make a much better webpage viewer, where the view can be sliced
    and diced, searched, etc.  Ideally there would be some sort of plugin mechanism for the viewing
    and editing, so that it could be endlessly customized beyond just the default affordances.

2020.06.01

-   IDEA (from a while ago): Have programs output ADR instead of text.  Make the terminal/shell be an ADR
    viewer with shell-specific augmentations (in addition to all the default functionality of ADR viewing).
-   IDEA: Have some sort of context-sensitive "opener" feature in ADR viewer.  For example:
    -   If you just compiled a program and got some errors, naturally the error output would be in the form
        of ADR, so you could easily navigate and analyze the error data, but also you should be able to click
        on certain things to "go to" those things.  So for example, there would be some notion of file
        location which, if you click on it, would bring up the appropriate editor.  But if you wanted to,
        you should also be able to click on structure interior to that error's file location, and bring
        up the corresponding thing.  E.g. if you just wanted to open the file, and not go to the precise
        location where the error is.

2020.06.06

-   Synthesis of some ideas: One big milestone goal for the ADR viewer is that it should have the following
    capabilities:
    -   A rich enough view/edit customization toolkit so that arbitrary rendering and editing UI can be
        provided.  This will allow ADR documents to be opened and effectively function as richly structured
        web pages.
    -   The branching concept would be used to be able to edit any document.  Some contexts will accept
        edits to documents.  This could be used to implement a collaborative document editing environment,
        ideally eventually providing real-time collaborative editing.

2020.06.17

-   Probably will rename ADR to SEPT (Structured Expression Project Toolkit, as part of the Structured
    Expression Project)

    In any sort of computerized endeavor -- programming, data analysis, networking, etc -- we have zillions
    of abstract data types flying around, and it's absolutely insane that we don't have an automatic,
    take-it-for-granted way to bring them up in a viewer (which is also capable of editing) that represents
    them in rich, visual form, as quickly and as easily as one would click on a link in a webpage.

    A design criteria for SEPT is that it should produce a utility-type program that is as ubiquitous
    as the text editor, and supplant the text editor entirely.  Examples:
    -   sept-shell output: A commandline program would generate its output in structured data, and the shell
        would be a sept program that displays the structured data with all its rich visual affordances and
        interactivity.  Of course, this can still handle traditional console input/output, as it would
        just manifest as sequences of string input/output.  Terminal emulation (which will still be a thing)
        could potentially be a "widget" available in sept-shell for e.g. ncurses based commandline programs.

        Examples of structured output:
        -   Plot of some computed data, such as
            -   Memory usage
            -   Network latency
            -   Results of some numerical simulation
        -   Histogram of some computed data, such as
            -   Profile/timing data
            -   Results from some numerical simulation
        -   Graph of some data, such as
            -   Network connections
            -   Active peer network
            -   Current state within a state machine (full state machine is rendered; current state is marked)
        -   Tree data, such as
            -   Recursively listed contents of a directory
            -   Function call graph
        -   Timestamped data from multiple channels, such as
            -   Log events from different subsystems
            -   Events from different systems on a network
            -   State machine transitions

        Structured data could also be accumulated, so they don't need to be presented in total for each
        "frame" of output.  Examples:
        -   Time series data, accumulating as time goes on
        -   Timestamped data from multiple channels, like log events being shown in real time
        -   A histogram, accumulating samples as time goes on
        -   Really, any data structure, with "diffs" indicating updates to the display.

        The difference between "absolute output" (meaning a static, whole piece of data that doesn't
        necessarily modify/update something previous) and "update output" could be that "update output"
        is given as a diff.

    -   sept-shell input: A commandline program could accept input using structured data, and could even
        simulate different levels of UI or even GUI in the following way:
        -   The program outputs a particular bit of structured data which is displayed by the sept-shell,
            and uses certain conventions (perhaps there's a particular type system used by convention by
            the sept-shell for interpreting requests for interaction)
        -   sept-shell recognizes the request for interaction and presents the corresponding UI to the
            user, and accepts their input, then submits the appropriate data back to the running program,
            which continues processing.
    -   sept-nav[igator] (something like a web browser) will have all the tools of sept and will be able
        to retrieve content from arbitrary references, which may be on the web or from other sources.
        Because of the visual affordances present in sept, this would automatically provide a rudimentary
        replacement for a lot of web and local content viewing and generation.
    -   The tricky part of the design of sept-shell will be:
        -   Having a reasonable plugin system (or something) that allows customization of viewing and editing

    Thinking more about this, there should be no distinction between sept-shell and sept-nav; sept-shell
    should be able to navigate and display arbitrary content, and that content could come from the web or
    whatever source.  However, it should be very clear when you're issuing input to a particular program or
    website or other target.  Similarly, it should be clear what the source of any content you're seeing is.

    sept-shell could probably function as a rudimentary client for a server-based app, using the same
    conventions as for a console-based app, though it would need to handle network connections and
    authentication.  ALTHOUGH, at most basic, this could be done via ssh (though this requires the user
    have a shell account on the target machine), since it wouldn't require any kind of specialized
    protocol or connection logic.

    sept-shell should be able to "open" executables in order to run them -- this would then connect the
    stdout (and stderr?  if that even makes sense anymore) as the input to the viewer.  Potentially
    the commandline arguments could be adapted to use sept data somehow, though this would be tricky.
    Potentially better solution is for the program to expect that its first input is the data it would
    otherwise receive in the commandline arguments.

    sept-shell should be able to open "sept files", which are serialized sept structured data.  All the
    rich sept tools would be available to view, navigate, create, modify, delete, etc.  It would be
    able to save files, and ideally also handle versioning of them (using the branching scheme that
    I've started working on in sept).

    There should be parsers and projectors for all the myriad file formats to convert it to a sept file,
    at which point, you could open the file in sept and go to town on it.  Example formats that would
    be interesting:
    -   Every kind of computer programming language source code
    -   Any kind of human-readable markup language
        -   JSON
        -   YAML
        -   Markdown
        -   CSV
    -   Serialized numerics data, such as saved numpy arrays (sept will have ample plotting capabilities)
    -   A text file, broken down into paragraphs and sentences
    -   A tar file, where the file index is given in a structure, and then each file body is given separately
        as unstructured bytes (which could potentially themselves be parsed)
    -   Any kind of image file; into two different forms:
        -   into the data structures present in the image file (e.g. whatever representation JPEG or PNG uses)
        -   into the image data directly; sept should be able to plot images

    Another idea for a sept-enabled console app:
    -   Use ADT to define an API for the program.  The API is used by issuing "commands" by serializing
        ADT to the program via stdin, and its output is given via serialized ADT.  The API itself should be
        explicitly encoded in ADT so that the API can be interactively explored by a human who runs the
        program.  This is one form of a self-documenting, self-teaching app.

    Using sept as a GUI viewer/editor that can run programs and interact with them leads to the possibility
    of running several programs at once, each having its own GUI elements and visualization, all of which
    you could integrate together.  You could copy stuff back and forth between them, or even feed the output
    of one into another.  Or countless other possibilities.

2020.07.12

-   Notes on what ADTs would be useful to implement for use in cbz/biome, or for proof of concept of the console
    app interaction with sept:
    -   OrderedMap(Domain,Codomain) // Domain must be ordered somehow; perhaps OrderedSet
    -   Tree (or would it suffice to allow anchors and references?)
    -   Some kind of string.  This would probably be ArrayE(C) where C is (ctype, atype) where ctype is the
        concrete type of each char (e.g. Uint8, or Uint32 for full unicode) and atype is the abstract type
        of the char (e.g. Ascii, Utf8, Utf32, Unicode, whatever).  Note from earlier that the likely choice
        is that the concrete type should be a function of the abstract type, so they don't have to both be
        specified.
    -   Anchors/references, so that stuff like DAGs could be implemented.  This should map to pointers in a
        C++ context, but have some pointer-independent representation within a projection/serialization.

    Examples of usage of (ctype, atype):
    -   Use in abstract syntax tree
        -   An "assignment" AST node would be a map/struct

                target_symbol     : String  // Name of symbol to assign to
                source_expression : ASTNode // Expression whose value will be assigned

            which has a few possible interpretations as a ctype:
            -   Tuple(String, ASTNode) // This loses the attribute name info
            -   OrderedMap(
                    OrderedSet("target_symbol", "source_expression"),
                    Set(String, ASTNode)
                ) // Need dependent type to not lose the specific attribute->type mapping
            -   Struct(
                    Attribute(target_symbol, String),
                    Attribute(source_expression, ASTNode)
                ) // This is a literal description of it, and it would be memory-equivalent to Tuple(String, ASTNode)
            -

    How would this be used in serialization with custom AST types?
    -   Would need to register each serializable type with the serializer object.  Basically this would record
        the ctype along with some identifier for the type, and a factory function for creating that type so that
        it could be populated during deserialization.

        Because the set of custom types in a program is not known in advance, there needs to be some sort of
        registration table at the beginning of the serialized stream that behaves as the "legend" for the types.
        There can be some marker that stands for "custom type", followed by the ID for the type within the legend,
        and the legend contains full information about that type, such as what its concrete type is.  These
        could be considered typedefs for the serialization.

    -   How would the serialization work?  Each custom type would need serialize and deserialize functions.
        The serialize function would write the concrete value to the stream.  How to enforce that this is
        done such that its actual ctype is serialized?

        Deserialization would involve creating the correct type from the registered factory function, and
        then populating it from the stream.

        Is it possible to have a direct mapping of memory to serialized form?  Probably not always, e.g. if
        you want compact serialization but aligned attributes in a struct.  Or if pointers are involved, then
        definitely not, since the serialized form will be some pointer-free representation.

    There needs to be an addressing scheme for elements, so that "file locations" can be provided in a
    way that respects the ADT structure (noting that that has nothing to do with the serialized form).
    Ideally this addressing scheme can operate independently of which faithful projection is used (i.e.
    in memory or in a serialization).

-   What is an OrderedMap?  There are several different representations:
    -   As an array of mapping pairs, ordered on the first element of each pair (the domain element).
        This is conducive to an immutable representation (e.g. in serialization or in a map that won't change).
        This admits log-time lookup.  Not sure if there are really any advantages to this over the two-array form.
    -   As two arrays, one which is the ordered domain elements, and the other which is the corresponding
        codomain elements.  This one is conducive to extracting the domain set, and extracting the codomain
        elements as a tuple (as in representing a struct as a tuple).  This admits log-time lookup.  This is
        also conducive to immutable representations (since inserting/removing is expensive).  Depending on
        if the domain elements are stored "in-line" (as opposed to using references), this could admit an
        efficient traversal in serialized form.
    -   As a binary search tree or other search tree structure (e.g. red-black tree).  The serialized form
        of this would be conducive to efficient traversal.
    Each of these forms correspond to a distinct ctype (concrete type), since that's what defines how the
    ADT is structured.

-   Compatibility with native language types (e.g. std::map in C++, etc) is important.  However, to fully
    implement the type system that sept will define will require providing some custom types.

-   How should anchors/references work?  Working in the memory -> serialization direction first,
    each piece of data has a unique pointer value which is its unique anchor.  To map this to the
    serialization form, it's necessary to create a dictionary which maps the pointer values to
    some pointer-independent, abstract anchor scheme.  Potentially this could be done using the
    byte offset into the serialized form.  Because cycles will be permitted, it will be necessary
    to "backtrack" to fill in the byte offsets for anchors that occur later in the serialization.
    Using byte offsets also allows efficient traversal of the serialized form, which will facilitate
    efficient usage of addressing.

    In the serialization -> memory direction, it will similarly be necessary to perform some
    backtracking to fill in pointer values whenever there's a cycle.

    References have an obvious application here (being able to refer to the value of an indirect
    object without needing to explicitly dereference it).  But pointers could be useful too, if
    the pointer value itself is wanted, especially if that pointer value is allowed to be null.

    Should types be referenceable?  Yes, since types should be data as well.

-   In-line storage vs out-of-line storage (i.e. using pointers/references)
    Many ADTs can't be stored in a fixed memory layout, e.g.
    -   Directed acyclic graphs
    -   Non-uniform trees
    so they use pointers to connect disjoint segments of memory together.  This form of representation
    in the serialization should be possible, where there's some sort of "pointer resolution" step.

    In memory, the obvious choice is the pointers supported by the language itself.  In serialization,
    there are a few different possibilities:
    -   Use absolute byte offsets into the serialized stream
    -   Use relative byte offsets into the serialized stream
    -   Use some sort of numbering or naming scheme as placeholders before pointer resolution is done.
    The key case is where there's a cyclic dependency.  If there's no cycle, then the nodes can
    be serialized in dependency order (i.e. dependents following their dependees).

    In the case of a cyclic dependency, like

        pointer0# ::= TreeNode(
            data := whatever
            child_array := ArrayE(TreeNode@)( // T@ is `pointer to T`
                pointer1@,
                pointer2@,
            )
        )
        pointer1# ::= TreeNode(
            data := stuff
        )
        pointer2# ::= TreeNode(
            data := hippo
        )

    To even explain this syntactically already presumes a solution.  In this case, it's using named labels.
    pointer1@ and pointer2@ would have to wait to be resolved until after those labels are seen.

    An alternative would be to use sequential indices to number each separate entry (each separate entry
    roughly corresponding to a malloc), with subelements being referred to relative to each entry.

    It seems like regardless of which approach is used, pointer resolution requires keeping a translation
    table which tracks the mapping between in-memory pointer values and serialization references.

    Because it doesn't necessarily need to be human-readable (in fact, human-readability is a bad design
    criteria for the serialization), could use random (or sequential?) integers for the labels.

    An easy choice for the `in-memory to serialization` direction would just be to use the pointer values
    as the labels directly, because they're guaranteed to be unique and consistent.  Going the other
    direction though would still require having a translation map between those labels and the in-memory
    pointer values of the newly-read-in ADTs.

-   Design notes for implementation of serialization projection
    -   Parameters needed: What type scheme the serialization will be rendered in.  The in-memory data will
        have some particular type, some of it specified at compile time and some of it specified at run time.
        Analogously, the serialized form will have some of the type data present in the serialization itself
        (analogous to run-time) and some of it will be present in the context (i.e. the complement of the
        projection).

        The two endpoints of a possible spectrum here are:
        -   Full type information is put into the serialization (fully dynamic; "maximal" encoding of the data;
            no outside context is needed for a 3rd party to read and fully interpret the data present).
            Examples:
            -   A Python pickle file
            -   Generic output from a generic program (e.g. a "rich" log of events generated by a server, where
                each event is a fully general ADT of arbitrary type)
        -   No type information is put into the serialization (fully static; minimal encoding of the data;
            the data can only be read and fully understood by a 3rd party who already knows the type information).
            Examples:
            -   Any static file format, such as .tar, .png, .mp3.
            -   A serialization of a particular RPC call from client to server, where both parties know the
                format of the data, and an important criteria is that the data be compact in size.

        Due to the type-theoretic features of complex type constructions, there are potentially many in-between
        states.  For a given type construction, there is a poset of "weakenings" of that type, where a "weakening"
        is defined to be "moving a piece of type information out into the fixed, external context".
        Examples:
        -   Given data

                ArrayES(T,N)(T_val1, T_val2, ..., T_valN)

            1. The "full" representation, which encodes all type information, would be something like

                Term
                    ParametricType   // this gives the Type of the Term
                        ArrayES
                            T
                            N
                    T_val1
                    T_val2
                    ...
                    T_valN

            2. But say in the external type context, N is already known.  Then the representation would be something like

                Term
                    ParametricType
                        ArrayE_         // hypothetical type which indicates "you already know the length of this array"
                            T
                    T_val1
                    T_val2
                    ...
                    T_valN

            3. Or, say the type T is known in the external context.  Then the representation would be something like

                Term
                    ParametricType
                        Array_S         // hypothetical type which indicates "you already know the element type of this array"
                            N
                    T_val1
                    T_val2
                    ...
                    T_valN

            Technically if you knew the data contained a Term, then you could omit Term as well.  So let's re-write 1, 2, and 3.

            1a. If you knew the data was a Term, then the representation would be something like

                ParametricType   // this gives the Type of the Term
                    ArrayES
                        T
                        N
                T_val1
                T_val2
                ...
                T_valN

            1b. If you knew the data was a Term and the type of kind `ParametricType`, then the representation
            would be something like

                ArrayES
                    T
                    N
                T_val1
                T_val2
                ...
                T_valN

            2a. If you knew the data was a Term of type ArrayES (which is a ParametricType) and you knew N, then you
            could eliminate more:

                T       // This is the element type.
                T_val1
                T_val2
                ...
                T_valN

            Actually, come to think of it, a new syntax would be useful.  Let _ denote "unknown" (and allow
            distinct ones, say by how many underscores are used, so that _ and __ and ___ would each be
            distinct; though really one could consider _ and __ and ___ etc to be symbolic references that
            refer to symbols in the external context).  This gives a way to specify a partial type without
            awkward English phrasing.  In terms of projecting a full type to a partial type, theoretically
            there should be a complement, and it would be useful to have a syntax to denote that as well.
            Examples:
            -   Full type: ArrayES(T,N)
                Projection: ArrayES(_,N)
                Complement:
                -   _ := T
            -   Full type: ArrayES(T,N)
                Projection: ArrayES(T,_)
                Complement:
                -   _ := N
            -   Full type: ArrayES(T,N)
                Projection: ArrayES(_,__)
                Complement:
                -   _ := T
                -   __ := N
            -   Here's a funny possibility:
                Full type: ArrayES(T,N)
                Projection: _(T,N)
                Complement:
                -   _ := ArrayES
            -   Does this work?
                Full type: ArrayES(T,N)
                Projection: _
                Complement:
                -   _ := ArrayES(T,N)
            -   Duplicated use of unknown placeholders
                Full type: Tuple(T,T)
                Projection: Tuple(_,_)
                Complement:
                -   _ := T

            The usefulness of this comes in when you want to decide/specify what type information to put into
            your serialization.

            In order to determine the projection and complement, some sort of pattern matching has to be done.

            Ok, now to re-do the serialization examples from above.  Given data

                ArrayES(T,N)(T_val1, T_val2, ..., T_valN)

            1.  Source data: Term(ArrayES(T,N), ...)    // ... is used as a placeholder for a sequence
                Projection: Term(ArrayES(T,N), ...)     // use the full spec as the projection for the serialization
                Complement: <empty>                     // no type info extracted for external context

                    Term
                        ParametricType
                            ArrayES
                                T
                                N
                        T_val1
                        T_val2
                        ...
                        T_valN

            2.  Source data: Term(ArrayES(T,N), ...)
                Projection: Term(ArrayES(_,N), ...)
                Complement: _ := T                      // only consider T known in external context
                Design criteria: The placeholder should not actually appear in the serialization; the serialization
                should be strictly shorter, where in the limit (i.e. where the complement is the full type), there
                becomes nothing beyond the T_val1 ... T_valN entries in the serialization.

                    Term
                        ParametricType
                            ArrayES
                                _                       // maybe make this a formal NonParametricTerm
                                N
                        T_val1
                        T_val2
                        ...
                        T_valN

            3.  In order to actually avoid having a placeholder in the serialization, it's necessary to encode some
                more info in the complement.
                Source data: Term(ArrayES(T,N), ...)
                Projection: _ := N, ... := ...          // We want to keep only the number of elements and the elements themselves
                Complement: Term(ArrayES(T,_), ...)     // now this external context is carrying Term, ArrayES, and T.

                    N
                    T_val1
                    T_val2
                    ...
                    T_valN

            3a. Source data: Term(ArrayES(T,N), ...)
                Projection: ... := ...                  // We want to keep only the elements themselves
                Complement: Term(ArrayES(T,N), ...)     // now this external context is carrying Term, ArrayES, T, and N.

                    T_val1
                    T_val2
                    ...
                    T_valN

            4.  Could use this to specify fixed values as well, it doesn't have to just be types.
                Source data: Term(ArrayES(Float64,3), 1.0, 4.0, 9.0)
                Complement: Term(ArrayES(Float64,3), 1.0, ...)
                Projection: ... := 4.0, 9.0

            5.  Similar; this could be useful in storing homogeneous coordinates in a particular canonical form
                Source data: Term(ArrayE(Float64), 1.0, ...)
                Complement: Term(ArrayE(Float64), 1.0, ...)
                Projection: N, ...                      // ArrayE has to store the number of elements.

            Note that the syntax `Term(T,V1,...,VN)` is equivalent to `T(V1,...,VN)`, where both mean the type
            T instantiated with the parameters V1,...,VN, except that the syntax `Term(T,V1,...,VN)` is closer
            to the serialized representation.  Similarly, `ArrayES(T,N)` is equivalent to
            `ParametricType(ArrayES,T,N)` or maybe really `Term(ParametricType(ArrayES),T,N)`.

    -   Because the main use case is serialization to a file, there should be a subclass of Projection
        which represents the concept of byte array, and allows for a streaming output.  And streaming
        input for the embedding.

-   In addition to anchors/references, there could be symbolic references, which correspond to some
    logic surrounding a symbol table (e.g. where the document can make `typedefs` and refer to them
    later).  What's the difference between a reference and a symbolic reference?  The reference uses
    some sort of pointer internally (or requires use of an address), whereas the symbolic reference
    uses a symbol identifier (i.e. name).

    If there were some mechanism for defining and looking up symbols, it could be done using a
    particular "command".  Built-in functions/types:
    -   DefineSymbol(SymbolId, SymbolContent)   // Defines SymbolId in the symbol table to be SymbolContent
    -   SymbolicRef(SymbolId)                   // Looks up SymbolId and substitutes in SymbolContent

-   Addressing scheme.  Design criteria:
    -   Should be independent of form, i.e. should work for in-memory data as well as serialized data.
        It might be acceptable for the serialized data form to require having the type context in
        order to function (i.e. the data itself has no type info, and the type info is all provided
        separately in the "projection complement", which is the type info that was lost in the projection).
    -   The lookup should be efficient when possible, based on the type of the data.  Potentially there
        could be types whose storage format is not intended for efficient access but rather for compact
        storage.
    -   It should have human readable form, as well as efficiently storable form.  Maybe this means
        that it's formal data itself, subject to all the same logic, what with projections and so forth.
    -   I think the only thing that could possibly work is some sort of query string, similar to how jq
        works.  This would be a sequence of array indices, map keys, etc.  The human readable form is
        obvious.  The binary form would be something like `ArrayE(QueryNode)` with atype `Address`.
        Here, `QueryNode` would be something like the union of all possible key types (e.g. ints for
        arrays, strings and arbitrary key types for maps).
    -   There should be a way to form relative addresses, e.g.
        -   The parent of this node -- required when appropriate
        -   Child X of this node -- required
        -   The previous sibling of this node -- optional, may not be defined in all cases
    -   There should be a way to return the atype/ctype of a node (this hints that more built-in
        functions would be wanted in the future)
    -   The addressing scheme depends on the ADTs involved.  Generally, there is not necessarily
        a
        -   Array
            -   Children are indexed by integers in a particular range
            -   There should be a way to return the type of a node

    In whatever human readable form data takes, pointers/references will be formed using addresses,
    since pointer/byte offset values are not meaningful in a human-readable representation.

    Forming addresses this way allows a rich set of operations on addresses to be defined that will
    be important in the diffing/merging of branching data.

2020.07.13

-   Design notes for OrderedMap
    -   OrderedMap is a type
    -   The terms of OrderedMap have the form OrderedMap(Domain,Codomain), where Domain and Codomain
        are types.  Terms of OrderedMap(Domain,Codomain) are effectively functions Domain -> Codomain,
        though they aren't necessarily defined on all of Domain.  It would be useful to also have some
        notion of "complete" map in which all elements of Domain are mapped.  In particular, an
        OrderedMap is a partial function.  The order is the one defined on the Domain type.
    -   Should be analogous to std::map.  In particular, std::map simply specifies the domain and
        codomain types at compile time.
    -   For now, an implementation analogous to ArrayTerm_c can be made, where std::map<Data,Data> is
        used, and a constraint, which is the value `OrderedMap(Domain,Codomain)` is specified.  As in
        ArrayTerm_c, where eventually an implementation of `vector_any` (where the element type is
        specified once, and all elements have that type, a sort of homogeneous, vectorized version
        of std::any), it would be useful to have an implementation of `map_any`.
    -   It would probably be useful to have a notion of FrozenMap, FrozenOrderedMap, etc, which
        is the thing that uses an array to store the key/value pairs.  This has the following uses
        and advantages:
        -   Explicit type specification for storage in a serialization
        -   If the map itself is immutable, then it can be represented in a form that doesn't admit
            fast/easy insertion/deletion of elements, but is simple and good for storage.
-   Similar considerations hold for Set, OrderedSet, FrozenSet, FrozenOrderedSet.
    Then a FrozenMap(D,C) could just be the pair (FrozenSet(D),Array(C)), and analogously a
    FrozenOrderedMap(D,C) could just be the pair (FrozenOrderedSet(D),Array(C)).

    Although this is getting into abstract type territory, and away from concrete type territory.

        AbstractType                ConcreteType
        -------------------------------------------------------------------------------------
        FrozenOrderedSet(T)         ArrayE(T)

        FrozenOrderedMap(D,C)       ArrayES(ArrayE,2)(ArrayE(D), ArrayE(C))
                                    Ideally, there would be a way to specify that the arrays
                                    have the same size.

                                    Alternatively, it could be
                                    ArrayE(ArrayES(Type,2)(D,C))
                                    This would be a sequence of (key,value) pairs.

2020.07.18

-   Idea for specification of ordering for various types.  When specifying OrderedSet or
    OrderedMap or whatever ordered structure, there should be a parameter in the type constructor
    which specifies the ordering.  This would manifest as a term, such as

        LexicographicalOrder

    or

        ShortLexOrder

    and would function as a "name" which defines that particular ordering.  For now, these would
    specify built-in orderings, since for the time being, there's no way to extend the type
    system within itself using code.

-   It would possibly be useful to have "zero" and "one" subtypes, as well as "binary" for each
    numeric type, e.g.

        Uint8{0} : Uint8ZeroType    // This type has exactly one inhabitant, which is Uint8{0}
        Uint8ZeroType < Uint8       // Subtype relationship

        Uint8{1} : Uint8OneType     // This type has exactly one inhabitant, which is Uint8{1}
        Uint8OneType < Uint8        // Subtype relationship

        Sint32ZeroType < Sint32ZeroOrOneType    // This name kinda sucks.  Could use Sint32ZorO which would be funny.
        Sint32OneType < Sint32ZeroOrOneType

    Could make it parametric like

        Uint8{0} : Zero(Uint8)      // Zero(Uint8) is the zero-subtype of Uint8, having single inhabitant Uint8{0}
        Uint8{1} : One(Uint8)       // One(Uint8) is the one-subtype of Uint8, having single inhabitant Uint8{1}
        ZeroOrOne(Uint8) < Uint8    // ZeroOrOne(Uint8) obviously has exactly 2 inhabitants
        Zero(Uint8) < ZeroOrOne(Uint8)
        One(Uint8) < ZeroOrOne(Uint8)
        Sigma(Sint32)               // This type has the 3 "sign" inhabitants, Sint32{-1}, Sint32{0}, Sint32{1}.

-   Before implementing serialization, references/anchors and addressing all have to be designed
    and implemented.  This is because serialization of those things is nontrivial and requires
    some thought.

    Also should implement some sort of string.  Ideally this would just use Array with a particular element type.
    The atype would convey the semantic information about the string, such as the encoding or what kind of
    data is held by the string.  For example:

        atype               ctype
        -----               -----
        AsciiString         ArrayE(Uint8)
        Utf8String          ArrayE(Uint8)
        Utf16String         ArrayE(Uint16)

    Perhaps a string should optionally have a fixed length.  The question of whether or not a null terminating
    character is to be included is up to the user of the data type:

        atype               ctype
        -----               -----
        AsciiStringS(n)     ArrayES(Uint8,n)
        Utf8StringS(n)      ArrayES(Uint8,n)
        Utf16StringS(n)     ArrayES(Uint16,n)

2020.07.20

-   From the type system and related functions, it's natural to grow a framework for rich set of type-theoretic
    tools, for example:
    -   Computing concrete type of a given type
    -   Declaring isomorphisms/monomorphism/epimorphisms between types
    -   Using quotient categories of the relevant types by `identification isomorphisms` to provide a formal
        framework for informal syntax (e.g. implicit casting)
    -   The logic behind strengthening/weakening of types with respect to serialization (i.e. how much type
        information will be present in the serialized data itself?)
-   All the type system, ADTs, serialization, functions on all of that, all should eventually inform a
    standard, whose ideal form specifies a rich and uniform type/ADT/serialization system that all languages
    could/should support for easy interoperability.

-   IDEA related to the biome code DB idea:
    -   Have a DB of sept data.
    -   Use various addressing/indexing/symbolic-referencing/hash-naming schemes.
    -   Using public/private key cryptography (or other, fancier cryptographic/identity schemes), data could
        be associated with users, and could even be signed.  Encrypted data is also an obvious possibility.
    -   A user identity could be used to create a fully unique URL-like identifier for each datum,
        thereby uniquely identifying any piece of datum in this sept DB scheme anywhere in existence.
    -   If there were a permissioned version of IPFS, meaning that you could define who is authorized to
        access what data within it, that probably be a great way to enable P2P communication of data.
        -   Using a peer-to-peer messaging system, a better replacement for email could be created
            which would automatically have the richness of sept.
    -   Perhaps save all data and all versions in hash-identified DB entries.
        Full git-like versioning data will be stored, with diffs and branching, etc.
    -   Keep track of a ton of metadata, such as:
        -   For work items, the sequence of timestamps for each edit
        -   A user can specify work session time blocks which are labeled with what was being done,
            and then those time blocks can be associated with the relevant edit timestamps and presented
            in the "parallel channels" SE.
        -   What program generated what datum (so that if you want, you can select that data to be deleted).
            If this is combined with stuff like timestamps and work session time blocks, then you can
            identify more accurately what data is what.
        -   Annotations/comments on whatever data, such as "TODO: check for this condition" or
            "I did this calculation on paper, but it should be checked", or "This function should
            only be called once".  This is analogous to code comments.
    -   Dependencies of each piece of data on all others are kept (e.g. when a particular datum
        refers to another, external one)
    -   "Horizontal" associations (e.g. associative indexes, sequences of work timestamps, etc)
        could also be tracked and used to generate interactive indices and other visual affordances.
    -   Because dependencies are tracked, it should be possible to "export" a complete set of data
        which includes all dependencies, and optionally all "horizontally associated" data.  This
        export would be "full" in the sense that it's not being projected down into a poor format
        ("poor" means "lacking features/richness" here).
    -   In a way, this would be an alternative to a filesystem.  It could potentially also be
        much much better, since it would use the many possible visual affordances of SE/sept to
        view/navigate/edit/create/modify the data, instead of just having a directory hierarchy.
    -   Use of a DB means that people would probably want to use DB techniques to query the sept data,
        though it's not obvious that this would be the best way or easy.  Though perhaps it's worth
        formally representing the various dependencies and associations within the DB scheme itself
        so that DB theory and practice can be taken advantage of.

2020.07.21

-   Notes on prioritization of work regarding serialization.
    -   In order to accomplish proofs of concept, it's really not necessary to have the type-stripping
        features working (though that will be a big nice-to-have later, in order to be able to richly
        read in existing, static file formats).
    -   More important for proofs of concept is having pointers/references, since most nontrivial ADTs
        don't have static memory layouts.
-   Design notes for serialization specifics.
    -   For serialize and deserialize, probably just use the registration pattern like compare and is_instance.
    -   There must be some size_of mechanism which determines the static memory layout for a given type, so
        that it's possible to skip past that element in a serialized stream.
    -   Similarly, arrays, maps, and other container types might need to specify the element count so that
        it's possible to skip beyond them.

    -   NonParametricTerm(X)
        e.g. Void, VoidType, Bool, BoolType, ArrayType, ArrayES, etc.
        -   Let TLTAG{X} denote the top-level tag that represents the vocabulary of fixed tags that are allowed as
            top-level items.
        -   Let NPTAG{X} denote the tag that represents NonParametricTerm `X` (the vocabulary of NonParametricTerms is fixed).
        -   Let BYTES{Y} denote the natural byte-array representation of Y, where the size of Y is not encoded; for
            example, since Sint32 specifies 4-byte values, BYTES{Sint32(123)} is a 4-element byte array.
        -   Let SRL{Z} denote the serialization of Z.  This allows inductive definitions of the serialization of each term.

        Serialization (indentation is only for readability; each "tag" is serialized sequentially into the stream
        in a contiguous, packed manner):

            TLTAG{NonParametricTerm}                    // A NonParametricTerm is uniquely identified by a single token, in this case, X.
                NPTAG{X}

    -   Example: Void

            TLTAG{NonParametricTerm}
                NPTAG{Void}

    -   True

            TLTAG{NonParametricTerm}
                NPTAG{True}

    -   Sint32(123)

            TLTAG{Term}                                 // A term has 2 fields: type and value
                TLTAG{NonParametricTerm}
                    NPTAG{Sint32}
                BYTES{Sint32(123)}                      // This is serialization of Sint32

    -   ArrayES(T,N) // This is the type itself

            TLTAG{Term}
                TLTAG{NonParametricTerm}                // ArrayES has two parameters, T and N.
                    NPTAG{ArrayES}
                SRL{T}                                  // The size of this serialization is not well-defined a-priori
                SRL{N}                                  // N should be a size that's known ahead of time, e.g. Uint32 or Uint64

        Note that because SRL{T} and SRL{N} could be any size, the constraint of knowing the size of the value part of the term from its
        type isn't satisfied, though it could be determined through a computation which would depend on the specific types of T and N.

    -   ArrayE(T)

            TLTAG{Term}
                TLTAG{NonParametricTerm}                // ArrayE has one parameter, T.
                    NPTAG{ArrayE}
                SRL{T}

    -   Array

            TLTAG{NonParametricTerm}
                NPTAG{Array}

    -   Array(Sint16(8), True, Float64(3.5))            // The run-time data stored is the number of elements.  Each element stores its own type.

            TLTAG{Term}
                TLTAG{NonParametricTerm}                // This is the type of the term, which is Array
                    NPTAG{Array}
                TLTAG{Term}                             // This Term is the array size
                    TLTAG{NonParametricTerm}
                        NPTAG{Uint64}
                    BYTES{Uint64(3)}
                TLTAG{Term}                             // element 0, which is Sint16(8)
                    TLTAG{NonParametricTerm}
                        NPTAG{Sint16}
                    BYTES{Sint16(8)}                    // 2 bytes
                TLTAG{NonParametricTerm}                // element 1, which is True
                    NPTAG{True}
                TLTAG{Term}                             // element 2, which is Float64(3.5)
                    TLTAG{NonParametricTerm}
                        NPTAG{Float64}
                    BYTES{Float64(3.5)}                 // 8 bytes

    -   ArrayE(Sint16)(2, 8, 55)                        // The run-time data stored is the number of elements.  The element type is stored once and
                                                        // the serialization of each element doesn't include the type.

            TLTAG{Term}
                TLTAG{Term}
                    TLTAG{NonParametricTerm}            // ArrayE has one parameter, T.
                        NPTAG{ArrayE}
                    TLTAG{NonParametricTerm}            // This is the parameter to ArrayE
                        NPTAG{Sint16}
                TLTAG{Term}                             // This Term is the array size
                    TLTAG{NonParametricTerm}
                        NPTAG{Uint64}
                    BYTES{Uint64(3)}
                BYTES{Sint16(2)}                        // Serialization of element 0, omitting type info
                BYTES{Sint16(8)}                        // Serialization of element 1, omitting type info
                BYTES{Sint16(55)}                       // Serialization of element 2, omitting type info

        It might be useful to have something like BytesArray, where it specifies an array size type (Uint64 in the
        above example), an array size (Uint64(3) in the above example), and then a sequence of byte arrays that
        represent each element.  This way, array-like content can be more easily reasoned about.

    -   ArrayES(T,Uint64(3))(T_val0, T_val1, T_val2)

            TLTAG{Term}
                TLTAG{Term}
                    SRL{ArrayES}
                    SRL{T}
                    SRL{Uint64(3)}
                // We can get right to the elements, since we already know the type and array size
                SRL{T_val0, complement:=Term(T,_)}      // Ser. of element 0; `complement:=` indicates that only the value should be included
                SRL{T_val1, complement:=Term(T,_)}      // Ser. of element 1
                SRL{T_val2, complement:=Term(T,_)}      // Ser. of element 2

    -   OrderedMapDC(U,V)

            TLTAG{Term}
                TLTAG{NonParametricTerm}
                    NPTAG{OrderedMapDC}
                SRL{U}
                SRL{V}

    -   OrderedMapDC(U,V)(u0:=v0, u1:=v1, u2:=v2)       // This is just made-up syntax to indicate the pairs

            TLTAG{Term}
                SRL{OrderedMap(U,V)}                    // Term type
                // First attempt, not sure if this is ideal -- array of key/value pairs; other choice would be array of keys, array of values.
                // Later, need to ensure that efficient lookup is possible, which requires constant-time indexing of key elements.
                SRL{                                    // The complement removes from the serialization already-known data, such as the
                                                        // encoding as an ArrayE and the element type of that ArrayE.  What is still present
                                                        // in the projection is the number of elements and the key/value pairs.
                    ArrayE(ArrayS(2)(U,V))((u0,v0), (u1,v1), (u2,v2)),
                    complement := ArrayE(ArrayS(2)(U,V))(...)
                }

        Alternative, with array of keys, array of values.  The disadvantage of this is that the size of the map is recorded twice,
        once in each array.  Could potentially address that by requiring that ArrayE(T) be instantiated by specifying the size as
        the first parameter.  Though it would still require the ability to specify the same placeholder in multiple places.

            TLTAG{Term}
                SRL{OrderedMap(U,V)}
                SRL{                                    // The specified complement causes the serialization only to encode the number of
                    ArrayE(U)(u0, u1, u2),              // elements and the elements themselves (excluding their types)
                    complement := ArrayE(U)(...)
                }
                SRL{
                    ArrayE(V)(v0, v1, v2),
                    complement := ArrayE(V)(...)
                }

    Let's get more specific with the placeholders for complements.  Use $0, $1, $2, etc.  But what do these mean exactly?
    For now, pretend that Tuple is a thing.  Then
    -   Full data: Tuple(Float64, Bool, Bool, Sint16, Float64, String)
        Complement: Tuple($0, $1, $1, $2, Float64, String)
        Projection:

            Float64 // This defines the value for $0
            Bool    // This defines the value for $1
            Sint16  // This defines the value for $2

    So far, the complement is what contains all the $-delimited placeholders.  Does it make sense for the projection to
    ever have placeholders in it?  To test this, just reverse the above example.
    -   Complement: Float64 Bool Sint16 // This implies that these terms are in a "sequence"
        Projection: Tuple($0, $1, $1, $2, Float64, String) // This implies that it substitutes the appropriate values in.
    This could be rephrased (somewhat inequivalently) though as
    -   Complement: $0(Float64, Bool, Bool, Sint16, $1, $2)
    -   Projection:

            Tuple
            Float64
            String

    and actually the projection is smaller, even though the complement is larger.

    In order to actually compute a (projection, complement) pair, it's necessary to somehow specify how to do the split.
    In talking to Max Sills on 2020.07.21, he thought it might just be a bit pattern specifying which tokens make it into
    the projection and which go into the complement.  This could be a pretty simple and straightforward way to do it,
    even if it doesn't allow for things like repeated placeholders.  Though probably repeated placeholders will be
    important for template-like constructions where the same type appears in many places.  Repeated placeholders
    should generally be used to represent true correlation/identification between instances of tokens.

    The syntax used above where even a type being instantiated can be placeheld (see `$0` being used in place of `Tuple`)
    implies that the given ADT classes can't be directly used for this purpose, since they each have constructors which
    take specific arguments.  Instead, some general syntax for things like Term (i.e. an instantiation of a type using
    a given sequence of parameters) would be necessary so that the constructed type could also be placeheld.  For example:

        Tuple(Float64, Bool, Bool, Sint16, Float64, String)

    would become

        Term(Tuple, Float64, Bool, Bool, Sint16, Float64, String)

    which could then accomodate placeholders in anything, including the `Tuple` slot, e.g.

        Term($0, $1, $2, $2, Sint16, $1, String)

    and if this were used as the complement, then the projection would be

        Tuple
        Float64
        Bool

    How, how about the Term part?  Is it necessary to operate using the syntax of the serialization in order to
    precisely control what appears in the serialization?

-   Idea regarding translation tables for pointer/symbol resolution: Have some notion of "local" table which limits the scope of
    particular pointers/symbols, so that a [de]serialization doesn't have to monotonically accumulate an unbounded number of
    lookups as it proceeds; it can discard the local lookup table once it's done.

    An example of where this would be used is in serializing some tree ADT where each parent-to-child relationship corresponds
    to a pointer.  To serialize a node X, you could serialize all its children, adding their pointers to the translation table,
    and then once X is done being serialized, you dump X's child translation table entries from the table.  Depending on how
    carefully the serialization is designed, it might be possible to do this such that the number of translation table entries
    is bounded by log(n), where n is the number of nodes in the tree.


-   Regarding the idea from before of having an `Uninitialized<T>` type indicate a non-constructed value of type T,
    a better term might be `Garbage<T>`, since that communicates more, and might suggest that it could be used in
    some sort of garbage collection scheme.

-   Idea for encoding integers based on LEB128

    This would be used for array lengths, so that the array size type doesn't have to be specified.

    First byte has the form

        CCCC VVVV

    where CCCC (the "following-byte-count bits" is a uint4 representing the number of bytes to follow this byte,
    and VVVV is the LSB (MSB?) of the value.  The rest of the bytes have the form

        VVVV VVVV

    i.e. they each encode the next 8 bits.  Thus it's possible to encode ints up to 4+8*15 = 124 bits.  A single
    byte can encode 4 bits.

    MSB (most-significant byte) order might be desired because then within the first byte you get an estimate
    as to the total magnitude of the value (it's VVVV * 256^CCCC).  This also makes the lexicographical order
    defined on it as a byte array the same as the numerical order, assuming that it represents an unsigned int.
    It would also be possible to define arithmetic on MSB.

    What are the advantages of LSB (least-significant byte) order?  LSB also has the magnitude estimate property,
    except that it gives a range instead of a particular good-to-N-digits estimate.  Arithmetic on LSB order is
    probably easier/faster on a LSB CPU (and conversely, MSB is faster on LSB CPU).

    Alternatively, the number of following-byte-count bits could be changed, rendering different capacities for
    max bits and number of bits for one byte.

        FBC bits        bits in single byte         max bit capacity
        ------------------------------------------------------------
        0               8                           8                   // Note that this is just uint8
        1               7                           15                  // Can store ascii char in single byte
        2               6                           30
        3               5                           61                  // This easily covers modern memory capacity in bytes
        4               4                           124                 // This easily covers uint64 which is the current theoretical max address
        5               3                           251
        6               2                           506
        7               1                           1017                // Can store boolean (0 or 1) in single byte
        8               0                           2040                // Note that first byte is just the FBC

    This format could be done with other base types, such as uint16 or uint32.

        import typing

        def cap (c:int, basebits:int) -> typing.Tuple[int,int]:
            s = basebits-c
            return (s, s + (2**c-1) * basebits)

        def table (basebits:int) -> typing.List[typing.Tuple[int,int,int,int]]:
            return [(c,basebits)+cap(c,basebits) for c in range(basebits+1)]

        table(8)

          c  bb m  cap
        [(0, 8, 8, 8),
         (1, 8, 7, 15),
         (2, 8, 6, 30),
         (3, 8, 5, 61),
         (4, 8, 4, 124),
         (5, 8, 3, 251),
         (6, 8, 2, 506),
         (7, 8, 1, 1017),
         (8, 8, 0, 2040)]

        table(16)

          c   bb   m  cap
        [(0,  16, 16, 16),
         (1,  16, 15, 31),
         (2,  16, 14, 62),
         (3,  16, 13, 125),
         (4,  16, 12, 252),
         (5,  16, 11, 507),
         (6,  16, 10, 1018),
         (7,  16,  9, 2041),
         (8,  16,  8, 4088),
         (9,  16,  7, 8183),
         (10, 16,  6, 16374),
         (11, 16,  5, 32757),
         (12, 16,  4, 65524),
         (13, 16,  3, 131059),
         (14, 16,  2, 262130),
         (15, 16,  1, 524273),
         (16, 16,  0, 1048560)]

        table(32)

          c   bb  m   cap
        [(0,  32, 32, 32),
         (1,  32, 31, 63),
         (2,  32, 30, 126),
         (3,  32, 29, 253),
         (4,  32, 28, 508),
         (5,  32, 27, 1019),
         (6,  32, 26, 2042),
         (7,  32, 25, 4089),
         (8,  32, 24, 8184),
         (9,  32, 23, 16375),
         (10, 32, 22, 32758),
         (11, 32, 21, 65525),
         (12, 32, 20, 131060),
         (13, 32, 19, 262131),
         (14, 32, 18, 524274),
         (15, 32, 17, 1048561),
         (16, 32, 16, 2097136),
         (17, 32, 15, 4194287),
         (18, 32, 14, 8388590),
         (19, 32, 13, 16777197),
         (20, 32, 12, 33554412),
         (21, 32, 11, 67108843),
         (22, 32, 10, 134217706),
         (23, 32,  9, 268435433),
         (24, 32,  8, 536870888),
         (25, 32,  7, 1073741799),
         (26, 32,  6, 2147483622),
         (27, 32,  5, 4294967269),
         (28, 32,  4, 8589934564),
         (29, 32,  3, 17179869155),
         (30, 32,  2, 34359738338),
         (31, 32,  1, 68719476705),
         (32, 32,  0, 137438953440)]

-   Another idea would be instead of encoding a linear number of following bytes, use an exponential scale,
    since that's probably how numbers and integer types are distributed anyway.

        EEEE VVVV

    The exponent, denoted by EEEE, will include the first byte, so that 0000 means "1 byte".  This specifies
    that the number of bytes is 2^EEEE.

    In a 1-bit exponent

        EVVVVVVV

        0VVVVVVV            : E = 0, byte count = 2^0 = 1, 7 bits
        1VVVVVVV VVVVVVVV   : E = 1, byte count = 2^1 = 2, 15 bits

    This is the same as the linear scale described above.

    Summarizing, for 1-bit exponent,
    -   Min usable bits: 7
    -   Max usable bits: 15

    In a 2-bit exponent

        EEVVVVVV

        00VVVVVV                                                                : E = 0, byte count = 2^0 = 1, 6 bits
        01VVVVVV VVVVVVVV                                                       : E = 1, byte count = 2^1 = 2, 14 bits
        10VVVVVV VVVVVVVV VVVVVVVV VVVVVVVV                                     : E = 2, byte count = 2^2 = 4, 30 bits
        11VVVVVV VVVVVVVV VVVVVVVV VVVVVVVV VVVVVVVV VVVVVVVV VVVVVVVV VVVVVVVV : E = 3, byte count = 2^3 = 8, 62 bits

    This is one bit better than the linear scale format with 3 FBC bits.  This also easily covers modern
    memory capacity, so would be sufficient for array sizes.

        import typing

        def exp_cap (word_bits:int, exponent_bits:int) -> typing.Tuple[int,int]:
            max_exponent = 2**exponent_bits - 1
            max_word_count = 2**max_exponent
            max_usable_bitcount = max_word_count*word_bits - exponent_bits
            min_usable_bitcount = word_bits - exponent_bits
            return (max_exponent, max_word_count, min_usable_bitcount, max_usable_bitcount)
        ```
    Summary of all the exponent bit counts:

        WordBitCount  ExponentBitCount  MinUsableBits  MaxUsableBits
        ------------------------------------------------------------
        8             0                 8              8
        8             1                 7              15
        8             2                 6              62                   this is just shy of 2^6 bits, which is 8 bytes (for a single int; i.e. uint64)
        8             3                 5              1021                 this is just shy of 2^10 bits, which is 128 bytes (for a single int)
        8             4                 4              262140               this is just shy of 2^18 bits, which is 32 kilobytes (for a single int)
        8             5                 3              17179869179          this is just shy of 2^34 bits, which is 2 gigabytes (for a single int)
        8             6                 2              73786976294838206458 this is just shy of 2^66, which is greater than addressable memory; max usable type
        8             7                 1              1361129467683753853853498429727072845817
        8             8                 0              463168356949264781694283940034751631413079938662562256157830336031652518559736

        16            0                 16             16
        16            1                 15             31
        16            2                 14             126
        16            3                 13             2045
        16            4                 12             524284
        16            5                 11             34359738363
        16            6                 10             147573952589676412922
        16            7                  9             2722258935367507707706996859454145691641
        16            8                  8             926336713898529563388567880069503262826159877325124512315660672063305037119480

-   Finally, let's try a format where the number of bits is specified by the exponent, instead of the number of bytes.
    This is free of the 8-bit-per-byte practical constraint, but might have some implementation awkwardness in such
    an architecture.

        import typing

        def bit_exp_cap (exp_bit_count:int) -> typing.Tuple[int,int,int]:
            min_exp = 0
            max_exp = 2**exp_bit_count - 1
            min_bit_count = 2**min_exp
            max_bit_count = 2**max_exp
            return (exp_bit_count, min_bit_count, max_bit_count)

    Summary:

        ExpBitCount  MinBitCount  MaxBitCount
        -------------------------------------
        0            1            1                                         This is the same as `bool`; no variability
        1            1            2                                         0, 1, 2, 3; bool is 2 bits.
        2            1            8                                         Can represent small values efficiently up to 255; bool is 3 bits.
        3            1            128                                       Can represent values up to 128 bit int (which is very large); bool is 4 bits.
        4            1            32768                                     This would be a single int that's 4 kilobytes
        5            1            2147483648                                This would be a single int that's 256 megabytes
        6            1            9223372036854775808 = 2^63, which is half the max capacity for memory (being used for a single int); max usable type.
        7            1            170141183460469231731687303715884105728
        8            1            57896044618658097711785492504343953926634992332820282019728792003956564819968

    With ExpBitCount = 1, the representable types are uint1, uint2
    With ExpBitCount = 2, the representable types are uint1, uint2, uint4, uint8
    With ExpBitCount = 3, the representable types are uint1, uint2, uint4, uint8, uint16, uint32, uint64, uint128 -- this is quite good

    Here's a biased version which probably has a more useful range.  If the uint exponent field reads N,
    then the number of value bits is defined to be 2^(N+1)-1.

        import typing

        def bit_exp_cap_biased (exp_bit_count:int) -> typing.Tuple[int,int,int]:
            min_exp = 1
            max_exp = 2**exp_bit_count
            min_bit_count = 2**min_exp-1
            max_bit_count = 2**max_exp-1
            return (exp_bit_count, min_bit_count, max_bit_count)

        0, 1, 1         equivalent to bool
        1, 1, 3         bool takes 2 bits; capacity is uint3 in 4 bits; could be useful for many enum types
        2, 1, 15        bool takes 3 bits; capacity is uint15 in 17 bits, which is pretty good
        3, 1, 255       bool takes 4 bits; capacity is uint255 in 258 bits -- this is a pretty versatile type
        4, 1, 65535
        5, 1, 4294967295
        6, 1, 18446744073709551615      this is 1 bit shy of the 64 bit memory capacity (actually the 6 bits of exponent would put it over)
        7, 1, 340282366920938463463374607431768211455       bool takes 8 bits; capacity is larger than all of storage on earth
        8, 1, 115792089237316195423570985008687907853269984665640564039457584007913129639935

    With ExpBitCount = 1, the representable types are uint1, uint3
    With ExpBitCount = 2, the representable types are uint1, uint3, uint7, uint15
    With ExpBitCount = 3, the representable types are uint1, uint3, uint7, uint15, uint31, uint63, uint127, uint255


    One last one, where the idea is that each number will be packed into as few bits as possible, with the convention
    that if the exponent uint reads N, then the number of bits in the value is 2^N-1, and if N == 0, then the value
    is defined to be zero.

        import typing

        def bit_exp_cap_packed (exp_bit_count:int) -> typing.Tuple[int,int,int]:
            min_exp = 0
            max_exp = 2**exp_bit_count-1
            min_bit_count = 2**min_exp-1
            max_bit_count = 2**max_exp-1
            return (exp_bit_count, min_bit_count, max_bit_count)

    Summary:
    Note that uint0 represents a 0-bit int whose only value is zero.
    Note that uint1 is bool.

        0, 0, 0                                         can store uint0 in 0 bits!
        1, 0, 1                                         can store uint0 (in 1 bit) or uint1 (in 2 bits)
        2, 0, 7                                         can store uint0 (in 2 bits), uint1 (in 3 bits), uint3 (in 5 bits), uint7 (in 9 bits)
        3, 0, 127                                       can store uint0 (in 3 bits), uint1 (in 4 bits), uint3, uint7, uint15, uint31, uint63, uint127
        4, 0, 32767
        5, 0, 2147483647
        6, 0, 9223372036854775807
        7, 0, 170141183460469231731687303715884105727
        8, 0, 57896044618658097711785492504343953926634992332820282019728792003956564819967


    Ok, last one.. honest...

        import typing

        def bit_cap_quadratic_packed (exp_bit_count:int) -> typing.Tuple[int,int,int]:
            min_param = 0
            max_param = 2**exp_bit_count-1
            min_bit_count = min_param**2
            max_bit_count = max_param**2
            return (exp_bit_count, min_bit_count, max_bit_count)

    Summary:

         0, 0, 0            uint0 in 0 bits
         1, 0, 1            uint0 in 1, uint1 in 2
         2, 0, 9            uint0 in 2, uint1 in 3, uint4 in 6, uint9 in 11
         3, 0, 49           uint0 in 3, uint1 in 4, uint4 in 7, uint9 in 12, uint16 in 19, uint25 in 28, uint36 in 39, uint49 in 52
         4, 0, 225          uint0, uint1, uint4, uint9, uint16, uint25, uint36, uint49, uint64 in 68, uint81, uint100, uint121, uint144, uint169, uint196, uint225
         5, 0, 961
         6, 0, 3969
         7, 0, 16129
         8, 0, 65025
         9, 0, 261121
        10, 0, 1046529
        11, 0, 4190209
        12, 0, 16769025
        13, 0, 67092481
        14, 0, 268402689
        15, 0, 1073676289
        16, 0, 4294836225

2020.07.22

-   Need to develop the "syntax" for serialization and projection/complement so that it's well-defined for both.

    Existing:

    -   Array(val0, val1, val2) Array(val3, val4)

            TLTAG{Term}
                TLTAG{NonParametricTerm}
                    NPTAG{Array}
                SRL{val0}
                SRL{val1}
                SRL{val2}
            TLTAG{Term}
                TLTAG{NonParametricTerm}
                    NPTAG{Array}
                SRL{val3}
                SRL{val4}

        No, this is ambiguous, because SRL{val#} might contain TLTAG{Term} and it wouldn't be clear where the
        end of the Array's elements is supposed to be.  Thus there needs to be a way to specify the number of
        elements or the end of the sequence.

    -   ArrayES(T,Uint64(3))(T_val0, T_val1, T_val2)

            TLTAG{Term}
                TLTAG{Term}
                    SRL{ArrayES}
                    SRL{T}
                    SRL{Uint64(3)}
                // We can get right to the elements, since we already know the type and array size
                SRL{T_val0, complement:=Term(T,_)}      // Ser. of element 0; `complement:=` indicates that only the value should be included
                SRL{T_val1, complement:=Term(T,_)}      // Ser. of element 1
                SRL{T_val2, complement:=Term(T,_)}      // Ser. of element 2

    -   OrderedMapDC(U,V)

            TLTAG{Term}
                TLTAG{NonParametricTerm}
                    NPTAG{OrderedMapDC}
                SRL{U}
                SRL{V}

    -   OrderedMapDC(U,V)(u0:=v0, u1:=v1, u2:=v2)       // This is just made-up syntax to indicate the pairs

            TLTAG{Term}
                SRL{OrderedMap(U,V)}                    // Term type
                // First attempt, not sure if this is ideal -- array of key/value pairs; other choice would be array of keys, array of values.
                // Later, need to ensure that efficient lookup is possible, which requires constant-time indexing of key elements.
                SRL{                                    // The complement removes from the serialization already-known data, such as the
                                                        // encoding as an ArrayE and the element type of that ArrayE.  What is still present
                                                        // in the projection is the number of elements and the key/value pairs.
                    ArrayE(ArrayS(2)(U,V))((u0,v0), (u1,v1), (u2,v2)),
                    complement := ArrayE(ArrayS(2)(U,V))(...)
                }

        Alternative, with array of keys, array of values.  The disadvantage of this is that the size of the map is recorded twice,
        once in each array.  Could potentially address that by requiring that ArrayE(T) be instantiated by specifying the size as
        the first parameter.  Though it would still require the ability to specify the same placeholder in multiple places.

            TLTAG{Term}
                SRL{OrderedMap(U,V)}
                SRL{                                    // The specified complement causes the serialization only to encode the number of
                    ArrayE(U)(u0, u1, u2),              // elements and the elements themselves (excluding their types)
                    complement := ArrayE(U)(...)
                }
                SRL{
                    ArrayE(V)(v0, v1, v2),
                    complement := ArrayE(V)(...)
                }

    The syntactical challenge right now is how to make `Term` part of the complement such that it's kept out
    of the projection.

    In the syntax

        X(Y1, Y2, Y3)

    there are the following properties:
    -   There is a sequence of "symbols".  In particular,
        -   X
        -   (
        -   Y1
        -   ,
        -   Y2
        -   ,
        -   Y3
        -   )
        whitespace is ignored with respect to the syntax.

    -   What is being conveyed by this syntax is an AST where `X` is the root, and there are a number of
        child nodes.  The parens are used to indicate the parent/child relationship and to indicate the
        boundary of the sequence of children.

    -   What are alternatives to the parenthetical grouping of children/parameters?
        -   Pre-specifying how many parameters are to follow, e.g.

                X
                sequence_of_size        // Indicates the beginning of a sequence with a fixed number of params
                3
                Y1
                Y2
                Y3

        -   The following is equivalent to parens, but maybe is a useful different presentation anyway

                X
                begin_sequence          // Indicates the beginning of a sequence with unspecified param count
                Y1
                Y2
                Y3
                end_sequence            // Indicates end of sequence

    Going back to specific examples above,
    -   Full data: Array(v0, v1, v2)
        Written out in symbol sequence

            Array                           // If you've only seen this far, you don't know if it will be Array or instance of Array.
            instantiate_begin               // This is what tells you it's an instance of Array.  Indicates beginning of parameter sequence.
            v0
            v1
            v2
            instantiate_end                 // Indicates end of parameter sequence.

        or with param count specified,

            Array                           // If you've only seen this far, you don't know if it will be Array or instance of Array.
            instantiate_with_param_count    // This is what tells you it's an instance of Array.
            3                               // Number of params to follow
            v0
            v1
            v2

        This is different than the `Term` construction, which would be more like

            Term
            instantiate_begin
            Array
            v0
            v1
            v2
            instantiate_end

        or with param count specified,

            Term
            instantiate_with_param_count
            4
            Array
            v0
            v1
            v2

        This is not as good, because projecting out `4` precludes the possibility of having something like `3 v0 v1 v2`.
        It also corresponds to the awkward LISP-like syntax `Term(Array, v0, v1, v2)`.

        So go with the `T instantiate_begin p1 ... pn instantiate_end` or `T instantiate_with_param_count n p1 ... pn`
        option.  The latter is more practical from the perspectives:
        -   Being able to pre-allocate memory when deserializing a stream into memory
        -   Being able to efficiently skip a section of the serialized stream and go to the next item

    How about using `Term` again?  Where Term expects two parameters, a type and a value.  Something like

        Term
        Array                           // This is the Type parameter for Term.
        instantiation_with_param_count  // This and the following 4 lines form the second parameter (the value) to Term
        3
        v0
        v1
        v2

    An alternative would be to have a "by_itself" token which can be used to avoid the ambiguity described above where
    it's not known if the stream contains the type Array, or an instance of Array, until the next token is read.  This
    would read something like

        Term
        Array
        instantiation_with_param_count
        3
        v0
        v1
        v2

    for `Array(v0,v1,v2)`, and

        Term
        Array
        by_itself

    for `Array`.  `by_itself` sort of functions like a semicolon in C.

    How about for iterated instantiation, such as in `ArrayES(T,3)(v1,v2,v3)` ?

        ArrayES
        instantiation_with_param_count
        2
        T
        3
        instantiation_with_param_count  // TODO: Could become `instantiation_with_known_param_count` then omit `3`
                                        // since it's deduced from ArrayES(T,3).
        3
        v1
        v2
        v3

    if `Term` is used as a prefix, then it only takes the first two params and stops there.
    So perhaps all Terms should have some term-ending delimiter.  The previous example would
    become

        ArrayES
        instantiation_with_param_count
        2
        T
        3
        instantiation_with_param_count
        3
        v1
        v2
        v3
        term_end

    One possibility is that `term_end` could be omitted if it's known that the term defined thus far is not a type,
    i.e. can't be instantiated further.  Though this would introduce context-dependence that requires knowing stuff
    about the types involved, and therefore would involve semantics in the representation.  Arguably use of semantics
    will happen anyway, e.g. with being able to use `instantiation_with_known_param_count` which uses knowledge
    about the instantiated type (in this case `ArrayES(T,3)`) to infer the number of params in the following
    instantiation.

-   Regarding efficient skipping of serialized Terms
    -   If the type of a term is not known, then the type has to be read in before deducing the size.
    -   If the type of a term is known,
        -   It may have a fixed size, and therefore skipping that term is straightforward and fast
        -   It may not have a fixed size (e.g. the size of `Array(...)` depends on the types of its contents),
            and therefore it's necessary to traverse the contents in order to determine the size to skip it.

    In the case where the `size_of` operation is not efficient (e.g. having to part or all of the content of
    a Term to make the determination), it should be possible to opt-in to produce tags within the serialization
    stream that pre-compute the size so that it can be skipped efficiently.

    However, even this only allows efficient skipping of that single Term.  If these are put in a sequence,
    then indexing into that sequence still requires an O(n) operation, skipping Terms until the desired
    indexed element is found.

    Efficient indexing (meaning O(1)) requires being able to compute the memory offset of the desired indexed
    element with a fixed formula (basically a multiplication and add), and this requires having one of the following:
    -   A fixed element size (e.g. `ArrayE(T)`, which specifies that all its elements have the same type, `T`)
    -   A lookup table which provides memory offsets into a sequence of non-uniformly sized elements.

    When serializing, there should be a number of options controlling what goes into the serialization.
    -   Specifics about the complement of the projection (the projection being what becomes serialized).
        This controls how much type (or value) info goes into the projection, and how much must be
        understood in the external context (i.e. the complement).
    -   If optimally efficient indexing of content is desired.  For example, should it be possible to
        be able to optimally execute a query on the serialized form?  Such as

            root[3][config][tls_enabled] // made up syntax

        This query would be executed optimally when the data is in memory, so the question is really about
        if the query can be done optimally when serialized form.  Examples of when it would be useful:
        -   Reading a particular value out of a config file
        -   If the data represents source code, and a compilation process reports an error and references
            a particular data node, and to print an error message, it should be possible to read that data
            node out of the offending file efficiently (instead of having to read through potentially the
            entire file, which might be really really big).
        Examples of when it wouldn't be useful:
        -   In just saving data to a file that you'll later read back into memory in its entirety.
        -   In transmitting serialized data over a network connection.
        -   In storing data that will be processed sequentially, i.e. no nonlinear access pattern.

    Obvious extensions of this querying would be to be able to specify ranges of indexes and keys,
    as well as other fancier queries, going in the direction of a true DB.  If the data structures
    support such queries, there's no reason it couldn't become as general as you want.

-   It should be possible to perform arbitrary queries on serialized data so long as the complement of
    the serialization-projection is available.  This probably implies that the complement should be
    the "template" (meaning that it contains the "outer framework" for the type/value information for
    the projection, and therefore it's sort of the container that the projection will be put into).

2020.07.25

-   Design notes for serialization with complement.
    -   The complement mechanism must allow recursive definitions of serialization (e.g. parts of
        a given complement that are involved in serializing a Term that's recursively defined
        should allow for a corresponding recursion in the complement).
    -   The syntax for defining a complement ideally would be of the same class as the syntax for
        defining Terms, but that may not be directly possible without some additional thought,
        such as a particular type which represents a placeholder (I'm thinking of the example
        where the type being constructed itself is a placeholder, like

            Full data: Tuple(Sint8, Float64)
            Complement: $0(Sint8, Float64)
            Projection: Tuple

        The challenge here is that the complement is essentially invoking a constructor on a
        placeheld type, so it's not clear what that really means.  Arguably all of this complement
        and projection stuff is just a syntactical process, so perhaps it's really just a syntactical
        construction, and therefore there should be a `Placeholder(n)` type which can be instantiated.

        Try this idea out:

            Tuple(Sint8, Float64)           // This means instantiate type `Tuple` with parameters `Sint8, Float64`
            Placeholder(0)(Sint8, Float64)  // This means instantiate type `Placeholder(0)` with parameters
                                            // `Sint8, Float64`.

        Then the "project" operation on `Tuple(Sint8, Float64)` with complement `Placeholder(0)(Sint8, Float64)`
        should result in `Tuple`.

        Repeated Placeholders: It should be possible to repeat a placeholder, which should impose a constraint
        on what can be projected.  Examples:
        -   If the full data is `Tuple(Sint8, Sint8)`
            and the complement is `Tuple(Placeholder(0), Placeholder(0))`
            then the projection should be `Sint8`.
        -   If the full data is `Tuple(Sint8, Float64)`
            and the complement is `Tuple(Placeholder(0), Placeholder(0))`
            then the projection is ill-defined and should result in an error, since the same placeholder is trying
            to be used for two distinct terms.

        If this `Placeholder` type works, then no alternate syntax is needed, and all this projection/complement
        stuff can be defined within the same type paradigm, no extra syntax needed.

        Other needed aspects:
        -   How to handle labeling Placeholder instances, when working with recursively defined projections,
            since the Placeholders will be defined "locally" for each recursive definition?  Possible solutions:
            -   Use a canonically-indexed tree label, i.e.

                    0               // 0th child of root
                    1               // 1th child of root
                        1.0         // 0th child of 1
                        1.1         // 1th child of 1
                            1.1.0   // 0th child of 1.1
                        1.2         // 2th child of 1
                    2               // 2th child of root
                        2.0         // 0th child of 2

                Advantages:
                -   It provides canonical labels for things.
                -   When a node is "popped" from the context, all its descendants can be forgotten.
                -   It clearly represents the structure of the placeholders, so that reasoning about
                    them (e.g. for debugging purposes) would be easier.
                Disadvantages:
                -   The label has unbounded size, so it wouldn't simply fit in a fixed-size type.
                -   It requires having a mapping between the canonical-tree-index and the sequential
                    index of the terms that show up in the projection (this correspondence is needed
                    to reconstruct from a (projection,complement) pair).

            -   Use a sequential index, where the next available index is passed in as a mutable
                reference to the serializer.
                Advantages:
                -   There's no need for a mapping to the sequential index, because it already has that form.
                -   Index would fit in a fixed-size int type.
                Disadvantages:
                -   Harder to reason about the tree structure, since that information is lost in the index.
                -   Not possible to parallelize this projection, since the mutable reference to the "next
                    available sequence index" is used completely serially (unless you can efficiently compute
                    the number of placeholders needed by a particular term, so you can compute the starting
                    index for each term and run their projections in parallel).
                -   It may not be possible/easy to be able to discard "finished" placeholder values, since
                    the tree structure that indicates which ones are needed is not present.

                This seems to be the way to go, it's just simpler.

            Finally, because each type's projection is defined recursively, the placeholder instances can't
            be assigned numerically in the definitions, so some sort of stand-in label has to be used.
            Perhaps use a user-supplied local index here, and then the "true" sequence index will be
            assigned during projection.

-   Design notes for SyntacTuple_t, which is meant as an implementation helper baseclass for implementing
    generic syntactical tuples (of which many other classes are subclasses of).
    -   SyntacTuple_t should be able to store 0 or more "parameters" of type Data.
    -   It should use CRTP (where you specify the derived class as a template param)
    -   There should be a flavor of SyntacTuple_t which has a fixed number of parameters, and another
        flavor that doesn't have a fixed number of params.  Potentially could also allow a range for
        the number of parameters.
    -   It might be useful to have a flavor of SyntacTuple_t which specifies the data type of each element.
        This would have to use std::tuple for storage, if it weren't a uniform type.
    -   Some uses of SyntacTuple_t might even have a parameter slot used for specifying the abstract type
        of the instance as a runtime value, since the whole point of SyntacTuple_t is to reduce boilerplate.
    -   Specific example(s):
        -   Placeholder(n) and Placeholder(n)(A,B,C) and Placeholder(n)(A,B,C)(X,Y)

            Placeholder(n) should be a SyntacTuple_t which allows exactly 1 parameter, and because
            its type is non-parametric, its type doesn't have to be stored at runtime.

            Placeholder(n)(A,B,C) should be a SyntacTuple_t with parameters:
            -   A   // This is placed first so that its indexing is correct.
            -   B
            -   C
            -   Abstract type (which in this case is `Placeholder(n)`) // maybe this should be a formal param

            Placeholder(n)(A,B,C)(X,Y) should be SyntacTuple_t with parameters:
            -   X
            -   Y
            -   Abstract type (which in this case is `Placeholder(n)(A,B,C)`)

    The ArrayTerm_c class could be refactored to use SyntacTuple_t (though once vector_any is created, there
    would need to be a uniform-parameter version of SyntacTuple_t).

    Tuple_c could be implemented as a SyntacTuple_t, where its type is Tuple_c of the types of each parameter.

    OrderedMap*Term couldn't be implemented using SyntacTuple_t, unless a map version of it were created.

-   TODO
    x   Move std::vector<Data> into DataVector.hpp and rename to DataVector
    -   Move DataOrderedMap to DataOrderedMap.hpp

2020.07.26

-   Notes on if using CRTP ( https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern )
    is worth it for SyntacTuple and SyntacTupleT.
    Advantages:
    -   Some functions admit implementations in the baseclass (i.e. in SyntacTuple and SyntacTupleT).
    -   There is potentially more control in the baseclass over stuff involving the derived class,
        like access to the typeid of the baseclass.
    -   Types can be distinguished (e.g. Placeholder(0) can be distinguished from ArrayE(T), where
        both of them use SyntacTuple_S_t to store their one parameter).
    Disadvantages:
    -   Additional complexity and template params
    -   Must be header only (this isn't necessarily a disadvantage though)
    -   There isn't a single common baseclass that can be used for e.g. an array of such things
        (though this could be fixed by factoring out a non-templatized baseclass, thereby adding
        complexity).

-   Consider allowing a dependency DAG to be specified for the unit tests, so they don't have to be
    explicitly ordered via numbers (they're just run in lexicographical order at present).  Then
    a topological sort could be done on the DAG to determine what order to run the tests in.
    Additionally, running one test could have the option of running all its dependency tests,
    so that any dependency failures could be identified as well.  Each test would just provide
    a list of strings of the names of the dependency tests.  The hierarchy should also be respected
    in this naming of dependencies, meaning that if "/abc/test_xyz" and "/abc/test_pqr" are tests,
    then naming "/abc" as a dependency would cause all of the tests under "/abc" to be considered
    dependencies.

-   Idea from conversation with Danny Palm, regarding his game Sea of Stars (SoS to be renamed later)

        one thing I'm envisioning in my sept GUI tool is being able to provide rudimentary, abstract
        UIs.. so like in SoS, there are various ADTs that describe the galaxy... you mentioned the
        hierarchy of sectors, star systems, planets, moons, etc

        so imagine playing SoS through the sept tool, where it sends you the ADT corresponding to the
        sector you're in, and you view that in the sept GUI tool as a tree in whatever view form you
        want.. like a file manager looking tree, or some more pretty, graphical view of it, or one
        of the other built-in tree viewing modes, like at https://treevis.net/

        and then you can issue whatever kind of command, maybe you click on a node and then that
        brings up the ADT of possible commands on that type of object

        and then you can form and issue a command, which is a sept-RPC call so like, you sorta get an
        abstract GUI for the game that you can test with, independent of whatever real graphical
        client you ultimately create

        then you get a client for free that's good for testing

        but the way I see it going in the longrun is that programmers will be able to develop plugins
        for the sept GUI tool to render ADTs and whatever data however they want.. it'll be the view
        part of the model/view architecture, so you could just write a plugin to render SoS as you'd
        actually want it

-   Idea (Danny mentioned using this pattern): Formally use the abstract/concrete type pattern in
    defining types like Term<AType,CType>, e.g.
    -   Term<EmailAddress,std::string> // The concrete type is std::string, but the abstract type is
        the type tag EmailAddress (which is something like an empty struct).  Contrast this with
        Term<Path,std::string>, which has the same concrete type but different semantic meaning.
    -   Term<LatLong,std::array<double,2>>
    -   Term<Complex,std::array<double,2>>
    -   Term<AsciiString,std::string>
    -   Term<UTF8String,std::string>

    Could also specify the abstract type as a runtime value, such as
    -   TermA<std::string>, where there's an extra field which stores some type identifier, which
        could be e.g. typeid(EmailAddress), typeid(AsciiString), typeid(UTF8String), etc.  The letter
        A indicates that the Abstract type is what's specified at runtime.
    -   TermC<AsciiString> could use std::any (or in this project, Data) as the concrete type,
        which is how the value is stored, but the Abstract type is specified.
    -   TermAC would have a runtime Abstract type and use std::any (or Data) as the concrete type.

    How would this manifest in this project?  Or e.g. in an AST class hierarchy?
    -   Term<FunctionCall,std::tuple<T,2>> where T is whatever type stores an AST node (e.g.
        std::unique_ptr<X> for some X, and the elements are 0:function expression to call, and
        1:parameter tuple to pass to the function.
    -   Term<ConditionalExpression,std::tuple<T,3>> where the three elements are 0:condition,
        1:expression if condition is true, 2: expression if condition is false.
    -   Term<Assignment,std::tuple<T,2>> where the elements are 0:target, 1:source.

2020.07.29

-   Current priorities
    -   The type projection/complement concept is cool but not actually necessary to implement
        serialization for use in the CLI-GUI interop proof of concept.  Thus defer this cool
        but low priority work for later.
    x   Highest priority is straightforward serialization/deserialization of all Terms with all type
        information encoded in the serialization.  This will be used for the CLI-GUI interop POC.
    -   CLI-GUI interop POC notes
        -   CLI will emit serialized data, GUI will consume that data -- all of this via a plain
            stdout->stdin pipe.
        -   There should be a rudimentary API defined by "control Terms" which enable the following
            actions:
            -   Phase 1
                -   Output(V) -- just indicates that V is output of the CLI tool, and the GUI should
                    accumulate it into the display.  The basic mode of GUI is that it just displays
                    the sequence of output from the CLI tool.
                -   ClearOutput -- similar to clearing a terminal; clears the sequence of output.
            -   Phase 2
                -   RequestSyncInput(T) -- when the GUI receives this, it pops up some UI workflow for
                    generating a Term of the requested type.  When the user confirms the value entry,
                    the GUI responds with the Term.  The CLI tool will wait for the response before
                    continuing.  Note that a two way, cross-connected pipe is needed for this.
                    See https://unix.stackexchange.com/questions/53641/how-to-make-bidirectional-pipe-between-two-programs
                -   SyncInputResponse(V) -- this is the response sent to the CLI by the GUI to
                    handle a RequestSyncInput(T).
            -   Phase 3
                -   RequestAsyncInput(T,AsyncRequestId) -- when the GUI receives this, it pops up some
                    UI workflow for generating a Term of the requested type, but it does this in a separate
                    thread, so that it can continue receiving data from the CLI in the meantime.  The ID is
                    necessary because there could potentially be multiple simultaneous async requests.
                    The GUI should queue them somehow and allow for the user to respond in whatever order.
                -   AsyncInputResponse(V,AsyncRequestId) -- this is what's sent back to the CLI in response
                    to the RequestAsyncInput.
            -   Phase 4
                -   Some sort of "declare a named value" action -- goes into some kind of symbol table
                -   Some sort of "delete a named value" action -- modifies symbol table
                -   Some sort of "accumulate entries into a named value" action (e.g. for sequences, maps)
                -   Some sort of "replace a named value (complete replacement, not incremental)" action
                -   Some sort of "modify a named value (incremental via diff, not complete replacement)" action
        -   Random idea: Use some sort of view layout ADT to convey UI configuration.  E.g. presenting
            particular pieces of named data (see symbol table) in particular places within a grid, or
            using some dock widgets, etc.
        -   The default output view should be something like a list view where each output sequence
            element is displayed.  Because the type of each output element is unknown, the display list
            has to be rather generic.  Probably there's something in QT that allows arbitrary stuff to
            be rendered into each list item.

2020.08.06

-   Notes on connecting CLI backend and GUI frontend.  Using anonymous pipes didn't work because there's no
    portable way to create a std::istream/ostream using a POSIX file descriptor (there is a boost solution,
    but I got linker errors that I wasn't able to resolve).

    Use of named pipes (i.e. FIFOs via `mkfifo`) makes the solution easy, but it also leaves the FIFO files
    laying around, which is not awesome.  Possible solutions using FIFOs:
    -   Create two fixed-name FIFOs and cross-connect the front and back programs.  This is not awesome
        because this is state that sticks around after the program is done, and is non-reentrant.
    -   Pipe the back into the front, where the default mode is:
        -   back is write-only
        -   front is read-only
        and then have back optionally send a control term which says "here's the name of a FIFO that you
        should open; I'm reading from this FIFO, and you should write your responses into this FIFO".
        This FIFO should be named using a temp name.  It doesn't look like there's a way to create
        an open, temp FIFO unfortunately, so potentially an attacker could open the FIFO before the
        front program does and could send the back program stuff.  One way to get around that problem:
        -   Have the back send the front a "repeat this nonce back to me" control term after doing the
            FIFO negotiation.  Then the front writes that nonce into the FIFO, proving that the front
            is connected, and not an attacker.  This assumes that the attacker hasn't somehow become
            a MITM for the initial pipe command (i.e. `./back | <mitm-command> | ./front`).

    There seems to be a problem with the 2-FIFO approach where both processes are blocked, waiting
    on some reads.  Related: https://condor.depaul.edu/glancast/343class/fifodoc.html

        An initial version of the program is provided. It works fine for only one child. But executing the
        program with 2 or more children runs the risk of the deadlock scenario described above.

        Some sort of "handshake" protocol between the parent and the child(ren) is needed before the parent
        starts writing the data. This protocol needs to guarantee that all children have opened the fifo for
        reading before the parent starts writing to it.

    The problem was that std::ifstream and std::ofstream do their own buffering (which is generally good)
    but it gets in the way of data making its way through the FIFO.  In particular, std::ostream::flush()
    seems to do the expected thing and send the buffered data through, but std::istream::sync() doesn't
    do the corresponding thing for input.  Instead, specifying that there should be no buffering should
    do the trick.

        https://stackoverflow.com/questions/16605233/how-to-disable-buffering-on-a-stream

    It didn't.  Need to implement a test where I use read() syscall directly.

    I wrote a minimal test in which two processes, connected by two FIFOs, each trade a single char.

        p0 writes a char -> p1 reads a char, then writes a char -> p0 reads a char

    Compiled with clang++-9 libc++-9 and g++-8 libstdc++.  The problem turned out that clang++-9
    libc++-9 implementation of std::ifstream tries to read() more than is warranted, and therefore
    it blocks on the unwarranted call to read().  However, g++-8 libstdc++ works, and doesn't read() more
    than is necessary, and therefore doesn't block when inappropriate.

    Same with clang++-10 libc++-10, and same with using g++-8 to link against libc++.  BUT, using either
    compiler to link against libstdc++ worked as desired.

    Related links:
    -   https://bugs.llvm.org/show_bug.cgi?id=23078 (bug from 2015, I guess it hasn't been fixed)
    -   https://stackoverflow.com/questions/47192984/why-does-libc-getline-block-when-reading-from-pipe-but-libstdc-getline-does

2020.08.07

-   Useful apropos concepts:
    -   Relational algebra -- this is an algebraic structure on DB concepts, which has similarity to
        projectional editing.
        https://en.wikipedia.org/wiki/Relational_algebra
        https://en.wikipedia.org/wiki/Projection_(relational_algebra)

2020.08.09

-   TODO: Create an "unvalidated" deserialize_data, where it gracefully reports malformed input.
-   Found this, which is a technique for creating cleaner sets of overloads using templates.
    The idea is that it can achieve what an `explicit` keyword for function parameters would
    otherwise achieve, by `deleting` specified template functions.
    https://stackoverflow.com/questions/175689/can-you-use-keyword-explicit-to-prevent-automatic-conversion-of-method-parameter

2020.08.10

-   Current priorities
    -   Having a string type of some form would be nice.  This is currently achievable via ArrayE(T)
        for some type T, though ideally where would be a type like Ascii(T) or Unicode(T) where it
        specifies the concrete type T.  For example Ascii(Uint8) or even Ascii(Uint7) if there were
        a Uint7 type.  Unicode(Uint32) or Unicode(Uint24).  For things like UTF8, Utf8(Uint8) could
        be the char type, which has variable size, and then ArrayE(Utf8(Uint8)) is the string type.
        Although practically speaking, this isn't a good approach, because the length of each char
        has to be tracked by itself, instead of having a contiguous array of Uint8 into which the
        UTF8 is encoded.

        In order to efficiently implement this, vector_any is needed.  This is because currently
        ArrayTerm_c is implemented using std::vector<Data>, and each Data instance holds:
        -   a pointer-sized union, which is either the value itself (if it fits) or a pointer
            to the heap object, and
        -   a pointer to the "manager", which is a function pointer that does stuff.
        Thus sizeof(Data) == 2*sizeof(void*), which on a 64bit architecture, is 16 bytes, which
        is quite huge to hold a single byte.

        Though at this point, memory efficiency really isn't a priority, so that can wait.

    -   Adding more ADTs to septview is a clear next step.
    -   Filling out the features of septview (especially in terms of creating and editing ADTs)
    -   Make a DAG ADT
    -   "Cartographic map view" -- make a prettier tree view, where it the hierarchy of items
        out as continents, states, counties, cities, neighborhoods, etc, allowing a direct
        spatial representation of the content.  This example is in the visual direction of what
        I'm thinking for a text-content tree, though this example is more artistic than practical:
        https://treevis.net/#Feinberg2010

        One challenge is how to update the view when the content changes.  The items should
        have a deterministic, continuous layout function, so that changes to the content
        doesn't radically alter the view, making it visually incoherent/discontinuous.

        Cartographic map for a tree structure, examples:
        -   Nested, similar to continents, regions, cities, etc
            https://treevis.net/#Zhao2015
            https://treevis.net/#Goertler2018
            https://treevis.net/#Steinbruckner2010 -- 3D
            https://treevis.net/#Dmitrieva2009 -- regions on a 3D globe
        -   Abstract nested
            https://treevis.net/#Schulz2013_1
            https://treevis.net/#Lu2013
            https://treevis.net/#Matela2011 -- this is potentially stable under changes
            https://treevis.net/#Balzer2005 -- Voronoi
        -   Self-similar
            -   Child nodes arrayed radially out from parent node (so is self-similar)
                https://treevis.net/#Hlawatsch2014
                https://treevis.net/#Rosindell2012
                https://treevis.net/#Ukrop2011 -- 3D
            -   Directional (generations proceed along some direction)
                https://treevis.net/#Bohnacker2012 -- pretty
        -   Space-efficient
            https://treevis.net/#Kratt2011 -- this could potentially work well for code
            https://treevis.net/#Wood2008 -- treemap

        Non-cartographic
        -   Child nodes arrayed radially out from root node (not self-similar)
            "rooted tree"
            https://treevis.net/#Woodburn2019
            https://treevis.net/#Huang2020
            https://treevis.net/#Draper2008
            https://treevis.net/#Lam2012 -- super pretty
            https://treevis.net/#Choi2011 -- 3D
        -   Regarding stability under changes
            https://treevis.net/#Sondag2018
        -   Space-efficient
            https://treevis.net/#Wetering2020
            https://treevis.net/#Block2012
            https://treevis.net/#Song2010
            https://treevis.net/#Hida2005 -- hyperbolic half-plane

        Reasonable starting point -- not literally cartographic, but rather an abstraction of it.
        The idea is that the generations of the tree will be layed out in columns (root at left,
        generations proceeding right), the size of the content in each generation being proportionally
        smaller than its parent, so that it has the cartographic zoom property, as well as taking up
        finite space on the screen.  A poor ascii rendition:

            +-----------------+--------+---+-+-+
            |                 |        | D +-+-+
            |                 |   B    +---+-+-+
            |                 |        +---+-+-+
            |                 |        | F | +-+
            |                 +--------+---+-+-+
            |        A        |        |   | | |
            |                 |        |   | +-+
            |                 |   C    |   | +-+
            |                 |        |   +-+-+
            |                 |        |   | | |
            |                 |        +---+-+-+
            |                 |        |   | | |
            +-----------------+--------+---+-+-+

        The width of each column is P times the width of the parent column, where 0 < P < 1.
        If you select a child to view, then the view zooms in so that the child takes up the
        left pane, and its children are now proportionately zoomed, so that the whole picture
        is self-similar.  Because of the geometrical relationship, you can have an infinitely
        deep tree which maps onto this finite 2D rectangle.

        For example, if B were selected and zoomed to, then the screen would appear with B's
        box at the full-height left position (where A is in the above diagram) and B's
        children would be zoomed up correspondingly, filling the second column from the left.

        Notes on implementation:
        -   QGraphicsScene docs show that `minimumRenderSize` will be a useful quantity,
            giving a lower bound on the size of the items it renders -- thus it will cut
            off rendering below a certain size, which is very necessary for a tree.
        -   Is it possible to only populate the content (and I mean in particular the nested
            subwidgets) of a given widget when it's actually painted?  And to somehow
            de-populate it when it's no longer being painted?  The use case I'm thinking
            of is a tree structure that has millions or even billions of nodes, where it
            would be infeasible to have the whole thing in memory, especially if the tree
            structure has to also manifest as a tree of nested widgets.
        -   Maybe can use QPainter with a delegate in order to render stuff without actually
            having to construct a full tree of nested widgets.  This would really reduce the
            amount of bookkeeping and structure duplication.

        Some subideas.  Examples:
        -   Email view -- X axis is time (say, binned into years, months, weeks, days,
            hours, etc), Y axis is category (e.g. personal, work, marketing, etc; or
            sender).  This creates a grid in which the emails can be placed.  Clicking
            on a grid item zooms into that grid item and shows more detail (perhaps
            refining the categorizations).

            Changing which categorizations are used in the view should cause a continuous
            transition to the new arrangement, which would be super pretty.

        -   Photo album view.  Various categories to organize by:
            -   Time (years, months, weeks, days, etc)
            -   Physical location
            -   Color
            -   Some categorization of the content (based on image processing or preexisting metadata)


    -   Make a DAG view
        -   Different modes
            -   Local -- focus on a single node, then give options for how many related nodes
                to show, such as
                -   show descendants up to depth
                -   show ancestors up to depth
                -   show siblings (show all nodes that share a visible ancestor or descendant)
            -   Global -- requires fancier node layout which is hard in general, which is why
                there are tools like `dot`.
    -   A way to view part of a DAG (must start at one node and go in one direction)
        is as its "universal cover", which is a tree structure.  The tree structure has a
        quotient which maps onto the DAG.  This means that some DAG nodes may show up multiple
        times in the universal cover.

-   Notes on atype (abstract type; defines semantics) vs ctype (concrete type; defines representation).
    There is some fixed set of ctypes.  Perhaps make the atypes be constructions taking those, like
    -   Unsigned(i32) // i32 is an LLVM type which is an integer, but doesn't have any notion of
                      // sign; rather it's what you do with it that carries signed/unsigned semantics
    -   Signed(i4)
    -   Array(Unsigned(i32)(4005), Unsigned(i16)(123))
    -   Union(Unsigned(i8), Array, Ascii(i7))

    This formulation is working under the assumption that the ctype should be derivable from the atype,
    which has its advantages (simpler to specify a type) and its disadvantages (not decoupled).

-   Idea: Make selection a formal operation (this would jive with the fact that selection is a formal
    operation in Relational Algebra).  Make a rich collection of ways to select stuff (and make it
    extensible).  In particular, it would be great to be able to apply a selection based on a
    projection.  Examples:
    -   Given an array of numerical data, generate a histogram from it (with whatever parameters,
        maybe a fixed number of buckets with a finite range), then given a selection on the buckets
        apply that selection to the elements of the original array which landed in those buckets.
        This would be something like the preimage of a selection under a projection.
    -   Project a bunch of data into a categorization (i.e. label each datum), then make a selection
        of those categories, and then pull the selection back through the projection to the original
        data set.

2020.08.12

-   Talked with Seth Heltsley a bit and he mentioned this interesting online design app, `mural`,
    which could be considered a specific application of structured expression, in that it lays out
    particular structured design processes called `templates` (along with visual layouts) and
    provides instructions on how to execute the design process.
    -   https://www.mural.co
    -   https://www.mural.co/templates

2020.08.17

-   In interview of Bjarne Stroustrup by Lex Fridman, Bjarne mentions this demonstration
    in which C++ is written in realtime, and the resulting motorola machine code is shown,
    and as the programmer works the C++ abstractions, the motorola machine code is updated,
    whittling it down to a smaller and smaller output.  This is a perfect example of the
    parallel channel structured expression, and is definitely a thing I want to put in biome
    (parallel channel views of the stages of compilation, including LLVM code, as well as
    machine-specific assembly, and perhaps machine code if that makes sense).
    -   https://youtu.be/5An1sNznblQ?t=215

    Matt Godbolt's compiler explorer -- shows parallel channels of C++ and assembly:
    -   https://gcc.godbolt.org/

-   Unity's new "Bolt" visual scripting language has a few cool elements of visual representation
    of various structured expressions.

    https://www.youtube.com/watch?v=DtbyC1OBpFg

2020.09.21

-   Ideas for septview
    -   Implement ability to select a contiguous group of sibling nodes and move them to be the children
        of a new container that is put in their place.

        If the data is

            Array
                "blah"
                "thingy"
                123
                Array
                    true
                    false
                    "maybe"
                10.12

        and the selection is of `123` and `Array(true, false, "maybe")`, then these can be collected
        into a container in a few different ways:

        -   Under an Array:

                Array
                    "blah"
                    "thingy"
                    Array
                        123
                        Array
                            true
                            false
                            "maybe"
                    10.12

        -   Under an OrderedMap, where dummy keys are used:

                Array
                    "blah"
                    "thingy"
                    OrderedMap
                        0: 123
                        1: Array
                            true
                            false
                            "maybe"
                    10.12

            Ideally, for this specific case, it would be possible to either:
            -   Select a corresponding sequence of nodes to be used as the keys
            -   Specify the key-generating scheme

    -   Implement ability to "dissolve" a container, inserting the child nodes in the place of the dissolved one.

        If the data is

            Array
                "blah"
                "thingy"
                123
                Array
                    true
                    false
                    "maybe"
                10.12

        and `Array(true, false, "maybe")` is dissolved, then it becomes

            Array
                "blah"
                "thingy"
                123
                true
                false
                "maybe"
                10.12

        In the case of dissolving an OrderedMap, there are a few ways to include the child elements:
        -   As (key,value) pairs
        -   As keys only (lossy transformation)
        -   As values only (lossy transformation)

2020.10.18

-   Idea: In the serialized tag for "Term", maybe also indicate the nesting level (or maybe just distinguish
    between root and non-root), so that it's possible to make some sense of a stream of serialized data from
    the middle somewhere.  This may or may not actually be useful, use case would be reading from a stream
    starting in the middle somewhere.

2020.12.13

-   It looks like json-ld ( https://json-ld.org/ ) is something like one step from json toward sept.
    It has a visualizer https://json-ld.org/playground/ which seems like a very primitive form of
    the visualization for sept, but doesn't really go very far.

2021.03.24

-   Notes for visualization of nested data types.
    Two ideas:
    -   Make a QT view widget for each data type.
        -   There should be "collapsed" and "expanded" modes for each widget.
        -   The "collapsed" mode should show some summary of its content.
        -   Ideally there should be some distinction for a view widget being shown in-line
            vs non-in-line (i.e. QHBoxLayout vs QVBoxLayout).
        -   This would use plain QT layouts and so forth, and be very low tech, but might not
            scale very well to very large data.
        -   Two forms of this idea:
            -   Plain mode, where there's no scaling of child element widgets.  This necessarily
                takes up more and more space as more elements are expanded.  Although for unbounded
                data types, they could have a bounded view widget using a scroll area.  Then the
                view could be re-rooted to that widget to make the number of view widgets tractable.
            -   Scaling mode, where child widgets are scaled down, and the parent widget has a
                bounded size, so the overall layout of the entire data hierarchy can be known.
    -   Place "summary" nodes in a hyperbolic half plane, placing child nodes correspondingly
        down in the hyperbolic space, scaled down.  Though for a data node that has a very high
        number of child nodes, it might still make sense to use a scroll area, even if this
        breaks the simplicity of the hyperbolic layout.  Perhaps the scroll area itself can just
        be a window into another hyperbolic half plane.

2021.03.26

-   TODO
    -   Collect data model methods together and do a bit more to formalize it
        -   non-parametric term registration (maybe also allow registration of specific inhabitants of parametric types)
        x   print
        x   eq
        x   compare (maybe rename this to something more specific like lexiographical_compare)
        x   inhabits
        x   construct_inhabitant_of
        x   abstract_type_of
        -   element_type_of
        -   domain_of
        -   codomain_of
        -   is_member (maybe get rid of this in preference of set/map specific methods)
        x   serialize (does this belong here in the data model?)
        x   deserialize (does this belong here in the data model?)
        -   convert (transformations; controlling lossy vs non-lossy)
        -   project (lossy transformations)
        -   embed (type-widening transformations)
        -   size
        -   hash
        -   make_random_inhabitant (needs a notion of RNG)
        -   unary operators
        -   binary operators
        -   operator()
        x   element_of aka operator[]
        -   maybe construction should happen via a method `make`?  but maybe operator() is sufficient.
    -   Start using terms to actually do stuff (e.g. as AST types, using them in ordinary C++ code)
        -   POC: Implement "query/fragment strings" so e.g. particular elements of data can be referenced
            via URIs, e.g. "http://fancysite.com/important/data/bob-info#['address'].size" or something
            -   This requires operator[] at bare minimum
        -   POC: Pattern-matching-based type definitions.  E.g.
            -   `Tuple($X, Add, $Y)` matches any Tuple with 3 elements whose middle element is `Add`.
                Here, $X and $Y represent free variables.
            -   `Tuple($X, Add, $X)` similar, except that the first and last elements must be equal.
            -   Ideally something like `$T($X, $Y, 1)` could be possible, where $T is perhaps constrained.
                This requires having a type whose inhabitants represent the `construct inhabitant of`
                expression itself.  Though if those constructors do any kind of nontrivial processing,
                then it may not be possible to do this kind of matching in a tractable way.
            -   Additional constraints should be specifiable outside of the expression syntax, perhaps
                denoted with a `where` clause.  Though that's only a syntax thing, the abstract concept
                isn't bound by the syntax.  A potentially useful precedent is statistics' syntax for
                https://en.wikipedia.org/wiki/Conditional_probability

                    (x:X, y:Y, z:Z; where <expression in x,y,z>) -> (w:W; where <expression in w,x,y,z>)

                could denote, for example, a function whose domain type involves a non-separable condition
                on x,y,z, and whose codomain involves a non-separable condition on w,x,y,z.  The `;` here
                stands in for `|` in the statistical syntax, since `|` usually means something different
                in programming languages, and `;` would be an unsurprising choice as a separator.
            -   Would need some way to define patterns on unbounded sequences, such as `Array(...)`.  An
                example would be `Array($X, ..., $X)` which should match an Array having uniform elements.
                Though this particular syntax is probably too vague.
        -   POC: Basic strong documentation -- to make this have significant value, some sort of reference
            mechanism is needed, e.g. symbolic references into a symbol table, some pointer-like references
            into local memory, or URI references (which would also need a table to function efficiently).
        -   Make a struct metatype?
            -   This would be a tuple whose elements are named, which would require having a string type
            x   Would this require defining a tuple type first?  Since right now there's
                only Array, which can't force different elements to have distinct types.
        -   POC: Make a simple AST hierarchy (ideally without introducing any new types)
            and a simple interpreter for a simple functional language (no state, only pure
            functions)
        -   POC: Start implementing the (algebra? of) diffs and branching/merging
    -   Make sept GUI program accept input (from stdin or from a child process) so it can
        render and manipulate the output of headless sept programs
    -   Implement hyperbolic tree view
    -   Create vector_any so that ArrayE can be correctly implemented.  Similarly, implement map_any,
        unordered_map_any, set_any, and unordered_set_any.
    -   Think about how references and URIs can work (especially within serialization and
        in the context of fast in-memory implementation -- via a reference cache or something)
    -   Terms use std::any (via sept::Data), which is not ideal.  Is there a better
        way to do this?  For terms that have constraints on their component values,
        more-specific types could be used, but in general, it would be std::any.

2021.03.27

-   Now that Tuple is implemented, there's clearly a specific need for concrete type vs abstract type.
    In particular, concrete_type_of(Tuple(...)) should be Tuple, and abstract_type_of(Tuple(x,y,z))
    should be Tuple(abstract_type_of(x), abstract_type_of(y), abstract_type_of(z)).
-   Is Tuple actually necessary?  Could Array function in place of Tuple?  If the concrete/abstract
    (or rather concrete/semantic) type distinction is made, then Array is basically exactly Tuple.
    In particular:
    -   Array(Uint32(123), Float64(89.0123), True) has:
        -   Concrete type `Array` (this is currently how Array(...)'s abstract type is defined)
        -   Abstract (semantic) type Array(Uint32, Float64, TrueType) (this is how tuple_type_of is defined)
-   Some notes on various kinds of types:
    -   The concrete type of a term defines the representation of that term.  This comes in two parts:
        -   The "format" of the concrete type, which defines how it's represented as a bit sequence
            (independent of the particular architecture or byte/word size), e.g:
            -   IEEE754 double precision floating point number
            -   32-bit integer in two's complement
            -   Fixed point number with M whole bits and N fractional bits
            -   Length-encoded integer (e.g. https://en.wikipedia.org/wiki/LEB128)
            -   boolean value, being precisely 1 bit
        -   The mapping/layout of the format bit sequence into storage (which usually means how it
            maps into a sequence of bytes based on CPU architecture, but should be dealt with at the level
            of bit sequence, since this should include the ability to render into a size-minimized serialized
            form) which consists of:
            -   Overall size of the necessary storage (including any necessary padding), including whether
                the size is parametric (e.g. length-encoded values)
            -   Alignment requirement on the storage
            -   Endianness of the storage
            This does not include compression, which should be considered a transformation of data, since
            it isn't a direct mapping of bits.

            As an example, a C-style union lives at the "memory layout" level of abstraction; the memory
            layout is held constant and the format is varied, i.e. interpreting the memory using different
            formats.

        A given format doesn't have a defined storage layout until other choices are made, in particular
        including but not limited to:
        -   CPU architecture, which determines byte size, word size, alignment requirements, endianness, etc.
        -   Serialized form
            -   Size-minimizing (i.e.. bit packing; length-encoded values).
            -   Some sort of network order serialization.
            -   In a form in which the data structures can be accessed efficiently in-situ.
            -   In a form that can be read directly into memory in one operation.
            -   In a compact form that makes reading easy and fast, but writing difficult or impossible.
                E.g. an ordered map being rendered as a sequence of (key,value) pairs ordered by
                key, instead of as a binary tree or red-black tree.

    -   The abstract type / semantic subtype -- information about what the value actually means, e.g.
        dimensional units for numeric quantities, categorizations (e.g. of strings; email address,
        legal name, phone number, etc), conditions (e.g. positive, negative, "open file", uninitialized,
        etc).

        Some semantic subtypes imply a particular concrete type or family of concrete types (e.g.
        "positive" applies only to types for which "positive" is a well-defined concept, e.g. numeric
        values).  Some semantic types are independent of concrete type, such as "undefined" (i.e. no
        known value assigned [yet]), "uninitialized" (similar to "undefined"), "unvalidated" (e.g. input
        that must be validated before use), "untrusted" (e.g. a value that is from a potentially malicious
        source), "trusted", "private" (e.g. a value that shouldn't be logged, or must be encrypted at
        rest), or "public".  Also useful could be "unused", which could be used to indicate padding
        within a storage layout.

        These names (e.g. "positive", "unvalidated", etc) should be called "semantic classes", because by
        themselves they don't define a type.

        Semantic subtypes basically have set theoretic properties because they define subsets of values.
        Thus it's natural to consider intersections and unions of semantic subtypes.

        How does this concept relate to, for example, C++ classes?  A C++ class defines a struct (and
        therefore a format and layout) and an associated collection of functions of a particular form,
        potentially where one of the struct members is the virtual method table.  This is a particular
        type construction that is more-specific than the discussed concrete/abstract types.

    -   Summarizing, a type has components (ConcreteType, SemanticClass), where ConcreteType is
        (DataFormat, StorageLayout).
        -   Concrete type : Defines the kind of quantity, but not necessarily anything about its semantics.
            -   Data format : Defines the bit representation of the data (e.g. double-precision float, 13-bit
                unsigned int in two's complement), independent of any CPU architecture or other storage medium.
            -   Storage layout : Defines how the bit representation is mapped into storage, accounting
                for endianness, byte size, alignment, etc.
        -   Semantic class : Defines the semantics for the type that go beyond the raw data value itself,
            e.g. "kilograms", "positive", "nonnull", "unvalidated", etc.

    -   How does this manifest e.g. in a programming language, or in a serialization format?

        In a programming language, typically there is some notion of a target architecture, which
        defines byte size, word size, alignment requirements, etc, and therefore given a data format,
        the storage layout is partially or fully determined.

        In a serialization format, the storage layout is arbitrary, and must be specified based on
        usage requirements.  Some example requirements:
        -   Endianness convention (e.g. "network order" is big endian by convention)
        -   Storage-size vs access-speed optimization.  For example, certain data structures can
            be represented using layouts that admit of efficient queries/access, which typically
            trade off size for speed.  Conversely, data that is to be streamed linearly, and not
            randomly accessed, could be represented in a small but "slow" format.
        -   Packed values vs aligned values (e.g. if bytes are the unit of serialization, then
            boolean values might be aligned at byte boundaries to limit complexity of serialization)

    -   A big question is about how to formalize this type system.  Is there set theoretic, algebraic,
        or category-theoretic structure that can be exploited?

        For semantic classes, there is at least set-theoretic structure:
        -   A semantic class defines a subset of values (out of the universe of possible values; this
            carries an implicit definition of which values the semantic class is valid for), and therefore
            semantic classes have a natural poset structure (derived from the poset structure on the
            subsets of values).  If A and B are semantic classes, then
            -   Let `ValuesOf(A)` denote the subset of values satisfying A.
            -   Let `A < B` denote `ValuesOf(A) \subset ValuesOf(B)` (strict subset).
            -   Let `A <= B` denote `ValuesOf(A) \subseteq ValuesOf(B)` (non-strict subset)
            -   Let `A == B` denote `ValuesOf(A) == ValuesOf(B)` (sets have the same members)
            -   Let `A ~/~ B` denote `not(A <= B) and not(B <= A)`.
        -   There are natural semantic classes "empty" and "any" corresponding to the empty set and
            the set of all values (which may or may not be a well-defined concept).
        -   Given semantic classes A, B, A1, A2, ..., An, the following semantic classes can be constructed:
            -   Not(A) -- The complement of A.
            -   I := And(A1, ..., An) -- The intersection of A1, ..., An (the value must satisfy all of the
                listed semantic classes.  In particular, I <= Ai for all i \in {1, ..., n}.
            -   U := Or(A1, ..., An) -- The union of A1, ..., An (the value must satisfy at least one of
                the listed semantic classes.  In particular, Ai <= U for all i \in {1, ..., n}.
            -   S := Sum(A1, ..., An) -- The "tagged union" of A1, ..., An, where a value of type S
                satisfies Ai for some well-defined i \in {1, ..., n}.  E.g. Rust enums

            Function types are related, but aren't really data types, because they come with a lot more
            baggage.  They have their own analogs of concrete type, such as calling convention.
            -   A -> B -- The function type with domain A and codomain B.  A function is a set of
                (domain_element, codomain_element) mapping pairs.  Note that if A != empty, then
                    C <= B      if and only if     (A -> C) <= (A -> B)
                and similarly,
                    C < B       if and only if     (A -> C) < (A -> B)

                However if A == empty, then (A -> C) == empty and (A -> B) == empty, which are equal.

            -   (x:A) -> B(x) -- Dependent function type (this is different notation than standard)

            TODO: Refer to a more-complete type system:
            https://en.wikipedia.org/wiki/Type_system#Specialized_type_systems

        For data formats, because they're dealing with [particular representations of] the data values
        themselves, the relevant structure will be data-specific.  For example, there are canonical
        identifications, embeddings, and projections between various numeric types.

            1-bit boolean           is identical with   1-bit unsigned int
            1-bit boolean           embeds into         N-bit TC int            *TC = two's complement
            M-bit unsigned int      embeds into         N-bit unsigned int      if M < N
            M-bit unsigned int      projects to         N-bit unsigned int      if M > N
            X-precision float       embeds into         Y-precision float       if X < Y
            X-precision float       projects to         Y-precision float       if X > Y

        For canonical identifications, embeddings, and projections, it would be sensible to require
        that they form commutative diagrams over the space of data formats.  This suggests that there's
        probably a functor at work here.

2021.03.29

-   More notes on concrete type vs semantic [sub]type.

    There's a subtle distinction to be made in semantic types.  A semantic type can be "abstract" in
    the sense that it underdetermines the concrete type of a value (e.g. "positive" can apply to any integer
    or floating point number, or arbitrary precision number).  A semantic type that determines the
    concrete type of a value might be called a "concrete semantic type", at the risk of overloading the
    word "concrete".  Although I had been using the term "semantic class" to refer to the abstract,
    underdetermining ones (e.g. "positive").  "Concrete" would be fair here because it determines the
    concrete type of a term of that semantic type.  And it could also be fair to use "abstract" here
    (in the sense of C++) because an abstract semantic type only carries semantic meaning, but no
    concretization.  Though an abstract semantic type plays a different role than an interface.

    Another important case is nominal subtyping, where a "sigil" N is applied to an existing type T, and
    this establishes a new type N(T) which has the same concrete type, but is distinct from the existing
    type T.  In this case, N(T) is a concrete semantic type (assuming that T is a concrete type).
    `N(T)` itself is vague syntax, and should be written as `NominalSubtype(N,T)` or some such.
    Perhaps there could also be `StructuralSubtype(N,T)` which would be considered to be semantically
    equivalent to T.

    "Concrete type" is what goes into the machine code (e.g. via producing and compiling LLVM IR) or
    data storage.  "Semantic type" is the meaning of that code/data.

    Specific examples of concrete types
    -   Float32(1.0101) has type:
        -   ConcreteType := Float32
            -   DataFormat := IEEE743 single precision
            -   StorageLayout := context-dependent (on CPU arch, serialization format, etc)
    -   True has type
        -   ConcreteType := TrueType            -- maybe this should be Bool and SemanticClass should be TrueType
            -   DataFormat := 1bit unsigned int
            -   StorageLayout := context-dependent (on CPU arch, serialization format, etc)
    -   Bool(b) has type
        -   ConcreteType := Bool
            -   DataFormat := 1bit unsigned int
            -   StorageLayout := context-dependent (on CPU arch, serialization format, etc)
    -   Array(Uint32(123), Float64(56.78), True)
        -   ConcreteType := Array := struct { size : Uint64, elements : Data[size] }
            -   DataFormat := <data format for struct/tuple>
            -   StorageLayout := context-dependent (on CPU arch, serialization format, etc), e.g.
                -   0x00 - 0x08 : size; little-endian
                -   0x08 - 0x10 : padding to achieve alignment
                -   0x10 - 0x10+size*sizeof(Data) : elements; define StorageLayout recursively
    -   Tuple(Uint32(123), Float64(56.78), True)
        -   ConcreteType := Tuple(Uint32, Float64, TrueType) -- maybe this has to be primitive?
            -   DataFormat := <data format for tuple>
            -   StorageLayout := context-dependent
    -   Struct(attr1 : Type1, attr2 : Type2)(attr1 := value1, attr2 := value2)
        -   ConcreteType := Struct(attr1 : Type1, attr2 : Type2)
            -   DataFormat := <data formal for struct>
            -   StorageLayout := context-dependent

    All of these concrete types have a semantic class of "Any".

    Add a SemanticTerm type:
    -   SemanticTerm(value, semantic_class) -- value can by any term.

2021.03.30

-   Notes on Union types
    x   BinOp = Union(LogicalBinOp, NumericBinOp)
    x   Expr = Union(BinOpExpr, CondExpr, ParenExpr, UnOpExpr)
    x   Logical = Union(LogicalExpr, LogicalOp)
    x   LogicalExpr = Union(LogicalBinOpExpr, LogicalCondExpr, LogicalParenExpr, LogicalUnOpExpr, LogicalValue)
    x   LogicalOp = Union(LogicalBinOp, LogicalUnOp)
    x   LogicalValue = Union(Bool, FalseType, TrueType)
    x   Numeric = Union(NumericExpr, NumericOp)
    x   NumericExpr = Union(double, NumericBinOpExpr, NumericCondExpr, NumericParenExpr, NumericUnOpExpr)
    x   NumericOp = Union(NumericBinOp, NumericUnOp)
    x   NumericValue = Float64
    x   Value = Union(LogicalValue, NumericValue)
    x   UnOp = Union(LogicalUnOp, NumericUnOp)
    NOTE: Could potentially implement LogicalXyz as Intersection(Logical,Xyz), though those types
    would need to be defined in a more fundamental way so there's no cyclic definition.

    Currently this particular usage of Union is currently problematic because it's a recursively
    defined relationship, and therefore requires some mechanism for indirection, such as
    symbolic references or pointer-like references or actual pointers.

    Question: How to maintain the C++-typed versions of inhabits(x, T) for when T is a Union?
    Certainly the inhabits_data(x, T) case can be taken care of.

    Two approaches: Define a templatized UnionTerm_t<T1,...,Tn> which has an inhabits function
    which accepts a templatized x parameter and checks that against each of the Ti elements.

    Maybe it's not worth having as C++ types, since the inhabits_data lookup is going to be
    decently fast.

    Regarding implementation of Union -- maybe it should take a Tuple(...) or Array(...) as its
    argument.  This might ease certain expressions and constructions and would reduce code
    duplication, though it would be extra syntax.

    UnionTerm_c would be the DataVector-carrying implementation.  There could possibly also be
    UnionTerm_t<T1,...,Tn> which would allow some tighter control on types.  This pattern could
    be also used for arrays, tuples, maps, etc (though really it should just use C++ STL types
    for those).

-   Notes on references
    -   There are various use cases that require references of some kind:
        -   Defining types that are recursive or mutually recursive
        -   Being able to have data structures that aren't plain trees (i.e. have multiple
            nodes that refer to the same node)
        -   Being able to refer to some named data, which could be defined:
            -   Locally in memory, as in a symbol table in a program
            -   Locally in the filesystem
            -   Via URI or URL
    -   Ideally references should have https://en.wikipedia.org/wiki/Referential_transparency
    -   References should survive a round-trip through serialization.
    -   The various use cases suggest that there should be several types of references.
        -   Context-free pointer-based references within local memory (used for recursion and for complex data)
        -   Context-dependent pointer-based references (e.g. "element 0 of the parent data")
        -   Context-dependent named references (e.g. symbolic reference into local data, e.g. codebase)
        -   Context-free named references (e.g. URIs or URLs or similar)
    -   Context-free pointer-based references
        -   Dereference should be no slower than ordinary pointers
        -   Should have referential transparency
        -   Should be re-assignable
        -   It's straightforward enough when the pointed-to type is known at compile time (in the C++ sense)
            but when the pointed-to type is Data, does that change things?
        -   Types

                Ref(T) -- has referential transparency, behaves as T; analogous to `T&` in C++.
                Ptr(T) -- no referential transparency, must be dereferenced; analogous to `T*` in C++.

            There needs to be a way to reference and dereference these terms.  This can be part of the
            data model.
        -   Serialization of a reference or a pointer requires more than just simple depth-first
            traversal of the data structure -- it requires adding a context object to the serialization
            process to track which references/pointers have been serialized/deserialized.  In particular:
            -   There must be a lookup table within the serialization context object which maps in-memory
                references to the in-serialization references.
            -   Form of the references in-serialization
                -   Simplest would be a bit or byte offset from the start of the serialized form.
                -   Next would be to store serialized local references within data structures
                    when feasible (e.g. "here are the elements of the OrderedMap").  This would
                    work well when it's known that the references are local (i.e. the data structure
                    doesn't have external references).
    -   Context-dependent pointer-based references
        -   Should be a "base pointer" and an offset (the offset could be small), though if the base
            pointer is stored explicitly, then it's context-free.  The base pointer is the context.
        -   Dereference should be no slower than an add and a pointer dereference
        -   Same other properties as context-free refs
        -   Main use for these is probably intra-structure references (e.g. a map structure, DAG, poset, etc)
    -   Context-dependent named references
        -   The context must be specified somehow.  Perhaps it's defined using a subtype of the named
            reference type.  E.g:
            -   `SymbolicRef(name)` could be a symbolic reference into a program's symbol table.
            -   `CodeRepoRef(name)` could refer to a particular global symbol in a package that's indexed
                by some programming language's package index (e.g. PyPi for Python or crates.io for Rust)
        -   Ideally the dereference should be faster than a full string lookup in whatever index.
        -   Perhaps the value is cached in local memory, and then turns into a pointer-based reference.
            This would involve managing the cached value, or would depend on some notion of immutability.
        -   Serialization can do one of two things:
            -   Serialize / deserialize just the name itself
            -   Serialize / deserialize also some definition of the context for the named reference
    -   Context-free named references
        -   Similar to context-dependent named references, but requires no context.  E.g. URI or URL.
        -   Naive serialization / deserialization is feasible.
    -   Questions
        -   Should there be notions of immutability?  If not, then the eventual translation to Rust
            will be difficult.

2021.04.02

-   A useful type would be FormalTypeOf(x) where x is any Term.  This provides a clear pattern to use
    for nonparametric terms that don't otherwise need to have a declared type.
-   Maybe also useful would be FormalTypeOfCollection(x1, ..., xn), where x1, ..., xn are declared to
    share a formal type.  Construction of this type would require a parameter, and could either be
    done abstractly (by verifying the construct_inhabitant_of argument is equal to an element in the
    collection, or perhaps by specifying an index into the collection.

2021.04.04

-   Notes on implementing more reference types
    -   A ref requires a "resolver", e.g.
        -   An in-memory pointer-based reference would resolve via pointer dereference.
        -   A symbolic reference would resolve via symbol lookup in local memory, e.g.
            -   In a programming language runtime, looking up the value [and type] of a global function/variable, local variable, etc.
            -   In a programming language compiler, looking up the symbol table entry for a declared function, variable, type, etc.
            This sort of thing is very application-specific, so it would have to be definable by that application.
        -   URI references.  A URI has the form `scheme:[//authority]path[?query][#fragment]`
            -   The scheme should [at least partially] dictate the resolver.  E.g.
                -   `http` -> HTTP GET
                -   `file` -> local filesystem operation
                -   `did:key` -> https://w3c-ccg.github.io/did-method-key/ -- extract the key from the DID URI
    -   For references that refer to non-local data, there could be some sort of data cache so that
        repeated resolutions of non-local references would just use the locally cached data.  This could
        be implemented as a detail of the symbol resolver.
    -   The ability to define new types of references must be left open.  Another Data model method could
        be defined.  This would be "referenced_data", and would return `this` data if it's not a reference,
        and would return the referenced data if it is a reference (and call referenced_data recursively).
        This would add a method lookup for each data access operation, which is not great.
-   Notes on reference types as it pertains to serialization
    -   In-memory pointer-based references
        -   The serialization process would need to store a lookup table of a serialization stream offset for
            each in-memory pointer value.  And for deserialization, the reverse.
        -   For serialization format that must have efficient queries (guaranteeing certain runtime complexity
            for various operations), the referenced values would need to be placed in a different "segment" of
            the serialized stream.  This requires either pre-computing the offset of that segment, or post-filling
            offsets after writing the segment.  It could also involve a totally separate serialization stream,
            but that's probably not a bit of complexity that should be added at the moment.
    -   Symbolic references
        -   The resolution method for symbolic references is a lookup based on the symbol ID.  Thus the serialized
            form has to serialize the symbol table.  The symbolic references themselves can just contain the symbol
            IDs themselves.  There are optimizations that could be made such as having the symbolic references just
            use an index into the symbol table, but for now, keep it simple.
    -   Context-free references
        -   This is the easiest, because there's no context to refer to, so the reference content (e.g a URL) can
            be serialized/deserialized directly.
    -   Some other ideas on serialization
        -   Some sort of locality property regarding serialization of data structures would be useful.  In particular,
            a complex data structure might involve many data nodes with references connecting everything together
            (e.g. a DAG).  Ideally the whole body of that data structure could be serialized into a contiguous
            block, so that it could be "taken out of context" and still understood.  This would imply that any
            references would have to be local to that contiguous block (i.e. relative).  But this concept also
            introduces a distinction in data -- those that can be serialized into a local, contiguous block, and
            those that can't (i.e. that aren't self contained; they refer to a context outside themselves somehow)

            For symbolic references, this would map onto "local symbol contexts", e.g. function-local symbols,
            symbols that are local to a code block.

2021.04.06

-   Notes on trying to implement multiple reference types efficiently.
    -   Could approach this using a function registration as in the rest of the data model,
        but the lookup would be much slower than a pointer dereference, and because this kind
        of dereference is very frequent, this would be an unacceptable slowdown.
    -   The data model is finite, so perhaps implementing a custom vtable would work, where
        types have to be registered with their vtables, and then Data could be changed from
        using std::any to using the vtable to determine the type, as well as call the data
        model methods more efficiently (at least for single-parameter methods which only
        depend on the type of the data).

        Note that multi-parameter data model methods couldn't use this vtable approach, since
        part of the design of the multi-parameter data model is to allow rich multi-parameter
        polymorphism, resolved through use of a type poset.
    -   The downside to using a custom vtable is that it requires creating a custom implementation
        of a std::any-like class.
    -   The upside to using a custom vtable is that type registration becomes more formal and
        probably more clear.

2021.04.07

-   Notes on reasoning about terms of reference type as inhabitants of reference types, dealing with
    the fact that referential transparency makes them a slippery fish.
    -   Make a function like "opaque" which turns a [transparent] reference term into an opaque reference
        (i.e. semantically equivalent to a pointer; requiring explicit dereferencing).  Then reasoning
        about its abstract type would be possible.
-   Idea for efficiently printing transparent references when reference cycles are possible:
    -   Instead of tracking exactly which references have been printed, just track a dereference
        depth counter, and cut it off after some number.  This would not fully print some data,
        for example data that has cycles but has a non-cycle path that is longer than a given depth
        counter limit.  This would be fine for debugging.
    -   Could potentially introduce a subtype of reference with semantics that it causes a cycle,
        and handle it accordingly.
    -   The full implementation would require tracking all dereferenced references, so that some
        sort of "<already-seen>" could be printed in its place.  This would be necessary for the
        sept GUI, since the main part of its mission is to be able to view and navigate arbitrary
        sept data.
